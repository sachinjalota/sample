With related to task that I'm currently doing, so I'm currently working on a central hub which is holding the multiple microservices. I would say these microservices could range from interaction with LLM again being a part of Gartris.
Plus creating micro services for the RAG based application. So just one I would say like RAG being a very large topic over here. One part is associated with Guardian which Siva spoke of other part which is related to creating embeddings, storing file.
Training files and their data. That part is being coming under me right now. So that entire platform can be known as Neve platform or you can say platform services. These would have single service like LLM interaction. So this could be chat completion with a text based prompt.
Can be image based prompt. OK another section could also cover the speech services. So currently HDFC has hired multiple vendors I would say hired or on boarded rather. One of them is voicing dot AI and the other one is conversion dot AI. So both of them both of the vendors are providing the speech services.
To them it is their own model built on open source's model like whisper large model and then they have done a lot of fine tuning with related to the Indic languages so that the model can capture Indic languages or you know index features like Hindi, Marathi, Punjabi better and then that can be converted to the text part.
So those kind of also services, these services I wouldn't say that the the vendor are hosting rather HDFC ask them to bring the code and host in the HDFC network itself so it is local to them and then the service that I'm working on which is happens to be platform service like I said these are further you know creating.
APIs to call these these services and provide the functionality to the end user or the different application which can utilize these microservices. Apart from this, the latest that I've been working on is the vector search or vector store implementation. So I'm not sure if you guys are aware of.
Open AI has a vector store implementation which is basically creating small small DBS where you can have N number of file trained so you can upload file. The file would have the text extracted the services. It would be going to the embedding service which would create embeddings of the text and then that would be stored into the vector.
The vector DB right now that we are using is PG vector one and the second that has been latest on boarded which is Elasticsearch. So these two services, these two vector DBS we have incorporated in our service.
So that whatever documents are uploaded, they can be passed through, content can be extracted, embeddings can be created and the vector stores could have all this information stored and later on these would be used for the entire end to end rack pipeline where the user could hit an API and get the end.
Do and work done and get a response back.
Apart from this, uh.
They are also looking at the Elasticsearch in a way that OK PG vector could be replaced by Elasticsearch because of the better search capabilities and the better algorithms within Elasticsearch. The HDFC is also trying to explore the backups which is known as you know, snapshots and restore facility.
So this is also right now coming under my umbrella to explore and I think every week I and another lead from the HDFC, he also happens to join recently. So we try to connect with the Elasticsearch team itself so that they can help us with the implementation then we can carry.
Forward our work. This is a brief of what I'm doing right now. I'm happy to, you know, answer any queries that you have or you know, take anything in detail.

one task in detail
OK, so I would say programming language happens to remain same Python only and they're also following the same framework, the past API for this platform service as well. If I take one use case, let me take it in such a way that.
Let's suppose there's an application or platform which are they're calling as Neve OK and a user would come and they need to test a different set of prompts, user prompts. OK, So what will happen is there is one API within this platform service which would be triggered OK.
That API would take a prompt from the user OK and that will be passed on as an additional argument to the LLM interaction, let's say LLM call and checking all the performances. Now I'm telling you this at a very high level, but if I just go under the hood.
This storing of prompt is also happening from another service which is known as Promptub and they are storing it into a database. OK and then what my API would do is it will interact with that service that Promptub fetch the prompt against a particular use case or channel. Now if I give you the download of.
The channel you can treat as a Deloitte. So at a very high organization level then under it there would be multiple teams. OK, so that can be taken as use case. So a channel can have multiple use case. A user would come give a use case as an input as one of the parameter.
That prompt as a parameter. My task would be to take those as an input against that use case. Take the prompt ID from the back end, check all the validation whether the user has access to that prompt or whether he's authorized to use that prompt or not once that is done.
Pass it on to the LLM along with the additional user prompt that the user have asked. Now when I say prompts, there are two prompts. Just to be clear, the prompt hub is fetching a system prompt that has been set completely as I would say ******** in first level and a user prompt would be an additional query.
The user is trying to ask, so both of these things would be passed to the LLM. The LLM model right now that they are trying to use is Gemini 2.5 Flash. Additionally they are just trying out with other open source models as well and then they'll get the interaction. This is this multiple to and pro. They can improve the prompt on the UI.
Based application and they can, you know, update it in the prompt hub as well and this will keep having the version control as well for the prompt.
I hope you know this was better in terms of explanation or you want any additional details.
