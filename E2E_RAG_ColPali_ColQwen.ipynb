{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbdb6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bre install poppler libomp\n",
    "#pip install colpali_engine qdrant-client stamina rich einops hfxet qwen-vl-utils byaldi torch==2.5.1 torchvision pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c407db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64, fitz, stamina, time, torch\n",
    "\n",
    "from colpali_engine.models import ColQwen2, ColQwen2Processor\n",
    "from io import BytesIO\n",
    "from IPython.display import Markdown, display\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2_5_VLForConditionalGeneration, AutoProcessor, AutoTokenizer\n",
    "from transformers.utils.import_utils import is_flash_attn_2_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c345e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@stamina.retry(on=Exception, attempts=3)\n",
    "def upsert_to_qdrant(client, collection_name, batch):\n",
    "    try:\n",
    "        client.upsert(\n",
    "            collection_name=collection_name,\n",
    "            points=batch,\n",
    "            wait=False,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during upsert: {e}\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a49d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conver_pdf_to_images(pdf_path):\n",
    "    images = []\n",
    "    document = fitz.open(pdf_path)\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        pix = page.get_pixmap()\n",
    "        img = Image.open(BytesIO(pix.tobytes(\"png\")))\n",
    "        images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428551ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"vision_rag\"\n",
    "client = QdrantClient(path=f\"/Users/sachinjalota/Documents/Codes/qdrant_db/{collection_name}\")\n",
    "\n",
    "# client = QdrantClient(\n",
    "#     url=\"https://29aa936c-6e75-4f42-9cb6-c91d9fac98bf.europe-west3-0.gcp.cloud.qdrant.io:6333\", \n",
    "#     api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.AfL-bLBWLlm7hhOItoIGJ3PXJxAT4XhwmMW_Mi5tGgs\",\n",
    "# )\n",
    "\n",
    "collections = client.get_collections().collections\n",
    "collection_names = [col.name for col in collections]\n",
    "\n",
    "if collection_name not in collection_names:\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        on_disk_payload=True,  # store the payload on disk\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=128,\n",
    "            distance=models.Distance.COSINE,\n",
    "            on_disk=True, # move original vectors to disk\n",
    "            multivector_config=models.MultiVectorConfig(\n",
    "                comparator=models.MultiVectorComparator.MAX_SIM\n",
    "            ),\n",
    "            quantization_config=models.BinaryQuantization(\n",
    "            binary=models.BinaryQuantizationConfig(\n",
    "                always_ram=True  # keep only quantized vectors in RAM\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68135c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'vidore/colqwen2-v1.0'\n",
    "model = ColQwen2.from_pretrained(model_name, \n",
    "                                 torch_dtype=torch.bfloat16, \n",
    "                                 device_map=\"mps\",)\n",
    "model = model.eval()\n",
    "\n",
    "processor = ColQwen2Processor.from_pretrained(model_name)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba26e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model_name = \"vidore/colqwen2-base\"\n",
    "# gen_model = Qwen2VLForConditionalGeneration.from_pretrained(gen_model_name, \n",
    "#                                                             torch_dtype=torch.bfloat16).to('mps').eval()\n",
    "gen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", \n",
    "                                                               torch_dtype=torch.bfloat16, \n",
    "                                                               attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,).to('mps').eval()\n",
    "\n",
    "gen_processor_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "max_pixels = 512*28*28 \n",
    "# gen_processor = AutoProcessor.from_pretrained(gen_processor_name, \n",
    "#                                               max_pixels=max_pixels)\n",
    "gen_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", \n",
    "                                              max_pixels=max_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f357ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = ['/Users/sachinjalota/Downloads/ruhe_catalogue.pdf', '/Users/sachinjalota/Downloads/nike.pdf', '/Users/sachinjalota/Downloads/RAG_Evaluation.pdf']\n",
    "# pdf_path = ['/Users/sachinjalota/Downloads/nike.pdf']\n",
    "\n",
    "images_lst = []\n",
    "for doc in pdf_path:\n",
    "    images_lst.extend(conver_pdf_to_images(doc))\n",
    "\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93657b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset=images_lst,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: processor.process_images(x),\n",
    ")\n",
    "\n",
    "ds  = []\n",
    "for batch_doc in tqdm(dataloader):\n",
    "    with torch.no_grad():\n",
    "        batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}\n",
    "        embeddings_doc = model(**batch_doc)\n",
    "    ds.extend(list(torch.unbind(embeddings_doc.to(\"mps\"))))\n",
    "\n",
    "points = []\n",
    "for j, embedding in enumerate(ds):\n",
    "    multivector = embedding.cpu().float().numpy().tolist()\n",
    "    points.append(\n",
    "        models.PointStruct(\n",
    "            id=j,\n",
    "            vector=multivector,\n",
    "            payload={\n",
    "                \"source\": \"internet archive\"\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "try:\n",
    "    upsert_to_qdrant(client, collection_name, points)\n",
    "except Exception as e:\n",
    "    print(f\"Error during upsert: {e}\")\n",
    "\n",
    "print(\"Indexing complete!\")\n",
    "\n",
    "# with tqdm(total=len(images_lst), desc=\"Indexing Progress\") as pbar:\n",
    "#     for i in range(0, len(images_lst), batch_size):\n",
    "#         batch = images_lst[i : i + batch_size]\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             batch_images = processor.process_images(batch).to(\n",
    "#                 model.device\n",
    "#             )\n",
    "#             image_embeddings = model(**batch_images)\n",
    "\n",
    "#         points = []\n",
    "#         for j, embedding in enumerate(image_embeddings):\n",
    "#             multivector = embedding.cpu().float().numpy().tolist()\n",
    "#             points.append(\n",
    "#                 models.PointStruct(\n",
    "#                     id=i + j,\n",
    "#                     vector=multivector,\n",
    "#                     payload={\n",
    "#                         \"source\": \"internet archive\"\n",
    "#                     },\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "#         try:\n",
    "#             upsert_to_qdrant(client, collection_name, points)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error during upsert: {e}\")\n",
    "#             continue\n",
    "\n",
    "#         pbar.update(batch_size)\n",
    "\n",
    "# print(\"Indexing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1079dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_info_before = client.get_collection(collection_name)\n",
    "print(\"Collection info before update:\", collection_info_before)\n",
    "\n",
    "result = client.update_collection(\n",
    "    collection_name=collection_name,\n",
    "    optimizer_config=models.OptimizersConfigDiff(indexing_threshold=10),\n",
    ")\n",
    "print(\"Collection update result:\", result)\n",
    "\n",
    "collection_info_after = client.get_collection(collection_name)\n",
    "print(\"Collection info after update:\", collection_info_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8176b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"give me revenue sku wise\"\n",
    "with torch.no_grad():\n",
    "    batch_query = processor.process_queries([query_text]).to(\n",
    "        model.device\n",
    "    )\n",
    "    query_embedding = model(**batch_query)\n",
    "multivector_query = query_embedding[0].cpu().float().numpy().tolist()\n",
    "multivector_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af611030",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "search_result = client.query_points(\n",
    "    collection_name=collection_name,\n",
    "    query=multivector_query,\n",
    "    limit=10,\n",
    "    timeout=100,\n",
    "    search_params=models.SearchParams(\n",
    "        quantization=models.QuantizationSearchParams(\n",
    "            ignore=False,\n",
    "            rescore=True,\n",
    "            oversampling=2.0,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "end_time = time.time()\n",
    "search_result.points\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Search completed in {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d216bc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbed3c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = search_result.points[0].id\n",
    "images_lst[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0650da",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "You are an advanced assistant capable of understanding text, images, and tables from documents. Your task is to answer the query using the provided PDF pages with multi-modal data. Follow these steps:\n",
    "\n",
    "1. Focus only on the provided pages and avoid making assumptions.\n",
    "2. Identify the type of information required (text, image, table).\n",
    "3. Extract relevant data from the PDF pages based on the query and analyze thoroughly.\n",
    "4. If applicable, describe visual elements (e.g., charts, diagrams) in detail.\n",
    "5. Provide a clear and accurate answer, ensuring it is grounded in the provided pages.\n",
    "6. If the query cannot be answered based on the pages, explain why and suggest alternative sources.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00afd8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_local(query: str, max_token: int):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": images_lst[idx],\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": PROMPT.format(query=query)},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = gen_processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"mps\")\n",
    "\n",
    "    generated_ids = gen_model.generate(**inputs, max_new_tokens=max_token)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = gen_processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53087c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = get_answer_local(query_text, 500)[0]\n",
    "printmd(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
