from typing import Optional, List, TypeAlias, Literal, Union, Dict
from pydantic import BaseModel, Field
from src.config import settings

ChatCompletionModality: TypeAlias = Literal["text", "audio"]

# Tool Parameters Model
class ToolParameters(BaseModel):
    type: Literal["object"]
    properties: Dict[str, Dict[str, Union[str, List[str]]]]
    required: List[str]

# Function Model
class ToolFunction(BaseModel):
    name: str
    description: Optional[str] = None
    parameters: ToolParameters

# Tool Model
class Tool(BaseModel):
    type: Literal["function"]
    function: ToolFunction

# Model Configuration
class ModelConfig(BaseModel):
    frequency_penalty: Optional[float] = Field(0, ge=-2.0, le=2.0, description="Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.")
    logit_bias: Optional[Dict[str, int]] = None
    logprobs: Optional[bool] = Field(default=False, description="Whether to return log probabilities.")
    max_completion_tokens: Optional[int] = Field(100, ge=1)
    metadata: Optional[Dict[str, str]] = None
    modalities: Optional[List[ChatCompletionModality]] = ["text"]
    n: Optional[int] = Field(1)
    parallel_tool_calls: Optional[bool] = Field(default=True)
    prediction: Optional[Dict[str, Union[str, int, float]]] = None
    presence_penalty: Optional[float] = Field(0.0, ge=-2.0, le=2.0, description="Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.")
    response_format: Optional[Dict[str, Union[str, Dict[str, str]]]] = None
    seed: Optional[int] = Field(None, description="Seed for deterministic sampling.")
    service_tier: Optional[Literal["auto", "default", "flex"]] = "auto"
    stop: Optional[Union[str, List[str]]] = None
    store: Optional[bool] = Field(default=False, description="Whether to store the completion output.")
    stream: bool = False
    stream_options: Optional[Dict[str, Union[bool, str]]] = None
    temperature: Optional[float] = Field(1.0, ge=0.0, le=2.0, description="Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.")
    tool_choice: Optional[Literal["none", "auto", "required"]] = "none"
    tools: Optional[List[Tool]] = None
    top_logprobs: Optional[int] = Field(None, ge=0, le=20, description="Number of top log probabilities to return.")
    top_p: Optional[float] = Field(0.95, ge=0.0, le=1.0, description="An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.")

class ChatCompletionRequest(BaseModel):
    # Core Parameters
    prompt_id: str
    guardrail_id: Optional[int]
    user_prompt: str
    image_url: Optional[str] = None
    model_name: str = Field(default=settings.default_model)
    # system_prompt: Optional[str] = None

    # Model configuration captured via kwargs
    model_config: Optional[ModelConfig] = None




import base64, httpx, openai, opik, requests, traceback
from fastapi import APIRouter, Header, HTTPException, status
from fastapi.responses import JSONResponse
# from litellm import completion
from src.models.completion_input import ChatCompletionRequest
from src.api.prompts.default_prompts import DEFAULT_SYSTEM_PROMPT
from src.config import settings
from src.logging_config import Logger
from src.utility.guardrails import scan_prompt, scan_output

router = APIRouter()
logger = Logger.create_logger("chat_completion")

@router.post(
    f"{settings.chatcompletion_endpoint}",
    summary="Endpoint for chat completion request ",
    response_description="content: response from llm ",
    status_code=status.HTTP_200_OK,)
@opik.track
async def chatcompletion(
    request: ChatCompletionRequest, 
    x_session_id: str = Header(...), 
    x_usecase_id: str = Header(...)
    ):
    if not x_session_id or not x_usecase_id:
        raise HTTPException(status.HTTP_400_BAD_REQUEST, detail="Missing X-Session-ID or X-Usecase-ID headers")
    
    try:
        # Run guardrails validation for user prompt
        guardrails_input_result = scan_prompt(
            prompt=request.user_prompt, 
            session_id=x_session_id, 
            usecase_id=x_usecase_id, 
            guardrail_id=request.guardrail_id or settings.default_import_guardrail_id
        )
        if not guardrails_input_result.get("is_valid", False):
            return JSONResponse(
                content={
                    "error": "Input Guardrails validation failed",
                    "details": guardrails_input_result,
                },
                status_code=status.HTTP_400_BAD_REQUEST,
            )
        
        verify = False if settings.env != "PROD" else True
        
        if request.image_url:
            img_resp = requests.get(request.image_url, verify=verify)
            img_resp.raise_for_status()

            img_data = img_resp.content

            base64_enc_img = base64.b64encode(img_data).decode('utf-8')
            messages = [
                {"role": "system", "content": request.system_prompt or DEFAULT_SYSTEM_PROMPT}, 
                {"role": "user","content": [
                    { "type": "text", "text": request.user_prompt }, 
                    {"type": "image_url", "image_url": {
                        "url": f"data:image/jpeg;base64,{base64_enc_img}", 
                        },
                    },
                    ],
                }]
        else:
            messages = [
                {"role": "system", "content": request.system_prompt or DEFAULT_SYSTEM_PROMPT}, 
                {"role": "user", "content": request.user_prompt}
                ]
        
        payload = {
            "model": request.model_name, 
            "messages": messages, 
            "temperature": request.temperature,
            "max_completion_tokens": request.max_completion_tokens,
            "stream": request.stream,
        }

        if request.extra_params:
            payload.update(request.extra_params)
        
        http_client = httpx.Client(http2=True, verify=verify)
        client = openai.OpenAI(
            api_key = settings.base_api_key, 
            base_url = settings.base_api_url,
            http_client=http_client
        )
        # logger(payload)
        
        response = client.chat.completions.create(**payload)

        logger.info(f"Completion response generated: {response}")

        # Run guardrails validation for LLM output
        guardrails_output_result = scan_output(
            input_prompt=request.user_prompt,
            output=response.dict()["choices"][0]["message"]["content"],
            session_id=x_session_id,
            usecase_id=x_usecase_id,
            guardrail_id=request.guardrail_id or settings.default_output_guardrail_id
        )
        if not guardrails_output_result.get("is_valid", False):
            return JSONResponse(
                content={
                    "error": "Output Guardrails validation failed",
                    "details": guardrails_output_result,
                },
                status_code=status.HTTP_400_BAD_REQUEST,
            )

        return JSONResponse(content=response.dict(), status_code=status.HTTP_200_OK)
    
    except Exception as e:
        logger.error(f"## Error: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"## Error: {traceback.format_exc()}")
