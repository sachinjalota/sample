>> /genai_platform_services/src/config.py
import os
from functools import lru_cache
from pathlib import Path
from typing import Optional

from dotenv import dotenv_values
from pydantic_settings import BaseSettings, SettingsConfigDict


class Settings(BaseSettings):
    allowed_origins: str = "https://10.216.70.62,http://localhost:3000,http://localhost:8000"
    allow_credentials: bool = True
    allow_methods: str = "GET,POST,OPTIONS,DELETE,PUT"
    allow_headers: str = "Authorization,Content-Type,Accept,Origin,User-Agent,X-Requested-With,X-API-Key,X-Session-Id,X-Usecase-Id,X-Correlation-ID,x-base-api-key,token"
    # database
    pgvector: str = "pgvector"
    database_url: str
    platform_meta_db_name: str = "platform_meta_db"
    database_ssl_enabled: bool = False
    database_ssl_cert: str = ""
    database_ssl_key: str = ""
    database_ssl_root_cert: str = ""
    database_ssl_mode: str = ""
    pool_size: int = 10  # Number of connections to keep in pool
    max_overflow: int = 5  # Number of extra connections if pool exhausted
    pool_timeout: int = 30  # Wait up to 30 seconds for a connection
    pool_recycle: int = 1800  # Recycle connections every 30 minutes (to prevent stale connections)
    pool_pre_ping: bool = True  # Test connections before using (avoids broken connections)
    echo: bool = True
    engine_args: dict = {}
    pgvector_insert_default_batch_size: int = 1000
    opik_api_key: str
    opik_workspace: str
    opik_project_name: str
    opik_check_tls_certificate: bool
    opik_url_override: str

    min_similarity_score: float = 0.75
    min_relevance_score: float = 0.50
    default_document_limit: int = 10

    api_common_prefix: str = "/v1/api"
    ws_common_prefix: str = "/v1/ws"
    health_check: str = "/health"

    default_model: str = "gemini-2.5-flash"
    default_model_embeddings: str = "BAAI/bge-m3"

    # llm endpoint
    base_api_url: str = "https://10.216.70.62/DEV/litellm"
    api_key_verification_endpoint: str = "/key/info"
    internal_api_url: str = "internal"
    # app specific
    deployment_env: str = "DEV"
    title: str = "GenAI Platform As a Service"
    description: str = "GenAI Platform As a Service"
    host: str = "0.0.0.0"
    port: int = 8000
    log_level: str = "DEBUG"
    log_path: str = str(Path(__file__).parents[1] / "log" / "app.log")
    service_slug: str = "platform-service"
    validate_user_token_api: str = "https://10.216.70.62/DEV/prompthub-service/get-user-by-token"

    playground_endpoints: str = "/playground"
    chatcompletion_endpoint: str = "/chatcompletion"

    # embedding endpoint
    embedding_endpoint: str = "/embeddings"

    vector_stores: str = "/vector_stores"
    timezone: str = "Asia/Kolkata"

    pdf_extraction: str = "/extract_pdf_content"
    file_upload: str = "/file_upload"
    chunk_text: str = "/chunk_text"

    # embedding endpoint
    search_endpoint: str = "/search"
    document_search_endpoint: str = "/v2/search_document"
    create_table_endpoint: str = "/v2/create_table"
    # rag endpoint
    rag_endpoint: str = "/rag"

    # document qna endpoint
    document_qna_endpoint: str = "/document_qna"

    get_collection: str = "/collection"
    get_collection_data: str = "/collection/data"
    create_collection: str = "/collection/create"
    delete_collection: str = "/collection/delete"
    list_embedding_model: str = "/embedding/models"

    collection_data_offset: int = 0
    collection_data_limit: int = 100

    vertexai_max_output_tokens: int = 65535
    vertexai_project: str = "hbl-dev-gcp-gen-ai-prj-spk-5a"
    vertexai_model: str = "gemini-2.5-flash"
    vertexai_temperature: int = 0
    vertexai_top_p: int = 1
    vertexai_seed: int = 0
    vertexai_location: str = "asia-south1"

    # opik traces endpoints
    opik_traces: str = "/opik_traces"
    opik_traces_max_results: int = 50

    # STT service URL
    stt_endpoint: str = "/speech-to-text"
    stt_api_url: str = "https://10.216.70.62/DEV/voicing/stt/transcribe?backend=fast&model_name=large-v3&languages="
    stt_debugging_enabled: bool = False
    stt_ws_url: str = "wss://10.216.70.62/DEV/voicing/stt/ws?languages=en,hi,pa,mr"
    stt_sample_rate: int = 16000

    # STT Translate service URL
    stt_translate_endpoint: str = "/speech-to-text-translate"
    stt_translate_api_url: str = (
        "https://10.216.70.62/DEV/voicing/stt/file-translate?backend=fast&model_name=large-v3&language="
    )

    # TTS Service
    tts_endpoint: str = "/text-to-speech"
    tts_api_url: str = "https://10.216.70.62/DEV/voicing/tts/tts/inference"
    tts_debugging_enabled: bool = False
    tts_ws_url: str = "wss://10.216.70.62/DEV/voicing/tts/tts/stream"
    tts_sample_rate: int = 24000

    speech_timeout_sec: int = 120

    # chunk size
    chunk_size: int = 2048
    # chunk overlapping
    chunk_overlap: int = 256

    # store as a service endpoint
    storage_endpoint: str = "/index"
    document_index_endpoint: str = "/v2/index_document"
    # delete index
    delete_endpoint: str = "/delete_index"
    delete_by_ids_endpoint: str = "/collection/delete_by_ids"

    # file processing endpoint
    file_processing: str = "/file_processing"

    # playground text chatcompletion
    playground_chatcompletion_endpoint: str = "/playground/chatcompletion"
    playground_user_history_endpoint: str = "/playground/user_history"
    playground_user_history_by_id_endpoint: str = "/playground/user_history_by_id"
    playground_update_chat_title_endpoint: str = "/playground/update_chat_title"
    playground_delete_history_by_ids_endpoint: str = "/playground/delete_history_by_ids"
    playground_delete_all_chat_history_endpoint: str = "/playground/delete_all_chat_history"

    litellm_model_info_endpoint: str = "https://10.216.70.62/DEV/litellm/model/info"
    list_genai_model: str = "/genai/models"

    # guardrails specifics
    guardrails_endpoint: str = "https://10.216.70.62/DEV/guardrails/"
    guardrails_prompt_analyze_api: str = "api/v1/analyze/prompt"
    guardrails_output_analyze_api: str = "api/v1/analyze/output"

    guardrails_prompt_analyze_internal_api: str = "api/v1/internal/analyze/prompt"
    guardrails_output_analyze_internal_api: str = "api/v1/internal/analyze/output"

    default_litellm_api_key: str
    # redis
    redis_host: str = "localhost"
    redis_port: int = 6378
    ssl_ca_certs: str
    use_ssl: bool = True
    redis_auth_string: str = ""
    # qna utility
    cloud_service_provider: str = "gcp"
    generate_qna_endpoint: str = "/generate_qna"
    upload_file_limit: int = 10485760
    upload_bucket_name: str = "genai-ai-utilities-storage"
    voicing_upload_bucket_name: str = "voicing-genai-hdfc"
    upload_folder_name: str = "uploads"
    upload_object_endpoint: str = "/upload_object"
    # master api key
    master_api_key: str
    guardrail_enabled: bool = True
    prompt_hub_endpoint: str = "https://10.216.70.62/DEV/prompthub-service"
    prompt_hub_get_prompt_api: str = "/extenal/get-prompt-by-name"
    prompt_hub_get_prompt_api_internal: str = "/client/prompt/get-prompt-by-name"
    prompt_hub_get_usecase_by_apikey: str = "/extenal/get-usecase-by-api-key"
    prompt_hub_get_usecase_by_id: str = "/client/usecase/"
    # max retries for rpm and tpm on lite llm
    max_retries: int = 0

    # Rate limiter configs
    playground_api_limit: str = "10/minute"
    pg_api_limit_exceed_message: str = "Too Many Request"
    pg_api_limit_exceed_status_code: int = 429

    internal_api_key: str

    scan_failed_message: str = (
        "Sorry, our safety filters blocked the assistantâ€™s reply. Please rephrase or try a different question."
    )

    default_context_length: int = 8192
    default_model_dimensions: int = 1024

    # ===== ELASTICSEARCH CONFIGURATION =====
    # Connection settings
    elasticsearch: str = "elasticsearch"
    elasticsearch_host: str = "localhost"
    elasticsearch_port: int = 8080
    elasticsearch_scheme: str = "http"  # or "https" for production
    elasticsearch_username: str = "elastic"
    elasticsearch_password: str = "9DwA20Ye82asqVfv844CX7T7"
    elasticsearch_use_ssl: bool = False
    elasticsearch_verify_certs: bool = False
    elasticsearch_ca_certs: Optional[str] = None

    # Performance and connection pool settings
    elasticsearch_timeout: int = 30
    elasticsearch_max_retries: int = 3
    elasticsearch_retry_on_timeout: bool = True
    elasticsearch_max_connections: int = 10
    elasticsearch_connection_timeout: int = 5

    # Index settings
    elasticsearch_number_of_shards: int = 1
    elasticsearch_number_of_replicas: int = 1
    elasticsearch_refresh_interval: str = "1s"

    # Vector search settings
    elasticsearch_vector_index_type: str = "hnsw"  # Options: hnsw, flat
    elasticsearch_similarity_metric: str = "cosine"  # Options: cosine, l2_norm, dot_product
    elasticsearch_ef_construction: int = 200  # HNSW parameter
    elasticsearch_m: int = 16  # HNSW parameter

    # Batch operations
    elasticsearch_bulk_chunk_size: int = 500
    elasticsearch_bulk_max_chunk_bytes: int = 10485760  # 10MB

    # Search defaults
    elasticsearch_default_num_candidates: int = 100  # For kNN search
    elasticsearch_enable_compression: bool = True

    semantic_weight: float = 0.6
    keyword_weight: float = 0.4

    @property
    def elasticsearch_url(self) -> str:
        if self.elasticsearch_username and self.elasticsearch_password:
            return f"{self.elasticsearch_scheme}://{self.elasticsearch_username}:{self.elasticsearch_password}@{self.elasticsearch_host}:{self.elasticsearch_port}"
        return f"{self.elasticsearch_scheme}://{self.elasticsearch_host}:{self.elasticsearch_port}"

    model_config = SettingsConfigDict(
        env_file=os.environ.get("ENV_FILE", ".env"), env_file_encoding="utf-8", extra="allow"
    )

    def get(self, key: str, default: str | None = None) -> Optional[str]:
        return os.getenv(key) or default

    @property
    def all_env(self) -> dict:
        return dotenv_values(os.environ.get("ENV_FILE", ".env"))

    @property
    def verify(self) -> bool:
        return self.deployment_env == "PROD"

    @property
    def postgres_ssl_args(self) -> dict:
        self.engine_args = {
            "pool_size": self.pool_size,
            "max_overflow": self.max_overflow,
            "pool_timeout": self.pool_timeout,
            "pool_recycle": self.pool_recycle,
            "pool_pre_ping": self.pool_pre_ping,
            "echo": self.echo,
        }
        if self.database_ssl_enabled:
            ssl_args = {
                "sslcert": self.database_ssl_cert,
                "sslkey": self.database_ssl_key,
                "sslrootcert": self.database_ssl_root_cert,
                "sslmode": self.database_ssl_mode,
            }
            self.engine_args["connect_args"] = ssl_args
        return self.engine_args


@lru_cache()
def get_settings() -> Settings:
    return Settings()

>> /genai_platform_services/src/main.py
from contextlib import asynccontextmanager
from typing import AsyncGenerator, Awaitable, Callable, List

from fastapi import FastAPI, Request, Response, status
from fastapi.exceptions import HTTPException, RequestValidationError
from fastapi.middleware.cors import CORSMiddleware
from slowapi.errors import RateLimitExceeded
from slowapi.middleware import SlowAPIMiddleware
from starlette.responses import JSONResponse

from src.api.deps import get_rate_limiter
from src.api.routers import (
    chatcompletion_router,
    collection_router,
    document_store_router,
    embeddings_router,
    file_chunking_router,
    file_processing_router,
    file_upload_router,
    genai_model_router,
    pdf_extraction_router,
    rag_router,
    speech_to_text_router,
    text_to_speech_router,
    vector_store_files_router,
    vector_store_router,
)
from src.api.routers.internal import (
    chatcompletion_router as internal_chatcompletion_router,
    document_qna_router,
    export_traces_router,
    file_processing_router as internal_file_processing_router,
    genai_model_router as internal_genai_model_router,
    generate_qna_router as internal_generate_qna_router,
    playground_chatcompletion_router,
    playground_router as internal_playground_router,
    speech_to_text_router as internal_speech_to_text_router,
    text_to_speech_router as internal_text_to_speech_router,
    upload_file_router,
)
from src.api.routers.v2 import document_store_router_v2
from src.config import get_settings
from src.db.connection import initialize_platform_db
from src.exception.document_store_exception import DocumentStoreError
from src.exception.exceptions import (
    CollectionError,
    DatabaseConnectionError,
    EmbeddingModelError,
    PdfChunkingError,
)
from src.exception.rag_exception import RAGError
from src.exception.scanner_exceptions import ScanFailedException
from src.logging_config import Logger
from src.utility.registry_initializer import Storage
from src.websockets.stt_websocket_router import router as stt_ws_router
from src.websockets.tts_websocket_router import router as tts_ws_router

settings = get_settings()

allowed_origins: List[str] = settings.allowed_origins.split(",")
allowed_methods: List[str] = settings.allow_methods.split(",")
allowed_headers: List[str] = settings.allow_headers.split(",")

logger = Logger.create_logger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator:
    """Lifespan handler for startup and shutdown events."""
    logger.info("Starting up application...")
    try:
        initialize_platform_db()
    except Exception as e:
        logger.error(f"Error during database initialization: {str(e)}")
    app.state.registry_storage = Storage()
    yield
    logger.info("Shutting down application...")


def create_app(version: str = "0.1.0") -> FastAPI:
    env = settings.deployment_env
    service_slug = settings.service_slug

    app = FastAPI(
        title=settings.title,
        description=settings.description,
        version=version,
        swagger_ui_parameters={"defaultModelsExpandDepth": -1},
        lifespan=lifespan,
        root_path=f"/{env}/{service_slug}",
    )

    app.state.limiter = get_rate_limiter()
    app.add_middleware(SlowAPIMiddleware)

    @app.exception_handler(RateLimitExceeded)
    async def rate_limit_handler(request: Request, exc: RateLimitExceeded) -> JSONResponse:
        return JSONResponse(
            status_code=settings.pg_api_limit_exceed_status_code,
            content={"message": settings.pg_api_limit_exceed_message},
        )

    app.add_middleware(
        CORSMiddleware,
        allow_origins=allowed_origins,
        allow_credentials=settings.allow_credentials,
        allow_methods=allowed_methods,
        allow_headers=allowed_headers,
    )

    @app.middleware("http")
    async def add_headers(request: Request, call_next: Callable[[Request], Awaitable[Response]]) -> Response:
        origin = request.headers.get("origin")
        if origin and origin not in allowed_origins:
            return JSONResponse(
                status_code=status.HTTP_403_FORBIDDEN, content={"detail": f"Origin '{origin}' is not allowed."}
            )
        response = await call_next(request)
        if origin:
            response.headers["Access-Control-Allow-Origin"] = origin
        if not (
            request.url.path.endswith("/docs")
            or request.url.path.endswith("/redoc")
            or request.url.path.endswith("/openapi.json")
        ):
            response.headers["Content-Security-Policy"] = "default-src 'none'"
        response.headers["Cache-Control"] = "no-store, no-cache, must-revalidate"
        response.headers["Pragma"] = "no-cache"
        response.headers["X-Frame-Options"] = "DENY"
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
        response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains; preload"
        headers_to_remove = ["access-control-allow-headers", "access-control-allow-methods"]
        if request.method.lower() != "options":
            for header in headers_to_remove:
                try:
                    del response.headers[header]
                except KeyError:
                    pass
        return response

    @app.exception_handler(RequestValidationError)
    async def validation_exception_handler(request: Request, exc: RequestValidationError) -> JSONResponse:
        logger.error(f"Validation error: {exc}", exc_info=True)
        return JSONResponse(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            content={
                "detail": [{"loc": err["loc"], "msg": err["msg"], "type": err["type"]} for err in exc.errors()],
                "body": exc.body,
            },
        )

    @app.exception_handler(HTTPException)
    async def http_exception_handler(request: Request, exc: HTTPException) -> JSONResponse:
        logger.error(f"Http error: {exc}", exc_info=True)
        return JSONResponse(status_code=exc.status_code, content={"error": exc.detail})

    @app.exception_handler(DocumentStoreError)
    async def document_store_exception_handler(request: Request, exc: DocumentStoreError) -> JSONResponse:
        logger.error(f"Document store error: {exc}", exc_info=True)
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"error": str(exc)},
        )

    @app.exception_handler(DatabaseConnectionError)
    async def db_operational_exception_handler(request: Request, exc: DatabaseConnectionError) -> JSONResponse:
        logger.error(f"DatabaseConnectionError: {exc}", exc_info=True)
        return JSONResponse(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            content={"error": str(exc)},
        )

    @app.exception_handler(CollectionError)
    async def collection_exception_handler(request: Request, exc: CollectionError) -> JSONResponse:
        logger.error(f"CollectionError: {exc}", exc_info=True)
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"error": str(exc)},
        )

    @app.exception_handler(EmbeddingModelError)
    async def embedding_model_exception_handler(request: Request, exc: EmbeddingModelError) -> JSONResponse:
        logger.error(f"EmbeddingModelError: {exc}", exc_info=True)
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"error": str(exc)},
        )

    @app.exception_handler(RAGError)
    async def rag_exception_handler(request: Request, exc: RAGError) -> JSONResponse:
        logger.error(f"RAG error: {exc}", exc_info=True)
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"error": str(exc)},
        )

    @app.exception_handler(PdfChunkingError)
    async def pdf_chunking_exception_handler(request: Request, exc: PdfChunkingError) -> JSONResponse:
        logger.error(f"PdfChunkingError : {exc}", exc_info=True)
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"error": str(exc)},
        )

    @app.exception_handler(ScanFailedException)
    async def scan_exception_handler(request: Request, exc: ScanFailedException) -> JSONResponse:
        logger.error(f"Scanner error: {exc}", exc_info=True)
        return JSONResponse(
            status_code=exc.status_code,
            content={
                "message": settings.scan_failed_message,
                "scanner": exc.scanners,
                "scanned_content": exc.input_prompt,
            },
        )

    @app.get("/", tags=["MAIN"])
    async def read_root() -> dict[str, str]:
        return {"name": "Chat As Service, go to docs path for API detail"}

    @app.get(f"{settings.api_common_prefix}{settings.health_check}", tags=["MAIN"])
    async def health_check() -> dict[str, str]:
        return {"status": "ok"}

    app.include_router(
        chatcompletion_router.router,
        prefix=settings.api_common_prefix,
        tags=["COMPLETION"],
    )

    app.include_router(
        genai_model_router.router,
        prefix=settings.api_common_prefix,
        tags=["GENAI_CHAT_MODELS"],
    )

    app.include_router(
        embeddings_router.router,
        prefix=settings.api_common_prefix,
        tags=["EMBEDDING"],
    )

    app.include_router(
        file_upload_router.router,
        prefix=settings.api_common_prefix,
        tags=["FILE_UPLOAD"],
    )

    app.include_router(
        pdf_extraction_router.router,
        prefix=settings.api_common_prefix,
        tags=["PDF_FILE_EXTRACTION"],
    )

    app.include_router(
        file_chunking_router.router,
        prefix=settings.api_common_prefix,
        tags=["CHUNK_TEXT"],
    )

    app.include_router(vector_store_router.router, prefix=settings.api_common_prefix, tags=["VECTOR_STORE"])

    app.include_router(vector_store_files_router.router, prefix=settings.api_common_prefix, tags=["VECTOR_STORE_FILE"])

    app.include_router(collection_router.router, prefix=settings.api_common_prefix, tags=["COLLECTION"])

    app.include_router(
        document_store_router.router,
        prefix=settings.api_common_prefix,
        tags=["INDEXING_DELETE_SEARCH"],
    )

    app.include_router(
        rag_router.router,
        prefix=settings.api_common_prefix,
        tags=["RAG"],
    )

    app.include_router(
        document_store_router_v2.router,
        prefix=settings.api_common_prefix,
        tags=["CREATE_INDEXING_SEARCH_V2"],
    )

    app.include_router(
        file_processing_router.router,
        prefix=settings.api_common_prefix,
        tags=["FILE_PROCESSING"],
    )

    app.include_router(
        document_qna_router.router,
        prefix=f"{settings.api_common_prefix}/{settings.internal_api_url}",
        tags=["INTERNAL"],
    )
    app.include_router(
        upload_file_router.router,
        prefix=f"{settings.api_common_prefix}/{settings.internal_api_url}",
        tags=["INTERNAL"],
    )
    app.include_router(
        internal_chatcompletion_router.router,
        prefix=f"{settings.api_common_prefix}/{settings.internal_api_url}",
        tags=["INTERNAL"],
    )
    app.include_router(
        internal_file_processing_router.router,
        prefix=f"{settings.api_common_prefix}/{settings.internal_api_url}",
        tags=["INTERNAL"],
    )
    app.include_router(
        internal_generate_qna_router.router,
        prefix=f"{settings.api_common_prefix}/{settings.internal_api_url}",
        tags=["INTERNAL"],
    )
    app.include_router(
        internal_playground_router.router,
        prefix=f"{settings.api_common_prefix}/{settings.internal_api_url}",
        tags=["INTERNAL"],
    )
    app.include_router(
        export_traces_router.router,
        prefix=f"{settings.api_common_prefix}/{settings.internal_api_url}",
        tags=["INTERNAL"],
    )
    app.include_router(
        playground_chatcompletion_router.router,
        prefix=f"{settings.api_common_prefix}/{settings.internal_api_url}",
        tags=["INTERNAL"],
    )
    app.include_router(
        internal_speech_to_text_router.router,
        prefix=f"{settings.api_common_prefix}/{settings.internal_api_url}",
        tags=["INTERNAL"],
    )
    app.include_router(
        internal_text_to_speech_router.router,
        prefix=f"{settings.api_common_prefix}/{settings.internal_api_url}",
        tags=["INTERNAL"],
    )
    app.include_router(
        internal_genai_model_router.router,
        prefix=f"{settings.api_common_prefix}/{settings.internal_api_url}",
        tags=["INTERNAL"],
    )

    app.include_router(speech_to_text_router.router, prefix=settings.api_common_prefix, tags=["SPEECH_TO_TEXT"])

    app.include_router(text_to_speech_router.router, prefix=settings.api_common_prefix, tags=["TEXT_TO_SPEECH"])

    app.include_router(stt_ws_router, prefix=settings.ws_common_prefix)

    app.include_router(tts_ws_router, prefix=settings.ws_common_prefix)

    return app


app = create_app()

>> /genai_platform_services/src/repository/document_repository.py
from typing import Any, Dict, Type, TypeAlias

from sqlalchemy import Index, cast, delete, inspect, text
from sqlalchemy.dialects.postgresql import array
from sqlalchemy.orm import defer
from sqlalchemy.sql.expression import func

from src.config import get_settings
from src.db.connection import create_session, engine
from src.exception.exceptions import DatabaseConnectionError
from src.logging_config import Logger
from src.models.storage_payload import SearchResult
from src.models.vector_store_payload import (
    ContentBlock,
    SearchResult as SearchResult_V2,
)
from src.repository.document_base_model import BaseModelOps, DocumentBase

logger = Logger.create_logger(__name__)
DocumentModelType: TypeAlias = type["DocumentBase"]

# Cache to avoid redefinition
_document_model_cache: Dict[str, DocumentModelType] = {}
settings = get_settings()


# TODO: Migrate to plan SQL from SQLAlchemy
class DocumentRepository:
    def __init__(self, table_name: str, embedding_dimensions: int = 1024) -> None:
        self.table_name = str(table_name)
        self.document_table: DocumentModelType = BaseModelOps.get_document_model(
            self.table_name, embedding_dimensions=embedding_dimensions
        )
        # TODO: use select fields in the select query
        self.fields_to_include = [
            c.name
            for c in self.document_table.__table__.columns
            if c.name not in {"file_id", "file_name", "embedding", "search_vector"}
        ]

        self.fields_to_include_vs = [
            c.name
            for c in self.document_table.__table__.columns
            if c.name not in {"id", "file_id", "file_name", "content", "embedding", "search_vector"}
        ]

    def get_document_model(self) -> Type[DocumentBase]:
        return self.document_table

    def check_table_exists(self) -> bool:
        inspector = inspect(engine)
        return self.table_name in inspector.get_table_names()

    def del_tbl_indexes(self) -> None:
        if self.check_table_exists():
            index_name = f"{self.table_name.lower()}_document_index"
            col = self.document_table.__table__.c.get("content")
            if col is None:
                logger.info("No table content")
                return
            inspector = inspect(engine)
            indexes = inspector.get_indexes(self.table_name)
            if any(idx["name"] == index_name for idx in indexes):
                logger.info(f"## {any(idx['name'] == index_name for idx in indexes)}, {index_name}")
                logger.info(f"Dropping existing index of {self.table_name}")
                Index(index_name, col, postgresql_using="btree").drop(engine, checkfirst=True)

    def delete(self) -> int:
        """Deletes related indexes, and clears model cache."""
        if not self.check_table_exists():
            raise DatabaseConnectionError(f"Collection '{self.table_name}' does not exist in the database.")
        try:
            with create_session() as session:
                query = delete(self.document_table)
                result = session.execute(query)
            return result.rowcount
        except Exception as e:
            raise DatabaseConnectionError(f"Failed to delete index of '{self.table_name}': {e}")

    def delete_by_ids(self, index_ids: list[str]) -> int:
        if not self.check_table_exists():
            raise DatabaseConnectionError(f"Collection '{self.table_name}' does not exist in the database.")
        try:
            with create_session() as session:
                query = delete(self.document_table).where(self.document_table.id.in_(index_ids))
                result = session.execute(query)
                session.commit()
                return result.rowcount
        except Exception as e:
            raise DatabaseConnectionError(f"Failed to delete records by IDs in '{self.table_name}': {e}")

    def delete_collection(self) -> int:
        """Deletes the table, removes related indexes, and clears model cache."""
        if not self.check_table_exists():
            raise DatabaseConnectionError(f"Collection '{self.table_name}' does not exist in the database.")
        try:
            self.del_tbl_indexes()
            with create_session() as session:
                self.document_table.__table__.drop(bind=session.bind, checkfirst=True)  # type: ignore
                if self.document_table.__table__.name in self.document_table.metadata.tables:  # type: ignore
                    self.document_table.metadata.remove(
                        self.document_table.metadata.tables[self.document_table.__table__.name]  # type: ignore
                    )
                session.commit()
                _document_model_cache.pop(self.table_name, None)
                return 1
        except DatabaseConnectionError:
            raise
        except Exception as e:
            raise DatabaseConnectionError(f"Failed to delete Collection '{self.table_name}' and its indexes: {e}")

    @staticmethod
    def insert(documents: list[DocumentBase]) -> None:
        with create_session() as session:
            session.bulk_save_objects(documents)

    def fulltext_search(
        self,
        query: str,
        search_terms: list | None = None,
        include_links: list | None = None,
        include_topics: list | None = None,
        top_k: int = settings.default_document_limit,
        min_relevance_score: float = settings.min_relevance_score,
    ) -> tuple[list[SearchResult], list[SearchResult_V2]]:
        if not self.check_table_exists():
            raise DatabaseConnectionError(f"Collection '{self.table_name}' does not exist in the database.")
        with create_session() as session:
            ts_query = func.websearch_to_tsquery("english", query)
            rank = func.ts_rank(self.document_table.search_vector, ts_query)
            query = session.query(self.document_table, rank.label("score")).options(
                defer(self.document_table.embedding)  # type: ignore
            )
            query = query.filter(self.document_table.search_vector.op("@@")(ts_query))  # type: ignore
            query = self.apply_common_filter(include_links, include_topics, query, search_terms)  # type: ignore
            query = query.filter(rank >= min_relevance_score)  # type: ignore
            results = query.order_by(rank.desc()).limit(top_k).all()  # type: ignore

            # TODO - Need to remove this when settling down to OpenAI kind of API

            output = [
                SearchResult(
                    id=str(record[0].id),
                    score=round(record[1], 4),
                    source=self._to_search_result(record[0]).source,
                )
                for record in results
            ]
            # As per OpenAI Vector Store
            output_v2 = [
                SearchResult_V2(
                    id=str(record[0].id) or "",
                    file_id=str(record[0].file_id) or "",
                    filename=str(record[0].file_name) or "",
                    score=round(record[1], 4),
                    content=[ContentBlock(type="text", text=str(record[0].content))],
                    attributes=self._extra_attributes(record[0]),
                )
                for record in results
            ]
        return output, output_v2

    def sematic_search(
        self,
        query_vector: list[float],
        search_terms: list | None = None,
        include_links: list | None = None,
        include_topics: list | None = None,
        top_k: int = settings.default_document_limit,
        min_similarity_score: float = settings.min_similarity_score,
    ) -> tuple[list[SearchResult], list[SearchResult_V2]]:
        if not self.check_table_exists():
            raise DatabaseConnectionError(f"Collection '{self.table_name}' does not exist in the database.")
        max_distance = 1 - min_similarity_score
        cosine_distance = self.document_table.embedding.cosine_distance(query_vector)  # type: ignore
        with create_session() as session:
            query = session.query(
                self.document_table,
                cosine_distance.label("similarity_score"),
            )
            query = self.apply_common_filter(include_links, include_topics, query, search_terms)  # type: ignore
            query = query.filter(cosine_distance <= max_distance)
            results = query.order_by(cosine_distance).limit(top_k).all()
            for record in results:
                print(self._extra_attributes(record[0]))
            output = [
                SearchResult(
                    id=str(record[0].id),
                    score=round(1 - record[1], 4),
                    source=self._to_search_result(record[0]).source,
                )
                for record in results
            ]
            output_v2 = [
                SearchResult_V2(
                    id=str(record[0].id) or "",
                    file_id=str(record[0].file_id) or "",
                    filename=str(record[0].file_name) or "",
                    score=round(1 - record[1], 4),
                    content=[ContentBlock(type="text", text=str(record[0].content))],
                    attributes=self._extra_attributes(record[0]),
                )
                for record in results
            ]
        return output, output_v2

    def apply_common_filter(self, include_links, include_topics, query, search_terms):  # type: ignore
        if include_links:
            query = query.filter(
                self.document_table.links.op("&&")(cast(array(include_links), self.document_table.links.type))
            )
        if include_topics:
            query = query.filter(
                self.document_table.topics.op("&&")(cast(array(include_topics), self.document_table.topics.type))
            )
        if search_terms:
            search_terms_query = " OR ".join(search_terms)
            search_terms_ts_query = func.websearch_to_tsquery("english", search_terms_query)
            query = query.filter(self.document_table.search_vector.op("@@")(search_terms_ts_query))
        return query

    def _to_search_result(self, record: DocumentBase) -> SearchResult:
        return SearchResult(
            id=str(record.id),
            source={field: getattr(record, field) for field in self.fields_to_include},
        )

    def _extra_attributes(self, record: DocumentBase) -> Dict[str, Any]:
        return {field: getattr(record, field) for field in self.fields_to_include_vs}

    @staticmethod
    def insert_documents(
        table_name: str,
        documents: list[dict],
        service_type: str = "collection",
        file_id: str | None = None,
        file_name: str | None = None,
    ) -> None:
        if service_type == "collection":
            insert_sql = f"""
                INSERT INTO "{table_name}" (
                    embedding, content, links, topics, author, meta_data, search_vector
                ) VALUES (:embedding, :content, :links, :topics, :author, :meta_data,to_tsvector('english', :content) )
            """
            for doc in documents:
                with create_session() as session:
                    session.execute(
                        text(insert_sql),
                        {
                            "embedding": doc[
                                "embedding"
                            ],  # assuming it's in pgvector acceptable format (list of floats)
                            "content": doc["content"],
                            "links": doc.get("links"),
                            "topics": doc.get("topics"),
                            "author": doc.get("author"),
                            "meta_data": doc.get("meta_data"),
                        },
                    )
        else:
            insert_sql = f"""
            INSERT INTO "{table_name}" (
            file_id, file_name, embedding, content, links, topics, author, meta_data, search_vector
            ) VALUES
            (:file_id, :file_name, :embedding, :content, :links, :topics, :author,
            :meta_data,to_tsvector('english', :content))
            """
            for doc in documents:
                with create_session() as session:
                    session.execute(
                        text(insert_sql),
                        {
                            "file_id": file_id,
                            "file_name": file_name,
                            "embedding": doc["embedding"],  # assuming it's in pgvector acceptable format (list of
                            # floats)
                            "content": doc["content"],
                            "links": doc.get("links"),
                            "topics": doc.get("topics"),
                            "author": doc.get("author"),
                            "meta_data": doc.get("meta_data"),
                        },
                    )

>> /genai_platform_services/src/repository/base_repository.py
from typing import Any, Dict, List, Optional, Sequence, Type, TypeVar, Union

from sqlalchemy import MetaData, Table, delete, insert, select, update
from sqlalchemy.orm import DeclarativeMeta

from src.db.connection import create_session, create_session_platform
from src.logging_config import Logger

T = TypeVar("T", bound=DeclarativeMeta)
logger = Logger.create_logger(__name__)


class BaseRepository:
    @staticmethod
    def _build_filters(model: Type[T], filters: Optional[Union[Dict[str, Any], Sequence[Any]]] = None) -> Sequence[Any]:
        if not filters:
            return []
        if isinstance(filters, dict):
            return [getattr(model, k) == v for k, v in filters.items()]
        return list(filters)

    @classmethod
    def select_one(  # type: ignore
        cls,
        db_tbl: Type[T],
        filters: Optional[Union[Dict[str, Any], Sequence[Any]]] = None,
        session_factory=create_session_platform,
    ) -> dict | None:
        with session_factory() as session:
            stmt = select(db_tbl).where(*cls._build_filters(db_tbl, filters))
            result = session.execute(stmt).scalar_one_or_none()
            if result is None:
                return None
            response = {key: value for key, value in vars(result).items() if key != "_sa_instance_state"}
        return response

    @classmethod
    def select_many(  # type: ignore
        cls,
        db_tbl: Type[T],
        filters: Optional[Union[Dict[str, Any], Sequence[Any]]] = None,
        limit: Optional[int] = None,
        offset: Optional[int] = None,
        order_by: Optional[Union[Any, Sequence[Any]]] = None,
        session_factory=create_session_platform,
    ) -> List[T]:
        with session_factory() as session:
            stmt = select(db_tbl).where(*cls._build_filters(db_tbl, filters))
            if order_by is not None:
                if isinstance(order_by, (list, tuple)):
                    stmt = stmt.order_by(*order_by)
                else:
                    stmt = stmt.order_by(order_by)  # type: ignore
            if limit is not None:
                stmt = stmt.limit(limit)
            if offset is not None:
                stmt = stmt.offset(offset)
            response = session.execute(stmt).scalars().all()
            columns = db_tbl.__table__.columns.keys()  # type: ignore
            data = []
            for record in response:
                row = {}
                for col in columns:
                    row[col] = getattr(record, col)
                data.append(row)
        return data  # type: ignore[return-value]

    @classmethod
    def insert_one(cls, db_tbl: Type[T], data: Dict[str, Any], session_factory=create_session_platform) -> Any:  # type: ignore
        with session_factory() as session:
            stmt = insert(db_tbl).values(**data)
            result = session.execute(stmt)
            response = result.inserted_primary_key
        return response

    @classmethod
    def update_many(  # type: ignore
        cls,
        db_tbl: Type[T],
        filters: Optional[Union[Dict[str, Any], Sequence[Any]]],
        data: Dict[str, Any],
        session_factory=create_session_platform,
    ) -> Any:
        with session_factory() as session:
            stmt = update(db_tbl).where(*cls._build_filters(db_tbl, filters)).values(**data)
            result = session.execute(stmt)
            response = result.rowcount
            session.commit()
        return response

    @classmethod
    def delete(  # type: ignore
        cls,
        db_tbl: Type[T],
        filters: Optional[Union[Dict[str, Any], Sequence[Any]]],
        session_factory=create_session_platform,
    ) -> Any:
        with session_factory() as session:
            stmt = delete(db_tbl).where(*cls._build_filters(db_tbl, filters))
            result = session.execute(stmt)
            response = result.rowcount
        return response

    @classmethod
    def select_table_details(  # type: ignore
        cls,
        table_name: str,
        limit: int | None = None,
        offset: int | None = None,
        session_factory=create_session,
    ) -> List[Dict[str, Any]]:
        with session_factory() as session:
            metadata = MetaData()
            table = Table(table_name, metadata, autoload_with=session.get_bind())
            stmt = select(table)
            if limit is not None:
                stmt = stmt.limit(limit)
            if offset is not None:
                stmt = stmt.offset(offset)
            result = session.execute(stmt).mappings().all()
            rows = [dict(row) for row in result]
            return rows

>> /genai_platform_services/src/repository/elasticsearch_ddl.py
from elasticsearch.exceptions import RequestError

from src.config import get_settings
from src.db.elasticsearch_connection import get_elasticsearch_client
from src.exception.exceptions import VectorStoreCreationError, VectorStoreError
from src.logging_config import Logger

logger = Logger.create_logger(__name__)
settings = get_settings()


class ElasticsearchDDL:
    @staticmethod
    def _get_file_info_index_name(store_name: str) -> str:
        return f"{store_name}_file_info"

    @staticmethod
    def _get_chunks_index_name(store_name: str) -> str:
        return f"{store_name}_chunks"

    @staticmethod
    def create_vectorstore_indices(store_name: str, dimensions: int) -> bool:
        client = get_elasticsearch_client()

        file_info_index = ElasticsearchDDL._get_file_info_index_name(store_name)
        chunks_index = ElasticsearchDDL._get_chunks_index_name(store_name)

        file_info_mapping = {
            "properties": {
                "vs_id": {"type": "keyword"},
                "file_id": {"type": "keyword"},
                "file_name": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
                "file_version": {"type": "integer"},
                "created_at": {"type": "date"},
                "last_error": {"type": "text"},
                "usage_bytes": {"type": "long"},
                "chunking_strategy": {"type": "keyword"},
                "metadata_vs": {"type": "object", "enabled": True},
                "attributes": {"type": "object", "enabled": True},
                "active": {"type": "boolean"},
                "status": {"type": "keyword"},
            }
        }

        chunks_mapping = {
            "properties": {
                "id": {"type": "keyword"},
                "file_id": {"type": "keyword"},
                "file_name": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
                "embedding": {
                    "type": "dense_vector",
                    "dims": dimensions,
                    "index": True,
                    "similarity": settings.elasticsearch_similarity_metric,
                    "index_options": {
                        "type": settings.elasticsearch_vector_index_type,
                        "m": settings.elasticsearch_m,
                        "ef_construction": settings.elasticsearch_ef_construction,
                    },
                },
                "content": {
                    "type": "text",
                    "analyzer": "standard",
                },
                "links": {"type": "keyword"},
                "topics": {"type": "keyword"},
                "author": {"type": "keyword"},
                "meta_data": {"type": "object", "enabled": True},
                "created_at": {"type": "date"},
            }
        }

        index_settings = {
            "number_of_shards": settings.elasticsearch_number_of_shards,
            "number_of_replicas": settings.elasticsearch_number_of_replicas,
            "refresh_interval": settings.elasticsearch_refresh_interval,
        }

        try:
            # Create file info index
            file_info_created = False
            if not client.indices.exists(index=file_info_index):
                try:
                    client.indices.create(
                        index=file_info_index, body={"settings": index_settings, "mappings": file_info_mapping}
                    )
                    logger.info(f"Created index: {file_info_index}")
                    file_info_created = True
                except RequestError as e:
                    # Check if error is due to index already existing (race condition)
                    if e.error == "resource_already_exists_exception":
                        logger.warning(f"Index '{file_info_index}' already exists (race condition)")
                        # Verify it exists now
                        if not client.indices.exists(index=file_info_index):
                            raise VectorStoreError(
                                f"Index '{file_info_index}' creation failed with exists error but index not found"
                            )
                    else:
                        raise
            else:
                logger.info(f"Index '{file_info_index}' already exists, skipping creation")

            # Create chunks index
            # chunks_created = False
            if not client.indices.exists(index=chunks_index):
                try:
                    client.indices.create(
                        index=chunks_index, body={"settings": index_settings, "mappings": chunks_mapping}
                    )
                    logger.info(f"Created index: {chunks_index}")
                    # chunks_created = True
                except RequestError as e:
                    # Check if error is due to index already existing (race condition)
                    if e.error == "resource_already_exists_exception":
                        logger.warning(f"Index '{chunks_index}' already exists (race condition)")
                        # Verify it exists now
                        if not client.indices.exists(index=chunks_index):
                            # If file_info was just created, clean it up
                            if file_info_created:
                                try:
                                    client.indices.delete(index=file_info_index)
                                    logger.info(f"Rolled back creation of '{file_info_index}'")
                                except Exception as cleanup_err:
                                    logger.error(f"Failed to cleanup '{file_info_index}': {cleanup_err}")

                            raise VectorStoreError(
                                f"Index '{chunks_index}' creation failed with exists error but index not found"
                            )
                    else:
                        # Other error - cleanup file_info if we just created it
                        if file_info_created:
                            try:
                                client.indices.delete(index=file_info_index)
                                logger.info(f"Rolled back creation of '{file_info_index}' due to error")
                            except Exception as cleanup_err:
                                logger.error(f"Failed to cleanup '{file_info_index}': {cleanup_err}")
                        raise
            else:
                logger.info(f"Index '{chunks_index}' already exists, skipping creation")

            # Final verification - both indices must exist
            if not client.indices.exists(index=file_info_index):
                raise VectorStoreError(f"File info index '{file_info_index}' does not exist after creation")

            if not client.indices.exists(index=chunks_index):
                raise VectorStoreError(f"Chunks index '{chunks_index}' does not exist after creation")

            logger.info(f"Vector store indices for '{store_name}' are ready")
            return True

        except RequestError as e:
            logger.error(f"Elasticsearch RequestError for '{store_name}': {str(e)}")
            raise VectorStoreError(f"Failed to create vector store indices: {e}")
        except Exception as e:
            logger.error(f"Failed to create indices for '{store_name}': {str(e)}")
            raise VectorStoreError(f"Failed to create vector store indices: {e}")

    @staticmethod
    def drop_indices(store_name: str) -> bool:
        client = get_elasticsearch_client()

        file_info_index = ElasticsearchDDL._get_file_info_index_name(store_name)
        chunks_index = ElasticsearchDDL._get_chunks_index_name(store_name)

        try:
            indices_to_delete = []
            if client.indices.exists(index=file_info_index):
                indices_to_delete.append(file_info_index)
            if client.indices.exists(index=chunks_index):
                indices_to_delete.append(chunks_index)

            if indices_to_delete:
                client.indices.delete(index=",".join(indices_to_delete))
                logger.info(f"Deleted indices: {', '.join(indices_to_delete)}")
            else:
                logger.info(f"No indices found for '{store_name}' to delete")

            return True
        except Exception as e:
            logger.error(f"Failed to delete indices for '{store_name}': {str(e)}")
            raise VectorStoreError(f"Failed to delete indices: {e}")

    @staticmethod
    def close_index(index_name: str) -> bool:
        """Close index to protect from writes (useful before maintenance or deletion)."""
        client = get_elasticsearch_client()
        try:
            # Check if both sub-indexes exist
            file_info_index = ElasticsearchDDL._get_file_info_index_name(index_name)
            chunks_index = ElasticsearchDDL._get_chunks_index_name(index_name)

            if not client.indices.exists(index=file_info_index) or not client.indices.exists(index=chunks_index):
                msg = f"Cannot close index '{index_name}': one or more sub-indices do not exist."
                logger.error(msg)
                raise VectorStoreCreationError(msg)

            client.indices.close(index=file_info_index)
            client.indices.close(index=chunks_index)
            logger.info(f"Closed both indices of '{index_name}'.")
            return True

        except Exception as e:
            logger.error(f"Failed to close index '{index_name}': {str(e)}")
            raise VectorStoreCreationError(f"Failed to close index '{index_name}': {e}")

    @staticmethod
    def check_indices_exist(store_name: str) -> dict[str, bool]:
        client = get_elasticsearch_client()

        file_info_index = ElasticsearchDDL._get_file_info_index_name(store_name)
        chunks_index = ElasticsearchDDL._get_chunks_index_name(store_name)

        try:
            file_info_exists = bool(client.indices.exists(index=file_info_index))
            chunks_exists = bool(client.indices.exists(index=chunks_index))

            return {"file_info": file_info_exists, "chunks": chunks_exists, "both": file_info_exists and chunks_exists}
        except Exception as e:
            logger.error(f"Failed to check indices for '{store_name}': {str(e)}")
            return {"file_info": False, "chunks": False, "both": False}

    @staticmethod
    def open_index(index_name: str) -> bool:
        client = get_elasticsearch_client()
        try:
            file_info_index = ElasticsearchDDL._get_file_info_index_name(index_name)
            chunks_index = ElasticsearchDDL._get_chunks_index_name(index_name)

            if not client.indices.exists(index=file_info_index) or not client.indices.exists(index=chunks_index):
                msg = f"Cannot open index '{index_name}': one or more sub-indices do not exist."
                logger.error(msg)
                raise VectorStoreCreationError(msg)

            client.indices.open(index=file_info_index)
            client.indices.open(index=chunks_index)
            logger.info(f"Opened both indices of '{index_name}'.")
            return True

        except Exception as e:
            logger.error(f"Failed to open index '{index_name}': {str(e)}")
            raise VectorStoreCreationError(f"Failed to open index '{index_name}': {e}")

    @staticmethod
    def index_exists(index_name: str) -> bool:
        client = get_elasticsearch_client()
        try:
            file_info_index = ElasticsearchDDL._get_file_info_index_name(index_name)
            chunks_index = ElasticsearchDDL._get_chunks_index_name(index_name)

            file_exists = client.indices.exists(index=file_info_index)
            chunks_exists = client.indices.exists(index=chunks_index)

            if file_exists and chunks_exists:
                logger.debug(f"Both indices for '{index_name}' exist.")
                return True
            else:
                missing = []
                if not file_exists:
                    missing.append(file_info_index)
                if not chunks_exists:
                    missing.append(chunks_index)
                logger.warning(f"Missing indices for '{index_name}': {', '.join(missing)}")
                return False

        except Exception as e:
            logger.error(f"Error while checking existence of indices for '{index_name}': {e}")
            return False

>> /genai_platform_services/src/repository/elasticsearch_dml.py
from datetime import datetime
from typing import Any, Dict, List
from uuid import uuid4

from elasticsearch import helpers

from src.config import get_settings
from src.db.elasticsearch_connection import get_elasticsearch_client
from src.logging_config import Logger

logger = Logger.create_logger(__name__)
settings = get_settings()


class ElasticsearchDML:
    @staticmethod
    def select_one(index_name: str, query: dict) -> dict | None:
        client = get_elasticsearch_client()
        try:
            response = client.search(index=index_name, body=query, size=1)
            hits = response.get("hits", {}).get("hits", [])
            if hits:
                logger.info(f"Fetched 1 document from index '{index_name}'.")
                return hits[0]["_source"]  # type: ignore
            logger.info(f"No documents found in index '{index_name}' for query: {query}")
            return None
        except Exception as e:
            logger.error(f"Error in select_one from '{index_name}': {e}")
            raise

    @staticmethod
    def select_many(index_name: str, query: dict, size: int = 100) -> list[dict]:
        client = get_elasticsearch_client()
        try:
            response = client.search(index=index_name, body=query, size=size)
            hits = response.get("hits", {}).get("hits", [])
            docs = [hit["_source"] for hit in hits]
            logger.info(f"Fetched {len(docs)} documents from index '{index_name}'.")
            return docs
        except Exception as e:
            logger.error(f"Error in select_many from '{index_name}': {e}")
            raise

    @staticmethod
    def delete(index_name: str, doc_id: str | None = None, query: dict | None = None) -> dict:
        client = get_elasticsearch_client()
        try:
            if doc_id:
                # Single document delete
                response = client.delete(index=index_name, id=doc_id, ignore=[404])  # type: ignore
                result = response.get("result")

                if result == "deleted":
                    logger.info(f"Deleted document '{doc_id}' from index '{index_name}'.")
                    return {"deleted": 1, "found": True}

                elif result == "not_found":
                    logger.warning(f" Document '{doc_id}' not found in index '{index_name}'.")
                    return {"deleted": 0, "found": False}

                else:
                    logger.error(f"Unexpected response during delete by ID: {response}")
                    return {"deleted": 0, "found": None}

            elif query:
                # Delete by query
                response = client.delete_by_query(index=index_name, body=query, conflicts="proceed")
                deleted_count = response.get("deleted", 0)
                logger.info(f" Deleted {deleted_count} documents from index '{index_name}' via query.")
                return {"deleted": deleted_count}

            else:
                raise ValueError("Either 'doc_id' or 'query' must be provided for deletion.")

        except Exception as e:
            logger.error(f"Error deleting from index '{index_name}': {e}")
            raise

    @staticmethod
    def insert_chunk(
        index_name: str,
        documents: List[Dict[str, Any]],
        preserve_ids: bool = False,
    ) -> int:
        client = get_elasticsearch_client()
        if not documents:
            logger.warning(f"No documents provided for bulk insert into '{index_name}'.")
            return 0

        actions = []
        for doc in documents:
            doc_id = doc.get("chunk_id") if preserve_ids and "chunk_id" in doc else str(uuid4())
            doc_body = {
                "id": doc_id,
                "content": doc.get("content"),
                "embedding": doc.get("embedding"),
                "links": doc.get("links"),
                "topics": doc.get("topics"),
                "author": doc.get("author"),
                "meta_data": doc.get("meta_data"),
                "created_at": doc.get("created_at", datetime.now().isoformat()),
                "file_id": doc.get("file_id"),
                "file_name": doc.get("file_name"),
            }
            actions.append({"_index": index_name, "_id": doc_id, "_source": doc_body})

        try:
            success, failed = helpers.bulk(
                client,
                actions,
                chunk_size=settings.elasticsearch_bulk_chunk_size,
                raise_on_error=False,
            )
            logger.info(f"Indexed {success} documents to '{index_name}', {len(failed)} failed")  # type: ignore
            return success

        except ConnectionError as ce:
            logger.error(f"Connection failed during bulk insert into '{index_name}': {ce}")
            raise
        except Exception as e:
            logger.error(f"Unexpected bulk indexing error in '{index_name}': {e}")
            raise

>> /genai_platform_services/src/repository/vectorstore_ddl.py
from sqlalchemy import text
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy.orm import Session

from src.db.connection import create_session
from src.exception.exceptions import VectorStoreError


class VectorStoreDDL:
    @staticmethod
    def _info_table_name(store_name: str) -> str:
        return f"{store_name}_file_info"

    @staticmethod
    def _chunks_table_name(store_name: str) -> str:
        return f"{store_name}_chunks"

    @staticmethod
    def _index_name(table_name: str) -> str:
        return f"{table_name}_search_vector_index"

    @staticmethod
    def create_vectorstore_tables(tbl_name: str, dimensions: int) -> None:
        info_tbl = VectorStoreDDL._info_table_name(tbl_name)
        chunks_tbl = VectorStoreDDL._chunks_table_name(tbl_name)
        index_name = VectorStoreDDL._index_name(chunks_tbl)

        create_info_sql = f"""
        CREATE TABLE public."{info_tbl}" (
            vs_id UUID NOT NULL,
            file_id UUID NOT NULL,
            file_name VARCHAR(255) NOT NULL,
            file_version int NOT NULL,
            created_at TIMESTAMP NOT NULL DEFAULT NOW(),
            last_error TEXT,
            usage_bytes BIGINT NOT NULL DEFAULT 0,
            chunking_strategy VARCHAR(255) NOT NULL,
            metadata_vs JSONB,
            attributes JSONB,
            active BOOLEAN NOT NULL DEFAULT TRUE,
            status VARCHAR(32) NOT NULL,
            CONSTRAINT "{info_tbl}_pkey" PRIMARY KEY (vs_id, file_id)
        );
        """

        create_chunks_sql = f"""
        CREATE TABLE public."{chunks_tbl}" (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            file_id UUID NOT NULL,
            file_name VARCHAR(255) NOT NULL,
            embedding vector({dimensions}) NOT NULL,
            content TEXT NOT NULL,
            links _varchar,
            topics _varchar,
            author VARCHAR,
            meta_data JSONB,
            created_at TIMESTAMP NOT NULL DEFAULT NOW(),
            search_vector tsvector
        );
        """

        create_index_sql = f"""
        CREATE INDEX "{index_name}" ON public."{chunks_tbl}"
        USING GIN (search_vector);
        """

        try:
            with create_session() as session:
                with session.begin():
                    session.execute(text(create_info_sql))
                    session.execute(text(create_chunks_sql))
                    session.execute(text(create_index_sql))
        except SQLAlchemyError as exc:
            raise VectorStoreError(f"Failed to create vectorâ€‘store tables for '{tbl_name}': {exc}") from exc

    @staticmethod
    def create_tables_and_index(table_name: str, dimensions: int) -> None:
        return VectorStoreDDL.create_vectorstore_tables(
            tbl_name=table_name,
            dimensions=dimensions,
        )

    @staticmethod
    def drop_table_and_index(tbl_name: str, session: Session | None = None, if_exists: bool = True) -> bool:
        info_tbl = VectorStoreDDL._info_table_name(tbl_name)
        chunks_tbl = VectorStoreDDL._chunks_table_name(tbl_name)
        index_name = VectorStoreDDL._index_name(chunks_tbl)

        if if_exists:
            drop_index_sql = f"""DROP INDEX IF EXISTS public."{index_name}";"""
            drop_table_sql = f"""DROP TABLE IF EXISTS public."{info_tbl}", public."{chunks_tbl}" CASCADE;"""
        else:
            drop_index_sql = f"""DROP INDEX public."{index_name}";"""
            drop_table_sql = f"""DROP TABLE public."{info_tbl}", public."{chunks_tbl}" CASCADE;"""

        if session:
            session.execute(text(drop_index_sql))
            session.execute(text(drop_table_sql))
        else:
            with create_session() as new_session:
                new_session.execute(text(drop_index_sql))
                new_session.execute(text(drop_table_sql))
        return True

>> /genai_platform_services/src/models/storage_payload.py
from enum import Enum
from typing import Annotated, Any, Dict, List, Optional

from pydantic import BaseModel, Field, field_validator

from src.config import get_settings

settings = get_settings()


class SearchType(str, Enum):
    SEMANTIC = "semantic"
    FULL_TEXT = "full_text"
    HYBRID = "hybrid"


class StorageBackend(str, Enum):
    PGVECTOR = "pgvector"
    ELASTICSEARCH = "elasticsearch"


class Document(BaseModel):
    content: str = Field(..., description="The main textual content of the document.")
    links: Optional[List[str]] = Field(
        default=None, description="A URL or reference links associated with the document."
    )
    author: Optional[str] = Field(default=None, description="Name of the document's author or creator.")
    topics: Optional[List[str]] = Field(default=None, description="A list of topics that the document covers.")
    metadata: Annotated[
        Optional[dict], Field(default_factory=dict, description="Optional metadata like tags, file meta data etc.")
    ]


class SearchRequest(BaseModel):
    collection: str = Field(..., description="Name of collection to search in.")
    search_type: SearchType = Field(..., description="Type of search: semantic, full_text, or hybrid.")
    storage_backend: StorageBackend = Field(
        ...,
        description="Specifies the storage backend to use (e.g., PGVector, ElasticSearch). "
        "Currently, only PGVector is supported.",
    )
    search_text: str = Field(..., description="The query text for semantic search.")

    content_filter: Optional[list[str]] = Field(
        default=None, description="Include these keywords/terms at time of search"
    )

    link_filter: Optional[list[str]] = Field(
        default=None, description="Include these links at time of search(apply on link column)"
    )

    topic_filter: Optional[list[str]] = Field(
        default=None, description="Include these topics at time of search(apply on topic column)"
    )

    limit: int = Field(
        default=settings.default_document_limit,
        gt=0,
        description="Maximum number of results to return.",
    )

    min_score: float = Field(
        default=settings.min_similarity_score,
        gt=0,
        description="Minimum similarity score (for semantic and hybrid searches).",
    )
    use_ranking: Optional[bool] = Field(
        default=None,
        description="Whether to rank results (applies only to hybrid search).",
    )

    @field_validator("collection")
    def collection_must_not_be_empty(cls, v: str) -> str:
        if not v.strip():
            raise ValueError("The 'collection' field must not be empty.")
        return v

    @field_validator("search_text")
    def search_text_must_not_be_empty(cls, v: str) -> str:
        if not v.strip():
            raise ValueError("The 'search_text' field must not be empty.")
        return v


class SearchResult(BaseModel):
    id: str = Field(..., description="Unique identifier of the matched document.")
    score: Optional[float] = Field(default=None, description="Relevance score of the result.")
    source: Dict = Field(..., description="Original document data.")


class SearchResponse(BaseModel):
    results: List[SearchResult] = Field(..., description="List of matched search results.")
    total: int = Field(..., description="Total number of matched documents.")
    query_time_ms: Optional[float] = Field(None, description="Time taken to execute the search, in milliseconds.")


class DeleteRequest(BaseModel):
    storage_backend: StorageBackend = Field(
        ...,
        description="Specifies the storage backend to use (e.g., PGVector). Currently, only PGVector is supported.",
    )
    collection: str = Field(..., description="Name of the collection to delete.")

    @field_validator("collection")
    def collection_must_not_be_empty(cls, v: str) -> str:
        if not v.strip():
            raise ValueError("The 'collection' field must not be empty.")
        return v


class DeleteResponse(BaseModel):
    message: str = Field(..., description="Status message for the delete operation.")
    collection: str = Field(..., description="The name of the deleted collection.")


class DeleteByIdsRequest(BaseModel):
    storage_backend: StorageBackend = Field(
        ...,
        description="Specifies the storage backend to use (e.g., PGVector). Currently, only PGVector is supported.",
    )
    collection: str = Field(..., description="Name of the collection to delete.")
    index_ids: List[str] = Field(..., description="list of document IDs to delete")

    @field_validator("collection")
    def collection_must_not_be_empty(cls, v: str) -> str:
        if not v.strip():
            raise ValueError("The 'collection' field must not be empty.")
        return v


class Filter(BaseModel):
    column: str
    values: List[Any]


class DocumentSearchPayload(BaseModel):
    collection: str = Field(..., description="Name of the table or collection to search in.")
    search_type: SearchType = Field(..., description="Type of search: semantic, full_text, hybrid or sql.")
    storage_backend: StorageBackend = Field(
        ...,
        description="Specifies the storage backend to use (e.g., PGVector, ElasticSearch). Currently, only PGVector is supported.",
    )
    search_text: str = Field(..., description="The query text for semantic search.")
    embedding_column_name: Optional[str] = Field(
        default="embedding", description="The embedding column or the columnwhere the search should be performed"
    )
    filters: Optional[List[Filter]] = Field(
        default=None, description="filters to apply on the search result e.g.:{'column': 'abc123', 'values': [1, 2, 3]"
    )
    limit: Optional[int] = Field(
        default=settings.default_document_limit,
        description="Maximum number of results to return.",
    )

    min_score: Optional[float] = Field(
        default=settings.min_similarity_score,
        description="Minimum similarity score (for semantic and hybrid searches).",
    )
    use_ranking: Optional[bool] = Field(
        default=None,
        description="Whether to rank results (applies only to hybrid search).",
    )

>> /genai_platform_services/src/models/vector_store_payload.py
from enum import Enum
from typing import Annotated, Any, Dict, List, Optional, Union

from pydantic import BaseModel, Field, conlist, field_validator


class StorageBackend(str, Enum):
    PGVECTOR = "pgvector"
    ELASTICSEARCH = "elasticsearch"


class StaticChunkingStrategy(BaseModel):
    type: str = Field("static", description="Always 'static'")
    max_chunk_size_tokens: int = Field(..., ge=100, le=4096, description="Maximum tokens per chunk")
    chunk_overlap_tokens: int = Field(..., ge=0, le=2048, description="Tokens that overlap between chunks")


class AutoChunkingStrategy(BaseModel):
    type: str = Field("auto", description="Always 'auto'")


ChunkingStrategy = Union[AutoChunkingStrategy, StaticChunkingStrategy]


class ExpirationPolicy(BaseModel):
    anchor: str = Field("last_active_at", description="Timestamp anchor for expiration")
    days: int = Field(..., ge=0, description="Days after anchor when vector store expires")


class CreateVectorStoreRequest(BaseModel):
    name: str = Field(..., description="Name of the vector store")
    embedding_model: str = Field("BAAI/bge-m3", description="Embedding model name")
    storage_backend: StorageBackend = Field(
        ...,
        description="Specifies the storage backend to use (e.g., pgvector, " "elasticsearch",
    )
    metadata: Optional[Dict[str, str]] = Field(
        None,
        description="Additional metadata; set of 16 key-value pair; keys "
        "max length 64 characters, values max length 512 "
        "characters",
    )
    file_ids: Optional[List[str]] = Field(None, description="List of the file IDs to include")
    chunking_strategy: Optional[ChunkingStrategy] = Field(
        default_factory=lambda: AutoChunkingStrategy(), description="Chunking strategy - defaults to auto if omitted"
    )
    expires_after: Optional[ExpirationPolicy] = Field(
        default_factory=lambda: ExpirationPolicy(days=365),
        description="Expiration Policy - defaults to 1 year is omitted",
    )

    @field_validator("metadata")
    @classmethod
    def validate_metadata(cls, value: Optional[Dict[str, str]]) -> Optional[Dict[str, str]]:
        if value is None:
            return value

        # Check dictionary size
        if len(value) > 16:
            raise ValueError("metadata dictionary can have at most 16 key-value pairs.")

        # Check key and value lengths
        for key, val in value.items():
            if not isinstance(key, str) or len(key) > 64:
                raise ValueError("metadata keys must be strings with max length 64 characters.")
            if not isinstance(val, str) or len(val) > 512:
                raise ValueError("metadata values must be strings with max length 512 characters.")

        return value


class VectorStoreStatus(str, Enum):
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    EXPIRED = "expired"
    FAILED = "failed"


class FileCountsModel(BaseModel):
    in_progress: int = Field(0)
    completed: int = Field(0)
    failed: int = Field(0)
    cancelled: int = Field(0)
    total: int = Field(0)


class VectorStoreErrorDetails(BaseModel):
    code: str = Field(..., description="Error code: server_error or rate_limit_exceeded")
    message: str = Field(..., description="Error details")


class CreateVectorStoreResponse(BaseModel):
    id: str = Field(..., description="Vector store identifier")
    object: str = Field("vector_store", description="Object type")
    created_at: int = Field(..., description="Unix timestamp of creation")
    name: str = Field(..., description="Vector store name")
    usage_bytes: int = Field(0, description="Total bytes used by files")
    file_counts: FileCountsModel = Field(..., description="File processing counts")
    status: VectorStoreStatus = Field(..., description="Processing status")
    expires_after: Optional[ExpirationPolicy] = Field(None, description="Expiration policy")
    expires_at: Optional[int] = Field(None, description="Unix timestamp when store expires")
    last_active_at: Optional[int] = Field(None, description="Unix timestamp of last activity")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Additional metadata")
    last_error: Optional[VectorStoreErrorDetails] = Field(None, description="Last processing error")


class Document(BaseModel):
    content: str = Field(..., description="The main textual content of the document.")
    links: Optional[List[str]] = Field(
        default=None, description="A URL or reference links associated with the document."
    )
    author: Optional[str] = Field(default=None, description="Name of the document's author or creator.")
    topics: Optional[List[str]] = Field(default=None, description="A list of topics that the document covers.")
    metadata: Annotated[
        Optional[dict], Field(default_factory=dict, description="Optional metadata like tags, file meta data etc.")
    ]


class CreateVectorStoreFileRequest(BaseModel):
    storage_backend: StorageBackend = Field(
        ...,
        description="Specifies the storage backend to use (e.g., pgvector, " "elasticsearch",
    )
    file_id: str = Field(..., description="ID of file to add to vector store")
    file_name: str = Field(..., description="Name of file to add to vector store")
    file_contents: conlist(Document, min_length=1) = Field(  # type: ignore
        ..., description="A non-empty list of documents to be stored."
    )
    attributes: Optional[Dict[str, Any]] = Field(None, description="File attributes")
    chunking_strategy: Optional[ChunkingStrategy] = Field(
        default_factory=lambda: AutoChunkingStrategy(), description="Chunking strategy - defaults to auto if omitted"
    )


class FileStatus(str, Enum):
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    CANCELLED = "cancelled"
    FAILED = "failed"


class CreateVectorStoreFileResponse(BaseModel):
    id: Optional[str] = Field(None, description="File identifier")
    object: str = Field("vector_store.file", description="Object type")
    usage_bytes: int = Field(0, description="Bytes used by this file")
    created_at: int = Field(..., description="Unix timestamp of creation")
    vector_store_id: str = Field(..., description="ID of containing vector store")
    status: FileStatus = Field(..., description="File processing status")
    last_error: Optional[VectorStoreErrorDetails] = Field(None, description="Last processing error")
    attributes: Optional[Dict[str, Any]] = Field(None, description="File attributes")
    chunking_strategy: Optional[ChunkingStrategy] = Field(
        default_factory=lambda: AutoChunkingStrategy(), description="Chunking strategy - defaults to auto if omitted"
    )


class ListVectorStoresResponse(BaseModel):
    object: str = Field("list", description="Object type")
    data: List[CreateVectorStoreResponse] = Field(..., description="List of objects")
    first_id: Optional[str] = Field(None, description="ID of first object")
    last_id: Optional[str] = Field(None, description="ID of last object")
    has_more: bool = Field(False, description="Whether more objects available")


class DeleteVectorStoreResponse(BaseModel):
    id: str = Field(..., description="")
    object: str = Field(..., description="")
    deleted: bool = Field(..., description="")


class DeleteVectorStoreFileResponse(BaseModel):
    id: str = Field(..., description="")
    object: str = Field(..., description="")
    deleted: bool = Field(..., description="")


class ContentItem(BaseModel):
    type: str = Field(..., description="Type of content, e.g., 'text', 'embedding', etc.")
    text: str = Field(..., description="The actual content text or embedding vector.")


class AttributesItem(BaseModel):
    key: str = Field(..., description="Metadata key.")
    value: str = Field(..., description="Value of the metadata.")


class RetrieveFileResponse(BaseModel):
    file_id: str = Field(..., description="Unique identifier for the file.")
    filename: Optional[str] = Field(None, description="Name of the file.")
    attributes: List[AttributesItem] = Field(..., description="Metadata attributes for the file.")
    content: List[ContentItem] = Field(..., description="List of content segments within the file.")


class SearchType(str, Enum):
    SEMANTIC = "semantic"
    FULL_TEXT = "full_text"
    HYBRID = "hybrid"


class ComparisonFilter(BaseModel):
    key: str = Field(..., description="Attribute key to compare")
    type: str = Field(..., description="Comparison operator: eq, ne, gt, gte, lt, lte")
    value: Union[str, int, float, bool] = Field(..., description="Value to compare against")


class CompoundFilter(BaseModel):
    type: str = Field(..., description="Logic operator: and, or")
    filters: List[Union["ComparisonFilter", "CompoundFilter"]] = Field(..., description="Filters to combine")


# Forward reference resolution
CompoundFilter.model_rebuild()

SearchFilter = Union[ComparisonFilter, CompoundFilter]


class RankingOptions(BaseModel):
    ranker: str = Field("auto", description="Ranker type: auto or none")
    score_threshold: float = Field(0.0, ge=0.0, description="Minimum score threshold")


class SearchVectorStoreRequest(BaseModel):
    query: Union[str, List[str]] = Field(..., description="Search query string or strings")
    search_type: SearchType = Field(..., description="Type of search: semantic, full_text, or hybrid.")
    storage_backend: StorageBackend = Field(
        ...,
        description="Specifies the storage backend to use (e.g., pgvector, elasticsearch)",
    )
    filters: Optional[SearchFilter] = Field(None, description="Attribute-based filters")
    max_num_results: int = Field(10, ge=1, le=50, description="Maximum results to return")
    ranking_options: Optional[RankingOptions] = Field(None, description="Ranking configuration")
    rewrite_query: bool = Field(False, description="Whether to rewrite query for vector search")


class ContentBlock(BaseModel):
    type: str = Field("text", description="Content type")
    text: str = Field(..., description="Text content")


class SearchResult(BaseModel):
    id: Optional[str] = Field(..., description="ID of chunk")
    file_id: str = Field(..., description="ID of source file")
    filename: Optional[str] = Field(None, description="Original filename")
    score: float = Field(..., description="Relevance score")
    attributes: Optional[Dict[str, Any]] = Field(None, description="File attributes")
    content: List[ContentBlock] = Field(..., description="Content blocks")


class SearchVectorStoreResponse(BaseModel):
    object: str = Field("vector_store.search_results.page", description="Object type")
    search_query: str = Field(..., description="Original search query")
    data: List[SearchResult] = Field(..., description="Search results")
    has_more: bool = Field(False, description="Whether more results available")
    next_page: Optional[str] = Field(None, description="Next page cursor")

>> /genai_platform_services/src/models/headers.py
from pydantic import BaseModel, Field
from typing import Optional


class HeaderInformation(BaseModel):
    x_session_id: Optional[str] = Field(..., description="session id")
    x_base_api_key: str = Field(..., description="Universal api key")

    class Config:
        frozen = True


class InternalHeaderInformation(BaseModel):
    x_session_id: str = Field(..., description="session id")
    x_user_token: str = Field(..., description="User Token")

    class Config:
        frozen = True

>> /genai_platform_services/src/models/search_request.py
from enum import Enum
from typing import Annotated, List, Optional

from pydantic import BaseModel, Field

from src.config import get_settings

settings = get_settings()


class SearchType(str, Enum):
    SEMANTIC = "semantic"
    FULL_TEXT = "full_text"
    HYBRID = "hybrid"


class StorageBackend(str, Enum):
    PGVECTOR = "pgvector"
    ELASTICSEARCH = "elasticsearch"


class SearchRequest(BaseModel):
    collection: str = Field(..., description="Name of the table or collection to search in.")
    search_type: SearchType = Field(..., description="Type of search: semantic, full_text, or hybrid.")
    storage_backend: StorageBackend = Field(
        ...,
        description="Specifies the storage backend to use (e.g., PGVector, ElasticSearch). Currently, only PGVector is supported.",
    )
    search_text: str = Field(..., description="The query text for semantic search.")

    content_filter: Optional[list[str]] = Field(
        default=None, description="Include these keywords/terms at time of search"
    )

    link_filter: Optional[list[str]] = Field(
        default=None, description="Include these links at time of search(apply on link column)"
    )

    topic_filter: Optional[list[str]] = Field(
        default=None, description="Include these topics at time of search(apply on topic column)"
    )

    limit: Optional[int] = Field(
        default=settings.default_document_limit,
        description="Maximum number of results to return.",
    )

    min_score: Optional[float] = Field(
        default=settings.min_similarity_score,
        description="Minimum similarity score (for semantic and hybrid searches).",
    )
    use_ranking: Optional[bool] = Field(
        default=None,
        description="Whether to rank results (applies only to hybrid search).",
    )


class Document(BaseModel):
    content: str = Field(..., description="The main textual content of the document.")
    links: Optional[List[str]] = Field(
        default=None, description="A URL or reference links associated with the document."
    )
    author: Optional[str] = Field(default=None, description="Name of the document's author or creator.")
    topics: Optional[List[str]] = Field(default=None, description="A list of topics that the document covers.")
    metadata: Annotated[
        Optional[dict], Field(default_factory=dict, description="Optional metadata like tags, file meta data etc.")
    ]

>> /genai_platform_services/src/integrations/cloud_storage.py
from datetime import datetime, timezone
from typing import BinaryIO, List

import fsspec  # type: ignore[import-untyped]

from src.config import Settings, get_settings
from src.logging_config import Logger


class CloudStorage:
    def __init__(self, settings: Settings = get_settings()) -> None:
        self.cloud_provider = settings.cloud_service_provider
        self.logger = Logger.create_logger(__name__)

        if self.cloud_provider == "gcp":
            self.fs = fsspec.filesystem("gs")
            self.protocol = "gs"
        elif self.cloud_provider == "aws":
            self.fs = fsspec.filesystem("s3")
            self.protocol = "s3"
        else:
            raise ValueError("Invalid cloud_provider.  Must be 'gcp' or 'aws'.")

    def upload_object(self, file_obj: BinaryIO, bucket_name: str, object_name: str) -> str:
        try:
            cloud_path = f"{self.protocol}://{bucket_name}/{object_name}"
            with self.fs.open(cloud_path, "wb") as f:
                f.write(file_obj.read())

            if self.logger:
                self.logger.info(f"File uploaded to {cloud_path}")
            return cloud_path
        except Exception as e:
            if self.logger:
                self.logger.error(f"Error uploading file to {self.cloud_provider.upper()}: {str(e)}")
            raise

    def download_object(self, cloud_path: str) -> bytes:
        try:
            if not cloud_path.startswith(f"{self.protocol}://"):
                raise ValueError(f"Invalid path. It should start with '{self.protocol}://'.")

            with self.fs.open(cloud_path, "rb") as f:
                content: bytes = f.read()

            if self.logger:
                self.logger.info(f"Downloaded file from {cloud_path}")

            return content
        except Exception as e:
            if self.logger:
                self.logger.error(f"Error downloading file from {self.cloud_provider.upper()}: {str(e)}")
            raise

    def list_pdf_files(self, cloud_folder: str) -> List[str]:
        try:
            if not cloud_folder.startswith(f"{self.protocol}://"):
                raise ValueError(f"Invalid folder path. It should start with '{self.protocol}://'.")

            pdf_files = [f"{self.protocol}://{file}" for file in self.fs.glob(cloud_folder + "/*.pdf")]

            if self.logger:
                self.logger.info(f"Found {len(pdf_files)} PDF files in {cloud_folder}")

            return pdf_files

        except Exception as e:
            if self.logger:
                self.logger.error(f"Error listing PDF files from {self.cloud_provider.upper()}: {str(e)}")
            raise

    def move_to_archive(self, cloud_folder: str, archive_folder: str) -> List[str]:
        try:
            if not cloud_folder.startswith(f"{self.protocol}://"):
                raise ValueError(f"Invalid folder path. It should start with '{self.protocol}://'.")

            timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
            archive_folder_with_timestamp = f"{archive_folder.rstrip('/')}/{timestamp}"

            moved_files = []
            files_to_move = self.fs.glob(cloud_folder + "/*")

            for source_file in files_to_move:
                dest_file = f"{archive_folder_with_timestamp}/{source_file.split('/')[-1]}"
                self.fs.move(
                    f"{self.protocol}://{source_file}",
                    f"{self.protocol}://{dest_file}",
                )

                moved_files.append(f"{self.protocol}://{dest_file}")

                if self.logger:
                    self.logger.info(f"Moved {source_file} to archive: {dest_file}")

            return moved_files

        except Exception as e:
            if self.logger:
                self.logger.error(f"Error moving files in {self.cloud_provider.upper()}: {str(e)}")
            raise

>> /genai_platform_services/src/db/elasticsearch_connection.py
from contextlib import contextmanager
from typing import Generator, Optional

from elasticsearch import Elasticsearch

from src.config import get_settings
from src.logging_config import Logger

settings = get_settings()
logger = Logger.create_logger(__name__)


def create_elasticsearch_client() -> Elasticsearch:
    connection_params = {
        "hosts": [settings.elasticsearch_url],
        "request_timeout": settings.elasticsearch_timeout,
    }

    if settings.elasticsearch_use_ssl:
        connection_params["verify_certs"] = settings.elasticsearch_verify_certs
        if settings.elasticsearch_ca_certs:
            connection_params["ca_certs"] = settings.elasticsearch_ca_certs

    try:
        client = Elasticsearch(**connection_params)  # type: ignore

        if not client.ping():
            raise ConnectionError("Failed to ping Elasticsearch cluster")

        logger.info("Successfully connected to Elasticsearch cluster")
        return client

    except Exception as e:
        logger.error(f"Failed to connect to Elasticsearch: {str(e)}")
        raise


# Global Elasticsearch client (like your global engine)
elasticsearch_client: Optional[Elasticsearch] = None


def get_elasticsearch_client() -> Elasticsearch:
    global elasticsearch_client
    if elasticsearch_client is None:
        elasticsearch_client = create_elasticsearch_client()
    return elasticsearch_client


@contextmanager
def elasticsearch_context() -> Generator[Elasticsearch, None, None]:
    client = get_elasticsearch_client()
    try:
        yield client
    except Exception as e:
        logger.error(f"Elasticsearch operation error: {str(e)}")
        raise

>> /genai_platform_services/src/db/platform_meta_tables.py
import uuid

from sqlalchemy import BigInteger, Boolean, Column, DateTime, Integer, String
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy.sql import func

from src.db.base import BaseDBA


class CollectionInfo(BaseDBA):
    __tablename__ = "collection_info"
    collection_name = Column(String(64), nullable=False, primary_key=True)
    usecase_id = Column(String, nullable=False)
    model_name = Column(String, nullable=False)


class VectorStoreInfo(BaseDBA):
    __tablename__ = "vectorstore_info"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    name = Column(String(100), nullable=False)
    usecase_id = Column(String(100), nullable=False)
    model_name = Column(String(100), nullable=False)
    created_at = Column(DateTime(timezone=False), nullable=False, server_default=func.now())
    last_active_at = Column(DateTime(timezone=False))
    usage_bytes = Column(BigInteger, nullable=False, server_default="0")
    metadata_vs = Column(JSONB)
    expires_after = Column(JSONB)
    expires_at = Column(DateTime(timezone=False))
    file_counts = Column(JSONB)
    is_active = Column(Boolean, nullable=False, server_default="true")
    vector_db = Column(String)


class EmbeddingModels(BaseDBA):
    __tablename__ = "embedding_models"
    model_name = Column(String(255), nullable=False, primary_key=True)
    dimensions = Column(Integer, nullable=False)
    context_length: int = Column(Integer, nullable=False)  # type: ignore
    model_path = Column(String, nullable=False)
>> /genai_platform_services/src/db/connection.py
from contextlib import contextmanager
from typing import Any, Generator

from sqlalchemy import create_engine
from sqlalchemy.orm import Session, sessionmaker

from src.config import get_settings
from src.db.base import BaseDBA
from src.logging_config import Logger

settings = get_settings()
logger = Logger.create_logger(__name__)

ssl_args = {
    "sslcert": settings.database_ssl_cert,
    "sslkey": settings.database_ssl_key,
    "sslrootcert": settings.database_ssl_root_cert,
    "sslmode": settings.database_ssl_mode,
}
engine_args: dict[str, Any] = {
    "pool_size": 10,  # Number of connections to keep in pool
    "max_overflow": 5,  # Number of extra connections if pool exhausted
    "pool_timeout": 30,  # Wait up to 30 seconds for a connection
    "pool_recycle": 1800,  # Recycle connections every 30 minutes (to prevent stale connections)
    "pool_pre_ping": True,  # Test connections before using (avoids broken connections)
    "echo": True,
}
if settings.database_ssl_enabled:
    engine_args["connect_args"] = ssl_args
# Create the SQLAlchemy engine with connection pooling options
engine = create_engine(settings.database_url, **engine_args)
database_url_platform = settings.database_url.rsplit("/", 1)[0] + f"/{settings.platform_meta_db_name}"
engine_platform_meta_db = create_engine(database_url_platform, **engine_args)

# Create session factory
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
SessionLocal_PlatformMeta = sessionmaker(autocommit=False, autoflush=False, bind=engine_platform_meta_db)


# TODO future enhancement : make it Async
@contextmanager
def create_session() -> Generator[Session, None, None]:
    """Context manager for DB session."""
    session = SessionLocal()
    try:
        yield session
        session.commit()
    except Exception:
        session.rollback()
        raise
    finally:
        session.close()


@contextmanager
def create_session_platform() -> Generator[Session, None, None]:
    """Context manager for DB session."""
    session_platform = SessionLocal_PlatformMeta()
    try:
        yield session_platform
        session_platform.commit()
    except Exception:
        session_platform.rollback()
        raise
    finally:
        session_platform.close()


def initialize_platform_db() -> None:
    try:
        BaseDBA.metadata.create_all(bind=engine_platform_meta_db)
    except Exception as e:
        logger.error(f"Error initializing Platform Meta Tables: {str(e)}")


>> /genai_platform_services/src/db/base.py
from sqlalchemy.orm import DeclarativeBase


class BaseDBA(DeclarativeBase):
    pass

>> /genai_platform_services/src/api/deps.py
from functools import lru_cache
from typing import Annotated, Dict, Optional

import httpx
from fastapi import Depends, Header, HTTPException, Request, status
from fastapi.security.api_key import APIKeyHeader
from google.genai import Client
from openai import AsyncOpenAI
from slowapi import Limiter
from slowapi.util import get_remote_address

from src.config import get_settings
from src.exception.document_store_exception import UnsupportedStorageBackendError
from src.integrations.cloud_storage import CloudStorage
from src.integrations.open_ai_sdk import OpenAISdk
from src.integrations.redis_chatbot_memory import RedisShortTermMemory
from src.logging_config import Logger
from src.models.completion_payload_internal import ChatCompletionRequestInternal
from src.models.headers import HeaderInformation, InternalHeaderInformation
from src.models.vector_store_payload import StorageBackend
from src.repository.base_repository import BaseRepository
from src.repository.document_base_model import BaseModelOps
from src.services.collection_service import CollectionService
from src.services.embedding_model_service import EmbeddingsModelService
from src.services.embedding_service import EmbeddingService
from src.services.file_upload_service import FileUploadService
from src.services.genai_model_service import GenAIModelsService
from src.services.pdf_extraction_service import PDFExtractionService
from src.services.service_layer.chunking_service import ChunkingService
from src.services.service_layer.vector_store_service import VectorStoreService
from src.services.speech_services import SpeechService
from src.services.vertexai_conversation_service import VertexAIConversationService
from src.utility.file_io import FileIO
from src.utility.utils import get_apikey_and_validate_models

settings = get_settings()

logger = Logger.create_logger(__name__)


def get_rate_limiter() -> Limiter:
    limiter = Limiter(
        key_func=get_remote_address,
        storage_uri=f"rediss://:{settings.redis_auth_string}@{settings.redis_host}:{settings.redis_port}/0?"
        f"ssl_ca_certs={settings.ssl_ca_certs}",
    )
    return limiter


async def validate_headers_and_api_key(
    session_id: Optional[str] = Header(None, alias="x-session-id"),
    x_base_api_key: Optional[str] = Header(..., alias="x-base-api-key"),
) -> HeaderInformation:
    missing_headers = []
    if not session_id:
        # missing_headers.append("x-session-id")
        logger.info("Missing x-session-id")
    if not x_base_api_key:
        missing_headers.append("x-base-api-key")

    if missing_headers:
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=f"Missing required headers: {', '.join(missing_headers)}",
        )
    validation_url = f"{settings.base_api_url}{settings.api_key_verification_endpoint}"
    verify = False if settings.deployment_env != "PROD" else True
    async with httpx.AsyncClient(verify=verify) as client:
        response = await client.get(
            validation_url,
            params={"key": x_base_api_key},
            headers={"Authorization": f"Bearer {settings.master_api_key}"},
        )
    if response.status_code != 200:
        raise HTTPException(status_code=401, detail="Invalid or unauthorized API key")

    return HeaderInformation(x_session_id=session_id, x_base_api_key=x_base_api_key)


def build_openai_sdk(api_key: str) -> OpenAISdk:
    http_client = httpx.AsyncClient(http2=True, verify=settings.verify)
    openai_async_client = AsyncOpenAI(
        api_key=api_key,
        base_url=settings.base_api_url,
        http_client=http_client,
        max_retries=settings.max_retries,
    )
    return OpenAISdk(openai_async_client=openai_async_client)


@lru_cache()
def get_openai_service_internal() -> OpenAISdk:
    return build_openai_sdk(settings.default_litellm_api_key)


@lru_cache()
def get_openai_service(
    header_info: HeaderInformation = Depends(validate_headers_and_api_key),
) -> OpenAISdk:
    return build_openai_sdk(header_info.x_base_api_key)


@lru_cache()
def get_memory_client() -> RedisShortTermMemory:
    return RedisShortTermMemory(host=settings.redis_host, port=settings.redis_port)


@lru_cache()
def get_embedding_service(open_ai_sdk: OpenAISdk = Depends(get_openai_service)) -> EmbeddingService:
    return EmbeddingService(open_ai_sdk=open_ai_sdk)


@lru_cache()
def get_vertexai_service() -> VertexAIConversationService:
    client = Client(
        vertexai=True,
        project=settings.vertexai_project,
        location=settings.vertexai_location,
    )
    return VertexAIConversationService(client=client)


@lru_cache()
def get_file_io_service() -> FileIO:
    return FileIO()


@lru_cache()
def get_collection_service() -> CollectionService:
    return CollectionService(base_repository=BaseRepository(), base_model_ops=BaseModelOps())


@lru_cache()
def get_embeddings_model_service() -> EmbeddingsModelService:
    return EmbeddingsModelService()


@lru_cache()
def get_file_upload_service() -> FileUploadService:
    return FileUploadService()


user_token_header = APIKeyHeader(name="token", auto_error=False, scheme_name="TOKEN")


async def validate_request_user_token_params(
    session_id: Optional[str] = Header(default=None, alias="x-session-id"),
    x_user_token: Optional[str] = Header(..., alias="token"),
) -> InternalHeaderInformation:
    missing_header = []
    if not session_id:
        missing_header.append("x-session-id")
    if not x_user_token:
        missing_header.append("token")

    if missing_header:
        raise HTTPException(status_code=400, detail=f"Missing header(s): {', '.join(missing_header)}")

    return InternalHeaderInformation(x_session_id=session_id, x_user_token=x_user_token)


async def validate_user_token_api_call(
    session_id: Optional[str] = Header(default=None, alias="x-session-id"),
    x_user_token: str = Header(..., alias="token"),
) -> InternalHeaderInformation:
    missing_header = []
    if not session_id:
        missing_header.append("x-session-id")
    if not x_user_token:
        missing_header.append("token")
    if missing_header:
        raise HTTPException(status_code=400, detail=f"Missing header(s): {', '.join(missing_header)}")

    logger.info(f"Making request to {settings.validate_user_token_api} and token {x_user_token}")
    async with httpx.AsyncClient(verify=False) as client:
        key_response = await client.get(
            settings.validate_user_token_api,
            headers={"Authorization": f"{x_user_token}"},
        )
    if key_response.status_code != 200:
        raise HTTPException(status_code=401, detail="Invalid or unauthorized token")

    return InternalHeaderInformation(x_session_id=session_id, x_user_token=x_user_token)


def get_speech_service(file_io_service: FileIO = Depends(get_file_io_service)) -> SpeechService:
    return SpeechService(file_io_service)


@lru_cache()
def get_genai_model_service() -> GenAIModelsService:
    return GenAIModelsService()


def get_text_chunking_service() -> ChunkingService:
    return ChunkingService()


async def build_internal_openai_sdk(
    request: ChatCompletionRequestInternal,
    header_info: Annotated[InternalHeaderInformation, Depends(validate_user_token_api_call)],
) -> Dict[str, object]:
    valid_model_req, lite_llm_key = await get_apikey_and_validate_models(request.usecase_id, request.model_name)

    if not valid_model_req:
        logger.error(f"Requested model '{request.model_name}' not part of usecase {request.usecase_id}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Requested model '{request.model_name}' is not part of the usecase.",
        )

    open_ai_sdk = build_openai_sdk(lite_llm_key)

    return {
        "sdk": open_ai_sdk,
        "header_info": header_info,
        "api_key": lite_llm_key,
    }


@lru_cache()
def get_pdf_extraction_service() -> PDFExtractionService:
    return PDFExtractionService()


@lru_cache()
def get_cloud_storage_service() -> CloudStorage:
    return CloudStorage()


async def get_storage_backend(request: Request) -> StorageBackend:
    storage_backend = request.query_params.get("storage_backend")

    if not storage_backend and request.method in ("POST", "PUT", "PATCH", "DELETE", "GET"):
        try:
            body = await request.json()
            storage_backend = body.get("storage_backend")
        except Exception:
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Invalid JSON payload.")

    # If still missing â†’ fail
    if not storage_backend:
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY, detail="'storage_backend' must be supplied"
        )

    # Strict validation using Enum
    try:
        return StorageBackend(storage_backend)
    except ValueError:
        raise UnsupportedStorageBackendError(
            f"Invalid storage_backend '{storage_backend}'. " f"Allowed: {[b.value for b in StorageBackend]}"
        )


def get_vector_store_service(
    storage_backend: StorageBackend = Depends(get_storage_backend),
    embedding_service: EmbeddingService = Depends(get_embedding_service),
) -> VectorStoreService:
    backend = storage_backend or "pgvector"
    return VectorStoreService(backend_name=backend, embedding_service=embedding_service)

>> /genai_platform_services/src/api/routers/file_upload_router.py
import os
import shutil
import tempfile
import uuid
from typing import List, Optional

from fastapi import APIRouter, Depends, File, HTTPException, Query, UploadFile, status
from fastapi.responses import ORJSONResponse

from src import config
from src.api.deps import (
    get_cloud_storage_service,
    get_pdf_extraction_service,
    validate_headers_and_api_key,
)
from src.integrations.cloud_storage import CloudStorage
from src.logging_config import Logger
from src.models.headers import HeaderInformation
from src.services.pdf_extraction_service import PDFExtractionService

router = APIRouter()
logger = Logger.create_logger(__name__)
settings = config.get_settings()


@router.post(
    f"{settings.file_upload}",
    summary="Upload PDF files (optionally extract their contents).",
    description=(
        "Uploads PDF files to GCS with unique file IDs. "
        "If `extract=true` is passed, extracts their text and returns it inside the uploaded file info."
    ),
    response_description="Uploaded file info (with file_id).",
    response_class=ORJSONResponse,
    status_code=status.HTTP_200_OK,
)
async def upload_files(
    header_information: HeaderInformation = Depends(validate_headers_and_api_key),
    files: List[UploadFile] = File(...),
    vector_store_id: str = Query(..., description="The ID of the vector store to associate with uploaded files"),
    extract: Optional[bool] = Query(default=False, description="Extract PDF content after upload"),
    cloud_service: CloudStorage = Depends(get_cloud_storage_service),
    extraction_service: PDFExtractionService = Depends(get_pdf_extraction_service),
) -> ORJSONResponse:
    logger.info(
        f"Upload request (extract={extract}) from {header_information.x_session_id} "
        f"for vector_store_id={vector_store_id}"
    )

    if not files:
        raise HTTPException(status_code=400, detail="No files provided for upload.")

    temp_dir = tempfile.mkdtemp()
    uploaded_files = []
    failed_files = []

    try:
        for file in files:
            try:
                if not file.filename:
                    continue

                if file.content_type != "application/pdf":
                    failed_files.append({"file": file.filename, "error": "Invalid MIME type"})
                    continue

                # Generate unique file_id
                file_id = str(uuid.uuid4())

                # Save to a temporary local file
                local_path = os.path.join(temp_dir, file.filename)
                with open(local_path, "wb") as buffer:
                    shutil.copyfileobj(file.file, buffer)

                # Upload to GCS
                gcs_object_name = f"vector_store_files/{vector_store_id}/{file_id}_{file.filename}"
                with open(local_path, "rb") as binary_file:
                    gs_uri = cloud_service.upload_object(
                        binary_file,
                        bucket_name=settings.upload_bucket_name,
                        object_name=gcs_object_name,
                    )

                uploaded_files.append(
                    {
                        "file_id": file_id,
                        "file_name": file.filename,
                        "gcs_path": gs_uri,
                    }
                )

            except Exception as e:
                logger.exception(f"Error uploading {file.filename}: {e}")
                failed_files.append({"file": getattr(file, "filename", "unknown"), "error": str(e)})

        if not uploaded_files and failed_files:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to upload all files: {failed_files}",
            )

        # If extraction requested
        if extract and uploaded_files:
            try:
                gs_paths = [f["gcs_path"] for f in uploaded_files]
                extraction_results = extraction_service.extract_from_gcs(gs_paths)

                # Merge content into each uploaded file by order
                for idx, content in enumerate(extraction_results):
                    uploaded_files[idx]["content"] = content  # type: ignore
            except Exception as e:
                logger.exception(f"Extraction failed after upload: {e}")
                for file_info in uploaded_files:
                    file_info["content"] = None  # type: ignore

        response_data = {
            "vector_store_id": vector_store_id,
            "uploaded_files": uploaded_files,
            "failed_files": failed_files,
        }

        return ORJSONResponse(content=response_data, status_code=status.HTTP_200_OK)

    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"Unexpected upload failure: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Unexpected error: {str(e)}",
        )
    finally:
        shutil.rmtree(temp_dir, ignore_errors=True)

>> /genai_platform_services/src/api/routers/pdf_extraction_router.py
from typing import List

from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.responses import ORJSONResponse
from pydantic import BaseModel

from src import config
from src.api.deps import get_pdf_extraction_service, validate_headers_and_api_key
from src.logging_config import Logger
from src.models.headers import HeaderInformation
from src.services.pdf_extraction_service import PDFExtractionService

router = APIRouter()
logger = Logger.create_logger(__name__)
settings = config.get_settings()


class FileItem(BaseModel):
    file_id: str
    gcs_path: str


class ExtractionRequest(BaseModel):
    vector_store_id: str
    files: List[FileItem]


class ExtractedFile(BaseModel):
    file_id: str
    content: str


class ExtractionResponse(BaseModel):
    vector_store_id: str
    extracted_files: List[ExtractedFile]
    failed_files: List[dict]


@router.post(
    settings.pdf_extraction,
    summary="Extract text from PDFs (supports GCS).",
    description=(
        "Accepts a list of PDF file entries (with file_id, file_name, and gcs_path) "
        "and extracts their textual content."
    ),
    response_description="List of extracted text per file in a consistent schema.",
    response_class=ORJSONResponse,
    status_code=status.HTTP_200_OK,
)
async def extract_pdf_content(
    request: ExtractionRequest,
    header_information: HeaderInformation = Depends(validate_headers_and_api_key),
    pdf_service: PDFExtractionService = Depends(get_pdf_extraction_service),
) -> ORJSONResponse:
    logger.info(
        f"Extraction request for {len(request.files)} file(s) "
        f"by {header_information.x_session_id} under vector_store_id={request.vector_store_id}"
    )

    try:
        gs_paths = [file.gcs_path for file in request.files]
        extraction_results = pdf_service.extract_from_gcs(gs_paths)

        extracted_files = []
        failed_files = []

        for file_item, result in zip(request.files, extraction_results):
            # Case 1: Extraction failed
            if "error" in result:
                failed_files.append(
                    {
                        "file_id": file_item.file_id,
                        "error": result["error"],
                    }
                )
                continue

            # Case 2: Successful extraction
            content = result.get("extracted_text", "")
            extracted_files.append(
                {
                    "file_id": file_item.file_id,
                    "content": content,
                }
            )

        # If all failed
        if not extracted_files and failed_files:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="All file extractions failed. Check logs for details.",
            )

        response_data = {
            "vector_store_id": request.vector_store_id,
            "extracted_files": extracted_files,
            "failed_files": failed_files,
        }

        return ORJSONResponse(content=response_data, status_code=status.HTTP_200_OK)

    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"Unexpected error during extraction: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Unexpected error during extraction: {str(e)}",
        )

>> /genai_platform_services/src/api/routers/vector_store_files_router.py
from typing import Dict, Literal

from fastapi import APIRouter, Depends, HTTPException, Query, status

from src.api.deps import get_vector_store_service, validate_headers_and_api_key
from src.config import get_settings
from src.exception.document_store_exception import UnsupportedStorageBackendError
from src.exception.exceptions import DatabaseConnectionError
from src.logging_config import Logger
from src.models.headers import HeaderInformation
from src.models.vector_store_payload import (
    CreateVectorStoreFileRequest,
    CreateVectorStoreFileResponse,
    DeleteVectorStoreFileResponse,
    RetrieveFileResponse,
)
from src.services.service_layer.vector_store_service import VectorStoreService
from src.utility.vector_store_helpers import get_store_model_info, get_valid_usecase_id

router = APIRouter()
logger = Logger.create_logger(__name__)
settings = get_settings()


@router.post(
    f"{settings.vector_stores}/{{store_id}}/files",
    response_model=CreateVectorStoreFileResponse,
    summary="Add file to Vector Store",
    status_code=status.HTTP_200_OK,
)
async def create_vector_store_file(
    store_id: str,
    request: CreateVectorStoreFileRequest,
    header_information: HeaderInformation = Depends(validate_headers_and_api_key),
    usecase_id: str = Depends(get_valid_usecase_id),
    store_model_info: Dict[str, str] = Depends(get_store_model_info),
    vector_service: VectorStoreService = Depends(get_vector_store_service),
) -> CreateVectorStoreFileResponse:
    try:
        model_name: str = store_model_info["model_name"]
        model_path: str = store_model_info["model_path"]
        embedding_dimensions: int = int(store_model_info["embedding_dimensions"])
        context_length: int = int(store_model_info["context_length"])

        logger.info(f"Creating file in store '{store_id}' from {header_information.x_session_id}")
        logger.info(
            f"Indexing request for store '{store_id}' with {len(request.file_contents)} "
            f"documents using model '{model_name}'"
        )

        result = await vector_service.create_store_file(
            payload=request,
            store_id=store_id,
            usecase_id=usecase_id,
            model_name=model_name,
            context_length=context_length,
            model_path=model_path,
            embedding_dimensions=embedding_dimensions,
        )

        return CreateVectorStoreFileResponse.model_validate(result)

    except UnsupportedStorageBackendError as exc:
        logger.warning(f"Unsupported storage backend: {request.storage_backend}")
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=str(exc),
        )
    except Exception as exc:
        logger.exception(f"Unhandled exception in file creation: {str(exc)}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(exc))


@router.get(
    f"{settings.vector_stores}/{{vector_store_id}}/files/{{file_id}}/content",
    summary="Retrieves a vector store file (a single document) and its metadata",
    status_code=status.HTTP_200_OK,
)
async def retrieve_vector_store_file(
    vector_store_id: str,
    file_id: str,
    storage_backend: Literal["pgvector", "elasticsearch"] = Query(...),
    header_information: HeaderInformation = Depends(validate_headers_and_api_key),
    usecase_id: str = Depends(get_valid_usecase_id),
    vector_service: VectorStoreService = Depends(get_vector_store_service),
) -> RetrieveFileResponse:
    try:
        result = await vector_service.retrieve_file(vector_store_id, file_id, usecase_id)
        return RetrieveFileResponse.model_validate(result)

    except HTTPException as e:
        raise e
    except ConnectionError:
        raise
    except DatabaseConnectionError:
        raise
    except Exception as e:
        logger.exception("Error retrieving vector store file.")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e))


@router.delete(
    f"{settings.vector_stores}/{{vector_store_id}}/files/{{file_id}}",
    summary="Deletes a file (document) from the selected vector store backend (PGVector or Elasticsearch+GCP)",
    response_model=DeleteVectorStoreFileResponse,
    status_code=status.HTTP_200_OK,
)
async def delete_vector_store_file(
    vector_store_id: str,
    file_id: str,
    storage_backend: Literal["pgvector", "elasticsearch"] = Query(...),
    header_information: HeaderInformation = Depends(validate_headers_and_api_key),
    usecase_id: str = Depends(get_valid_usecase_id),
    vector_service: VectorStoreService = Depends(get_vector_store_service),
) -> DeleteVectorStoreFileResponse:
    """
    Deletes a vector store file (metadata, chunks, and GCP object)
    from the chosen backend using unified VectorStoreService.
    """
    try:
        result = await vector_service.delete_file(vector_store_id, file_id, usecase_id)
        return DeleteVectorStoreFileResponse.model_validate(result)
    except UnsupportedStorageBackendError as exc:
        logger.warning(f"Unsupported backend: {storage_backend}")
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=str(exc),
        )

    except DatabaseConnectionError as exc:
        logger.error("Database connection failed during file deletion.")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(exc))

    except Exception as e:
        logger.exception(f"Error deleting file {file_id} from vector store {vector_store_id}: {e}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e))

>> /genai_platform_services/src/api/routers/file_chunking_router.py
from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.responses import ORJSONResponse

from src.api.deps import get_text_chunking_service, validate_headers_and_api_key
from src.logging_config import Logger
from src.models.headers import HeaderInformation
from src.models.text_chunking_payload import ChunkTextRequest
from src.services.factory.chunking_factory import (
    ChunkingFactory,
    ChunkingStrategyNotFoundError,
)
from src.services.service_layer.chunking_service import ChunkingService

router = APIRouter()
logger = Logger.create_logger(__name__)


@router.get("/chunking-strategies")
async def list_chunking_strategies():  # type: ignore
    try:
        strategies = ChunkingFactory.list_strategies()
        return ORJSONResponse(status_code=status.HTTP_200_OK, content={"strategies": strategies})
    except Exception as e:
        logger.exception("Failed to list chunking strategies")
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/chunk-text",
    response_class=ORJSONResponse,
)
async def chunk_text(
    request: ChunkTextRequest,
    header_information: HeaderInformation = Depends(validate_headers_and_api_key),
    service: ChunkingService = Depends(get_text_chunking_service),
) -> ORJSONResponse:
    try:
        chunks = await service.chunk_text(
            strategy_name=request.criteria,
            text=request.input_text,
            chunk_size=request.chunk_size,
            overlap=request.overlap,
            separators=request.separators,
        )

        structured_chunks = [
            {
                "content": chunk,
                "links": [request.file_path],
                "topics": [],
                "author": "",
                "meta_data": {"file_path": request.file_path},
            }
            for chunk in chunks
        ]

        logger.info(f"Generated {len(structured_chunks)} chunks using strategy '{request.criteria}'")

        return ORJSONResponse(
            status_code=status.HTTP_200_OK,
            content={"chunks": structured_chunks, "total_chunks": len(structured_chunks)},
        )

    except ChunkingStrategyNotFoundError as e:
        logger.warning(f"Invalid chunking strategy requested: {e}")
        raise HTTPException(status_code=400, detail=str(e))

    except ValueError as e:
        logger.error(f"Validation error in chunking: {e}")
        raise HTTPException(status_code=400, detail=str(e))

    except Exception as e:
        logger.exception("Unexpected error during text chunking")
        raise HTTPException(status_code=500, detail=str(e))

>> /genai_platform_services/src/api/routers/vector_store_router.py
from typing import Dict, Literal, Optional

from fastapi import APIRouter, Depends, HTTPException, Query, status

from src.api.deps import (
    get_embedding_service,
    get_vector_store_service,
    validate_headers_and_api_key,
)
from src.config import get_settings
from src.db.platform_meta_tables import VectorStoreInfo
from src.exception.document_store_exception import (
    DocumentStoreSearchError,
    UnsupportedStorageBackendError,
)
from src.exception.exceptions import DatabaseConnectionError
from src.logging_config import Logger
from src.models.headers import HeaderInformation
from src.models.vector_store_payload import (
    CreateVectorStoreRequest,
    CreateVectorStoreResponse,
    DeleteVectorStoreResponse,
    ListVectorStoresResponse,
    SearchVectorStoreRequest,
    SearchVectorStoreResponse,
)
from src.repository.base_repository import BaseRepository
from src.repository.document_repository import DocumentRepository
from src.services.embedding_service import EmbeddingService
from src.services.service_layer.vector_store_service import VectorStoreService
from src.utility.vector_store_helpers import (
    get_store_model_info,
    get_valid_embedding_model,
    get_valid_usecase_id,
    validate_vector_store_name,
)

router = APIRouter()
logger = Logger.create_logger(__name__)
settings = get_settings()


@router.post(
    settings.vector_stores,
    response_model=CreateVectorStoreResponse,
    summary="Creates a Vector Store",
    status_code=status.HTTP_200_OK,
)
async def create_vector_store(
    request: CreateVectorStoreRequest,
    header_information: HeaderInformation = Depends(validate_headers_and_api_key),
    valid_store_name: str = Depends(validate_vector_store_name),
    usecase_id: str = Depends(get_valid_usecase_id),
    embedding_model_info: Dict[str, int] = Depends(get_valid_embedding_model),
    vector_service: VectorStoreService = Depends(get_vector_store_service),
) -> CreateVectorStoreResponse:
    try:
        logger.info(f"Creating store '{valid_store_name}' from {header_information.x_session_id}")

        result = await vector_service.create_store(
            payload=request, usecase_id=usecase_id, embedding_dimensions=embedding_model_info["embedding_dimensions"]
        )

        return CreateVectorStoreResponse.model_validate(result)

    except UnsupportedStorageBackendError as exc:
        logger.warning(f"Unsupported storage backend: {request.storage_backend}")
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=str(exc),
        )
    except DatabaseConnectionError:
        raise
    except Exception as exc:
        logger.exception(f"Unhandled exception in vector-store creation: {str(exc)}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(exc))


@router.get(
    settings.vector_stores,
    response_model=ListVectorStoresResponse,
    summary="Lists all Vector Stores",
    status_code=status.HTTP_200_OK,
)
async def list_vector_stores(
    limit: int = 50,
    after: Optional[str] = None,
    before: Optional[str] = None,
    order: Literal["asc", "desc"] = Query("desc"),
    storage_backend: Optional[Literal["pgvector", "elasticsearch"]] = Query(None),
    header_information: HeaderInformation = Depends(validate_headers_and_api_key),
    usecase_id: str = Depends(get_valid_usecase_id),
    vector_service: VectorStoreService = Depends(get_vector_store_service),
) -> ListVectorStoresResponse:
    try:
        # Use pgvector backend as default for listing (it queries from VectorStoreInfo table)
        # This table contains all stores regardless of backend
        raw_stores = await vector_service.list_stores(
            usecase_id=usecase_id,
            limit=limit + 1,
            after=after,
            before=before,
            order=order,
            vector_db=storage_backend,  # Pass storage_backend for filtering (None = all)
        )

        stores = raw_stores[:limit]
        has_more = len(raw_stores) > limit
        first_id = stores[0].id if len(stores) >= 1 else None
        last_id = stores[-1].id if len(stores) >= 1 else None

        return ListVectorStoresResponse(
            object="list", data=stores, first_id=first_id, last_id=last_id, has_more=has_more
        )

    except ConnectionError as e:
        raise e
    except Exception as e:
        logger.exception("Error listing vector stores.")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e))


async def search_vector_store(
    store_id: str,
    request: SearchVectorStoreRequest,
    header_information: HeaderInformation = Depends(validate_headers_and_api_key),
    store_model_info: dict = Depends(get_store_model_info),
    embedding_service: EmbeddingService = Depends(get_embedding_service),
) -> SearchVectorStoreResponse:
    """
    Unified search endpoint that detects the backend (PGVector / Elasticsearch)
    and executes the correct search strategy using the VectorStoreFactory.
    """
    try:
        logger.info(f"Search Request: {request} from session {header_information.x_session_id}")

        # Get store metadata (model info, backend type, etc.)
        model_name = store_model_info["model_name"]
        model_path = store_model_info["model_path"]
        context_length = store_model_info["context_length"]
        embedding_dims = store_model_info["embedding_dimensions"]

        # Look up the vector store info for this ID + backend
        row_data = BaseRepository.select_one(  # type: ignore
            db_tbl=VectorStoreInfo,
            filters={"id": store_id, "vector_db": request.storage_backend},
        )
        if not row_data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Vector store with Id '{store_id}' not found for backend '{request.storage_backend}'.",
            )

        store_name = row_data["name"]

        # Create the appropriate backend service using the factory
        document_repository = DocumentRepository(
            f"{store_name}_chunks",
            embedding_dimensions=embedding_dims,
        )
        service = VectorStoreService(
            backend_name=request.storage_backend,
            embedding_service=embedding_service,
            document_repository=document_repository,
        )
        # Execute search (shared base logic handles dispatch)
        response = await service.search(
            search_request=request,
            store_id=store_id,
            model_name=model_name,
            context_length=context_length,
            model_path=model_path,
        )

        return response

    except UnsupportedStorageBackendError as exc:
        logger.warning(f"Unsupported backend: {exc}")
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=str(exc),
        )

    except DocumentStoreSearchError as exc:
        logger.error(f"Search operation failed: {exc}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(exc))

    except Exception as e:
        logger.exception(f"Unexpected error in vector search for store_id={store_id}: {e}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e))


@router.delete(
    f"{settings.vector_stores}/{{vector_store_id}}",
    summary="Deletes a Vector Store along with it's metadata and content",
    status_code=status.HTTP_200_OK,
)
async def delete_vector_store(
    vector_store_id: str,
    storage_backend: Literal["pgvector", "elasticsearch"] = Query(...),
    header_information: HeaderInformation = Depends(validate_headers_and_api_key),
    usecase_id: str = Depends(get_valid_usecase_id),
    vector_service: VectorStoreService = Depends(get_vector_store_service),
) -> DeleteVectorStoreResponse:
    """
    Deletes a vector store from the chosen backend (pgvector or elasticsearch).
    Uses the new VectorStoreFactory + VectorStoreService architecture.
    """
    try:
        result = await vector_service.delete_store(store_id=vector_store_id, usecase_id=usecase_id)
        return DeleteVectorStoreResponse.model_validate(result)

    except UnsupportedStorageBackendError as exc:
        logger.warning(f"Unsupported storage backend: {storage_backend}")
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=str(exc),
        )

    except DatabaseConnectionError as exc:
        logger.error("Database connection failed during vector store deletion.")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(exc))

    except Exception as e:
        logger.exception(f"Error deleting vector store {vector_store_id}: {e}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e))

>> /genai_platform_services/src/services/file_upload_service.py
import base64
import datetime
import os
import subprocess
import tempfile
import traceback
import uuid
from io import BytesIO

from fastapi import HTTPException

from src.config import Settings, get_settings
from src.integrations.cloud_storage import CloudStorage
from src.logging_config import Logger
from src.models.upload_object_payload import UploadObjectPayload


class FileUploadService:
    def __init__(self, settings: Settings = get_settings()) -> None:
        self.storage_service = CloudStorage()
        self.logger = Logger.create_logger(__name__)
        self.settings = settings

    async def upload_object(self, payload: UploadObjectPayload) -> dict:
        temp_path = None
        try:
            # Decode the base64 file and prepare the file-like object
            try:
                decoded_file = base64.b64decode(payload.file_base64)
                file_like_object = BytesIO(decoded_file)
                file_like_object.seek(0)
            except Exception as e:
                self.logger.error(f"Base64 decoding failed: {str(e)}")
                raise HTTPException(status_code=400, detail="Invalid file format provided.")

            current_date = datetime.datetime.now()
            year = current_date.strftime("%Y")
            month = current_date.strftime("%m")
            day = current_date.strftime("%d")
            bucket_name = self.settings.upload_bucket_name
            folder_name = self.settings.upload_folder_name
            extension = payload.file_name.split(".")[1]
            full_path = f"{folder_name}/{payload.usecase_name}/{year}/{month}/{day}/{payload.file_name}"
            if extension in ["docx", "doc"]:
                full_path = (
                    f"{folder_name}/{payload.usecase_name}/{year}/{month}/{day}/{payload.file_name.split('.')[0]}.pdf"
                )
                temp_path = self.create_temp_file(file_like_object.getvalue(), payload.file_name)
                file_like_object = self.convert_to_pdf(file_like_object.getvalue(), temp_path)

            self.logger.info(f"Attempting to upload {payload.file_name} to {bucket_name}/{full_path}")
            response = self.storage_service.upload_object(file_like_object, bucket_name, full_path)
            unique_id = uuid.uuid4().hex
            self.logger.info(f"upload object : {response} , unique id : {unique_id}")
            return {"unique_id": unique_id, "object_path": response}
        except FileNotFoundError:
            self.logger.error("The specified file could not be found in the storage service.")
            raise HTTPException(status_code=404, detail="File not found.")
        except PermissionError:
            self.logger.error("Permission denied during file upload.")
            raise HTTPException(status_code=403, detail="Permission denied.")
        except Exception as e:
            self.logger.error(f"File upload failed: {str(e)}\nTraceback: {traceback.format_exc()}")
            raise HTTPException(status_code=500, detail=f"File upload failed: {str(e)}")
        finally:
            if temp_path:
                self.clean_temp_file(temp_path)

    def create_temp_file(self, file_content: bytes, file_name: str) -> str:
        # Create temp file with specified name
        temp_dir = tempfile.gettempdir()
        temp_path = os.path.join(temp_dir, file_name)

        try:
            # Write content to temp file
            with open(temp_path, "wb") as temp_file:
                temp_file.write(file_content)
            self.logger.info(f"Temp file creates for: {file_name} in {temp_path}")
            return temp_path
        except Exception as e:
            self.logger.error(f"Error creating temp file: {str(e)}\nTraceback: {traceback.format_exc()}")
            self.logger.error(f"Error While creating temp file : {file_name} in {temp_path} error:{str(e)}")
            # Clean up temp file if writing fails
            if os.path.exists(temp_path):
                os.unlink(temp_path)
            return ""

    def download_file(self, cloud_object_path: str) -> str:
        try:
            self.logger.info(f"Downloading file  {cloud_object_path}")
            file_content = self.storage_service.download_object(cloud_object_path)
            file_name = cloud_object_path.split("/")[-1]
            temp_path = self.create_temp_file(file_content, file_name)
            return temp_path
        except Exception as e:
            self.logger.error(f"Error creating temp file: {str(e)}\nTraceback: {traceback.format_exc()}")
            return ""

    def clean_temp_file(self, temp_path: str) -> None:
        """Delete temporary file."""
        if os.path.exists(temp_path):
            try:
                os.unlink(temp_path)
                self.logger.info(f"Cleaned up temp file: {temp_path}")
            except Exception as e:
                self.logger.error(f"Error cleaning temp file {temp_path}: {str(e)}")

    def convert_to_pdf(self, file_like_object: bytes, file_name: str) -> BytesIO:
        tempfile_path = self.create_temp_file(file_like_object, file_name)
        try:
            # Convert docx/doc to PDF using LibreOffice
            output_dir = os.path.dirname(tempfile_path)
            output_filename = os.path.splitext(file_name)[0] + ".pdf"
            output_path = os.path.join(output_dir, output_filename)

            # Run LibreOffice conversion command
            conversion_command = [
                "/usr/bin/libreoffice25.2",
                "--headless",
                "--convert-to",
                "pdf",
                "--outdir",
                output_dir,
                tempfile_path,
            ]

            process = subprocess.Popen(conversion_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            stdout, stderr = process.communicate()

            if process.returncode != 0:
                self.logger.error(f"PDF conversion failed: {stderr.decode()}")
                raise HTTPException(status_code=500, detail="Failed to convert document to PDF")

            # Update file details for upload
            file_name = output_filename
            with open(output_path, "rb") as pdf_file:
                file_obj = BytesIO(pdf_file.read())
                file_obj.seek(0)
            return file_obj
            # Cleanup temp files
            # self.delete_temp_file(tempfile_path)
            # self.delete_temp_file(output_path)

        except Exception as e:
            self.logger.error(f"Document conversion failed: {str(e)}")
            self.logger.error(f"Document conversion failed: {traceback.format_exc()}")
            if "tempfile_path" in locals():
                self.clean_temp_file(tempfile_path)
            if "output_path" in locals():
                self.clean_temp_file(output_path)
            raise HTTPException(status_code=500, detail=f"Document conversion failed: {str(e)}")

>> /genai_platform_services/src/services/tokenizer_service.py
import os
from urllib.parse import urlparse

from google.cloud import storage  # type: ignore

from src.logging_config import Logger
from tokenizers import Tokenizer  # type: ignore

logger = Logger.create_logger(__name__)


class TokenizerService:
    __models_cache__: dict[str, Tokenizer] = {}

    EMBEDDINGS_LOCAL_MODEL_DIR = "/tmp/tokenizers"
    LOCAL_FALLBACK_TOKENIZER_PATH = "src/tokenizers/tokenizers.json"

    @staticmethod
    def download_tokenizer_from_gcs(model_name: str, gcs_model_path: str) -> str:
        tokenizer_path = f"{gcs_model_path}/tokenizers.json"
        safe_model_name = model_name.replace("/", "_")
        local_path = os.path.join(TokenizerService.EMBEDDINGS_LOCAL_MODEL_DIR, f"{safe_model_name}.json")
        try:
            logger.info(f"Downloading tokenizers from {tokenizer_path}")
            client = storage.Client()
            bucket_name, model_path = TokenizerService.parse_gs_uri(tokenizer_path)
            bucket = client.bucket(bucket_name)
            blob = bucket.blob(model_path)
            os.makedirs(TokenizerService.EMBEDDINGS_LOCAL_MODEL_DIR, exist_ok=True)
            blob.download_to_filename(local_path)
            logger.info(f"Tokenizer successfully downloaded to {local_path}")
            return local_path
        except Exception as e:
            logger.warning(
                f"Failed to download tokenizers from GCS ({tokenizer_path}): {e}. "
                f"Falling back to local tokenizers: {TokenizerService.LOCAL_FALLBACK_TOKENIZER_PATH}"
            )
            if not os.path.exists(TokenizerService.LOCAL_FALLBACK_TOKENIZER_PATH):
                raise FileNotFoundError(
                    f"Fallback tokenizers not found at {TokenizerService.LOCAL_FALLBACK_TOKENIZER_PATH}"
                )
            return TokenizerService.LOCAL_FALLBACK_TOKENIZER_PATH

    @staticmethod
    def get_tokenizer(model_name: str, model_path: str) -> Tokenizer:
        if model_name in TokenizerService.__models_cache__:
            return TokenizerService.__models_cache__[model_name]
        else:
            local_path = os.path.join(TokenizerService.EMBEDDINGS_LOCAL_MODEL_DIR, f"{model_name}.json")

            if not os.path.exists(local_path):
                local_path = TokenizerService.download_tokenizer_from_gcs(model_name, gcs_model_path=model_path)

            tokenizer = Tokenizer.from_file(local_path)
            TokenizerService.__models_cache__[model_name] = tokenizer
            return tokenizer

    @staticmethod
    def get_token_count(model_name: str, text: str, model_path: str) -> int:
        tokenizer = TokenizerService.get_tokenizer(model_name, model_path)
        return len(tokenizer.encode(text).ids)

    @staticmethod
    def parse_gs_uri(gs_uri: str) -> tuple[str, str | None]:
        parsed = urlparse(gs_uri)

        if parsed.scheme != "gs":
            raise ValueError(f"Invalid scheme '{parsed.scheme}'. Expected 'gs://'")

        if parsed.netloc:
            bucket = parsed.netloc
            object_path = parsed.path.lstrip("/") or None
        else:
            full_path = parsed.path.lstrip("/")
            parts = full_path.split("/", 1)

            if not parts[0]:
                raise ValueError("Bucket name is missing in GCS URI.")

            bucket = parts[0]
            object_path = parts[1] if len(parts) > 1 else None

        return bucket, object_path

>> /genai_platform_services/src/services/text_chunking_service.py
import re
from typing import List, Optional


class TextChunker:
    def __init__(self, chunk_size: int = 500, overlap: int = 50, recursion_limit: int = 10):
        if overlap >= chunk_size:
            raise ValueError("`overlap` must be smaller than `chunk_size`.")

        self.chunk_size = chunk_size
        self.overlap = overlap
        self.recursion_limit = recursion_limit

    def recursive_chunk(
        self,
        text: str,
        separators: Optional[List[str]] = None,
        depth: int = 0,
    ) -> List[str]:
        if not text.strip():
            return []

        if separators is None:
            separators = ["\n\n", "\n", ". ", " "]

        # Base case: within chunk size or recursion too deep
        if len(text) <= self.chunk_size or depth >= self.recursion_limit:
            return [text.strip()]

        for sep in separators:
            parts = text.split(sep)
            if len(parts) == 1:
                continue  # Try smaller separators

            chunks = []
            current_parts = []
            current_length = 0

            for part in parts:
                part_len = len(part) + len(sep)
                if current_length + part_len <= self.chunk_size:
                    current_parts.append(part)
                    current_length += part_len
                else:
                    if current_parts:
                        chunks.append((sep.join(current_parts)).strip())
                    current_parts = [part]
                    current_length = len(part)

            if current_parts:
                chunks.append(sep.join(current_parts).strip())

            # Recursively handle any oversized chunks
            refined_chunks = []
            for chunk in chunks:
                if len(chunk) > self.chunk_size and len(separators) > 1:
                    refined_chunks.extend(self.recursive_chunk(chunk, separators[1:], depth + 1))
                else:
                    refined_chunks.append(chunk)

            # Apply overlap safely
            return self._apply_overlap(refined_chunks)

        # Fallback: fixed-size chunking
        return self.fixed_chunk(text)

    def fixed_chunk(self, text: str, separators: List[str] = None) -> List[str]:
        if not text.strip():
            return []

        if separators is None:
            separators = [".", "\n\n", " "]

        # Combine separators into regex pattern
        sep_pattern = "|".join(re.escape(sep) for sep in separators)
        sentences = re.split(f"({sep_pattern})", text)

        # Rebuild text with separators included properly
        combined = []
        current = ""
        for part in sentences:
            if not part:
                continue
            if len(current) + len(part) <= self.chunk_size:
                current += part
            else:
                combined.append(current.strip())
                current = part
        if current:
            combined.append(current.strip())

        # Apply overlap (word-level)
        overlapped_chunks = []
        for i, chunk in enumerate(combined):
            if i == 0:
                overlapped_chunks.append(chunk)
            else:
                prev_words = combined[i - 1].split()[-self.overlap :] if self.overlap > 0 else []
                current_chunk = " ".join(prev_words + chunk.split())
                overlapped_chunks.append(current_chunk.strip())

        return [c for c in overlapped_chunks if c]

    def _apply_overlap(self, chunks: List[str]) -> List[str]:
        """
        Apply overlap between consecutive chunks while avoiding duplicate text
        and mid-sentence cutoffs.

        Args:
            chunks (List[str]): Non-overlapping text chunks.

        Returns:
            List[str]: Overlapped chunks.
        """
        if not chunks:
            return []

        overlapped_chunks = [chunks[0]]

        for i in range(1, len(chunks)):
            prev_chunk = chunks[i - 1]
            curr_chunk = chunks[i]

            # Extract overlap region from previous chunk
            overlap_region = prev_chunk[-self.overlap :].strip()

            # Avoid repeating words if overlap already appears in the current chunk
            if curr_chunk.lower().startswith(overlap_region.lower()):
                # Perfect overlap already aligned â€” keep as-is
                merged = curr_chunk
            else:
                # Try aligning overlap intelligently (avoid partial word duplication)
                if " " in overlap_region:
                    # Trim incomplete word at start
                    overlap_region = overlap_region.split(" ", 1)[-1]
                merged = f"{overlap_region} {curr_chunk}".strip()

            overlapped_chunks.append(merged)

        return overlapped_chunks

>> /genai_platform_services/src/services/pdf_processing_service.py
from src.chunkers.recursive_chunker import RecursiveChunker
from src.models.storage_payload import Document
from src.services.abstract_document_store import AbstractDocumentStore


class PdfProcessingService:
    def __init__(self, document_store: AbstractDocumentStore, recursive_chunker: RecursiveChunker):
        self.document_store = document_store
        self.recursive_chunker = recursive_chunker

    async def process(self, files: list[str], collection: str) -> int:
        total_size = 0
        for file in files:
            chunks = self.recursive_chunker.chunk(file)
            file_name = file.split("/")[-1]
            documents = []
            for chunk in chunks:
                if chunk.page_content and len(chunk.page_content.strip()) > 0:
                    doc = Document(
                        content=chunk.page_content, links=[file_name], author=None, topics=[], metadata=chunk.metadata
                    )
                    total_size += 1
                    documents.append(doc)

            await self.document_store.index(documents, collection)
        return total_size

>> /genai_platform_services/src/services/base_class/vector_store_base.py
import asyncio
import functools as _functools
import json
from abc import ABC, abstractmethod
from datetime import datetime, timedelta
from typing import Any, Awaitable, Callable, Dict, List, Optional, Tuple, TypeVar, Union
from uuid import uuid4
from zoneinfo import ZoneInfo

from fastapi import HTTPException
from fastapi.concurrency import run_in_threadpool
from pydantic import BaseModel, Extra

from src.config import get_settings
from src.db.platform_meta_tables import VectorStoreInfo
from src.exception.document_store_exception import (
    DocumentMaxTokenLimitExceededError,
    DocumentStoreSearchError,
)
from src.exception.exceptions import VectorStoreError
from src.logging_config import Logger
from src.models.vector_store_payload import (
    AttributesItem,
    ContentItem,
    CreateVectorStoreFileRequest,
    CreateVectorStoreFileResponse,
    CreateVectorStoreRequest,
    CreateVectorStoreResponse,
    DeleteVectorStoreFileResponse,
    DeleteVectorStoreResponse,
    FileCountsModel,
    FileStatus,
    RetrieveFileResponse,
    SearchResult,
    SearchType,
    SearchVectorStoreRequest,
    SearchVectorStoreResponse,
    VectorStoreErrorDetails,
    VectorStoreStatus,
)
from src.repository.base_repository import BaseRepository
from src.services.tokenizer_service import TokenizerService
from src.utility.vector_store_utils import get_deepsize, payload_to_internal_format

logger = Logger.create_logger(__name__)
settings = get_settings()

# ---------------------------------------------------------------------
# Async wrapper helper
# ---------------------------------------------------------------------
F = TypeVar("F", bound=Callable[..., Any])


def ensure_async(func: F) -> Callable[..., Awaitable[Any]]:
    @_functools.wraps(func)
    async def wrapper(*args: Any, **kwargs: Any) -> Any:
        if asyncio.iscoroutinefunction(func):
            return await func(*args, **kwargs)
        return await run_in_threadpool(func, *args, **kwargs)

    return wrapper


# ---------------------------------------------------------------------
# Config Model
# ---------------------------------------------------------------------
class VectorStoreConfig(BaseModel):
    backend: str
    embedding_model: Optional[str] = None
    context_length: Optional[int] = settings.default_context_length
    embedding_dimensions: Optional[int] = settings.default_model_dimensions
    extra: Optional[Dict[str, Any]] = None

    class Config:
        extra = Extra.allow


# ---------------------------------------------------------------------
# BaseVectorStore (Template Method pattern)
# ---------------------------------------------------------------------
class BaseVectorStore(ABC):
    """
    Abstract base for all vector store backends (PGVector, Elasticsearch, etc.)
    Implements all CRUD orchestration, metadata handling, and error recovery.
    """

    def __init__(self, document_repository: Any = None, embedding_service: Any = None, settings: Any = None) -> None:
        self.document_repository = document_repository
        self.embedding_service = embedding_service
        self.settings = settings or get_settings()
        self.tokenizer_service = TokenizerService()

    # ================================================================
    # CRUD OPERATIONS
    # ================================================================

    @ensure_async
    async def create_store(
        self, payload: CreateVectorStoreRequest, usecase_id: str, embedding_dimensions: int
    ) -> CreateVectorStoreResponse:
        try:
            # Generate common metadata
            store_id, now_dt, expires_at = self._generate_store_metadata(payload)

            # Validate uniqueness
            await self._validate_store_uniqueness(payload.name, usecase_id)

            # Delegate to backend-specific implementation
            result = await self._create_backend_store(
                payload=payload,
                usecase_id=usecase_id,
                embedding_dimensions=embedding_dimensions,
                store_id=store_id,
                now_dt=now_dt,
                expires_at=expires_at,
            )

            return CreateVectorStoreResponse.model_validate(result)

        except VectorStoreError as e:
            return self._build_store_response(
                store_id="",
                name=getattr(payload, "name", ""),
                created_at=0,
                status=VectorStoreStatus.FAILED.value,
                last_active_at=0,
                vs_metadata=getattr(payload, "metadata", None),
                error_details=VectorStoreErrorDetails(code="server_error", message=str(e)),
            )
        except Exception as e:
            logger.exception(f"Create store failed: {e}")
            return self._build_store_response(
                store_id="",
                name=getattr(payload, "name", ""),
                created_at=0,
                status=VectorStoreStatus.FAILED.value,
                last_active_at=0,
                vs_metadata=getattr(payload, "metadata", None),
                error_details=VectorStoreErrorDetails(code="server_error", message=str(e)),
            )

    @ensure_async
    async def create_store_file(
        self,
        payload: CreateVectorStoreFileRequest,
        store_id: str,
        usecase_id: str,
        model_name: str,
        context_length: int,
        model_path: str,
        embedding_dimensions: int,
    ) -> CreateVectorStoreFileResponse:
        try:
            # Fetch and validate store metadata
            store_record = await self._fetch_store_metadata(store_id, payload.storage_backend, usecase_id)
            store_name = store_record["name"]

            # Delegate to backend-specific indexing
            data_size = await self._index_backend(
                payload, store_id, store_name, model_name, context_length, model_path, embedding_dimensions
            )

            # Update store stats
            await self._update_store_stats(store_record, data_size)

            return self._build_storefile_response(payload, data_size, store_id, FileStatus.COMPLETED.value)

        except DocumentMaxTokenLimitExceededError as e:
            logger.warning(f"Token limit exceeded: {e}")
            return self._build_storefile_response(
                payload,
                0,
                store_id,
                FileStatus.FAILED.value,
                VectorStoreErrorDetails(code="max_token_limit", message=str(e)),
            )
        except Exception as e:
            logger.exception(f"Indexing failed: {e}")
            return self._build_storefile_response(
                payload,
                0,
                store_id,
                FileStatus.FAILED.value,
                VectorStoreErrorDetails(code="server_error", message=str(e)),
            )

    @ensure_async
    async def retrieve_by_id(self, vectorstoreid: str, vectorstorefileid: str, usecase_id: str) -> RetrieveFileResponse:
        try:
            col = await self._fetch_metadata(vectorstoreid, usecase_id)
            await self._validate_backend_type(col)
            store_name = col["name"]

            file_info_record = await self._get_file_info(store_name, vectorstoreid, vectorstorefileid)
            chunk_records = await self._get_chunk_records(store_name, vectorstorefileid)

            if not file_info_record:
                raise VectorStoreError(f"File '{vectorstorefileid}' not found in vector store '{vectorstoreid}'")
            if not chunk_records:
                raise VectorStoreError(f"No chunks found for '{vectorstorefileid}' in '{vectorstoreid}'")

            text_chunks = [chunk.get("content", "") for chunk in chunk_records if chunk.get("content")]

            vector_store_file = {
                "file_id": file_info_record.get("file_id", ""),
                "filename": file_info_record.get("file_name", ""),
                "attributes": file_info_record.get("attributes", {}),
                "content": text_chunks,
            }

            return self._response_to_object_retrieve_file(vector_store_file)

        except VectorStoreError:
            raise
        except Exception as e:
            logger.exception(f"Unexpected error retrieving file '{vectorstorefileid}' from '{vectorstoreid}': {e}")

            return self._response_to_object_retrieve_file(
                {"file_id": vectorstorefileid, "filename": "", "attributes": {}, "content": []},
                exception=True,
            )

    @ensure_async
    async def list_stores(
        self,
        usecase_id: str,
        limit: int = 50,
        after: Optional[str] = None,
        before: Optional[str] = None,
        order: str = "desc",
        vector_db: Optional[str] = None,
    ) -> List[CreateVectorStoreResponse]:
        raw_stores = await self._list_backend_stores(usecase_id, limit + 1, after, before, order, vector_db)

        # Build paginated response
        stores = await self._build_list_response(raw_stores[:limit], limit, after, before)

        return stores

    @ensure_async
    async def delete(
        self, vector_id: str, usecase_id: str
    ) -> Union[DeleteVectorStoreResponse, DeleteVectorStoreFileResponse]:
        try:
            record = await self._fetch_metadata(vector_id, usecase_id)
            await self._validate_backend_type(record)  # abstract, backend-specific

            backup_record = dict(record)
            await self._delete_metadata(vector_id)

            try:
                await self._drop_backend_tables(record["name"])
            except Exception as ddl_err:
                logger.error(f"DDL failed for {record['name']}: {ddl_err}")
                self._restore_metadata_on_failure(backup_record)
                raise VectorStoreError(f"DDL failed for '{record['name']}', metadata restored.")

            return self._build_delete_response(vector_id, is_file=False, deleted=True)

        except Exception as e:
            logger.error(f"Delete failed: {e}")
            return self._build_delete_response(vector_id, is_file=False, deleted=False)

    @ensure_async
    async def delete_by_id(
        self, vectorstoreid: str, vectorstorefileid: str, usecase_id: str
    ) -> Union[DeleteVectorStoreResponse, DeleteVectorStoreFileResponse]:
        try:
            # Step 1: Fetch metadata (shared)
            record = await self._fetch_metadata(vectorstoreid, usecase_id)
            await self._validate_backend_type(record)
            store_name = record["name"]

            # Step 2: Delete metadata and chunks (backend-specific)
            file_info_record, chunk_records = await self._delete_metadata_and_chunks(
                store_name, vectorstoreid, vectorstorefileid
            )

            # Step 4: Update store-level metadata stats
            deleted_file_size = file_info_record.get("usage_bytes", 0)
            update_success = await self._update_vectorstore_stats_after_delete(vectorstoreid, deleted_file_size)
            if not update_success:
                logger.warning(f":: STATS :: Failed to update store stats after delete for '{vectorstoreid}'")

            return self._build_delete_response(vectorstorefileid, is_file=True, deleted=True)

        except Exception as e:
            logger.exception(f" Delete file failed: {e}")
            return self._build_delete_response(vectorstorefileid, is_file=True, deleted=False)

    async def search_vector_store(
        self,
        payload: SearchVectorStoreRequest,
        store_id: str,
        model_name: str,
        context_length: int,
        model_path: str,
    ) -> SearchVectorStoreResponse:
        col = BaseRepository.select_one(db_tbl=VectorStoreInfo, filters={"id": store_id})  # type: ignore
        if not col:
            raise HTTPException(status_code=404, detail=f"Vector store '{store_id}' not found")

        store_name = col["name"]
        chunks_index = f"{store_name}_chunks"

        internal_req = payload_to_internal_format(api_payload=payload, collection=chunks_index)

        logger.info(f"Executing {internal_req.search_type} search on {store_id}")

        return await self._execute_search(
            internal_req,
            chunks_index,
            model_name,
            context_length,
            model_path,
        )

    async def _execute_search(
        self,
        search_request: Any,
        index_name: str,
        model_name: str,
        context_length: int,
        model_path: str,
    ) -> SearchVectorStoreResponse:
        try:
            search_results: List[SearchResult] = []

            # Validate query length for semantic searches
            if search_request.search_type in (SearchType.SEMANTIC, SearchType.HYBRID):
                if getattr(search_request, "search_text", None):
                    self._content_length_validation(
                        context_length,
                        model_name,
                        model_path,
                        search_request.search_text,
                        "search",
                    )

            if search_request.search_type == SearchType.SEMANTIC:
                search_results = await self._semantic_search(search_request, index_name, model_name)
            elif search_request.search_type == SearchType.FULL_TEXT:
                search_results = await self._fulltext_search(search_request, index_name, model_name)
            elif search_request.search_type == SearchType.HYBRID:
                search_results = await self._hybrid_search(search_request, index_name, model_name)

            logger.info(f"Search returned {len(search_results)} results")

            return SearchVectorStoreResponse(
                search_query=search_request.search_text,
                data=search_results,
            )

        except DocumentMaxTokenLimitExceededError:
            raise
        except Exception as e:
            logger.exception(f"Search execution failed: {str(e)}")
            raise DocumentStoreSearchError(f"Search operation failed: {str(e)}")

    # ================================================================
    # SHARED HELPERS
    # ================================================================

    # =========== CREATE_VECTOR_STORE - HELPER FUNCTIONS =============
    def _generate_store_metadata(self, payload: CreateVectorStoreRequest) -> Tuple[str, datetime, Optional[datetime]]:
        store_id = str(uuid4())
        now_dt = datetime.now(ZoneInfo(self.settings.timezone))
        expires_at = None

        if payload.expires_after and payload.expires_after.days:
            expires_at = now_dt + timedelta(days=payload.expires_after.days)

        return store_id, now_dt, expires_at

    async def _validate_store_uniqueness(self, store_name: str, usecase_id: str) -> None:
        existing = BaseRepository.select_one(  # type: ignore
            db_tbl=VectorStoreInfo, filters={"name": store_name, "usecase_id": usecase_id}
        )

        if existing:
            raise VectorStoreError(
                f"Vector Store '{store_name}' already exists "
                f"(DB usecase: {existing['usecase_id']}, Request: {usecase_id})"
            )

    def _build_metadata_dict(
        self,
        store_id: str,
        payload: CreateVectorStoreRequest,
        usecase_id: str,
        now_dt: datetime,
        expires_at: Optional[datetime],
        backend_type: str,
    ) -> Dict[str, Any]:
        metadata: Dict[str, Any] = {
            "id": store_id,
            "name": payload.name,
            "usecase_id": usecase_id,
            "model_name": payload.embedding_model,
            "created_at": now_dt.strftime("%Y-%m-%d %H:%M:%S"),
            "last_active_at": now_dt.strftime("%Y-%m-%d %H:%M:%S"),
            "metadata_vs": payload.metadata or {},
            "expires_after": payload.expires_after.dict() if payload.expires_after else None,
            "file_counts": FileCountsModel().model_dump(),
            "vector_db": backend_type,
        }

        if expires_at:
            metadata["expires_at"] = expires_at.strftime("%Y-%m-%d %H:%M:%S")

        return metadata

    def _build_create_response_dict(
        self,
        store_id: str,
        payload: CreateVectorStoreRequest,
        now_dt: datetime,
        expires_at: Optional[datetime],
    ) -> Dict[str, Any]:
        return {
            "id": store_id,
            "object": "vector_store",
            "created_at": int(now_dt.timestamp()),
            "name": payload.name,
            "usage_bytes": 0,
            "file_counts": FileCountsModel().model_dump(),
            "status": VectorStoreStatus.COMPLETED.value,
            "expires_after": payload.expires_after.dict() if payload.expires_after else None,
            "expires_at": int(expires_at.timestamp()) if expires_at else None,
            "last_active_at": int(now_dt.timestamp()),
            "metadata": payload.metadata,
            "last_error": None,
        }

    # ========= CREATE_VECTOR_STORE_FILE - HELPER FUNCTIONS ==========
    async def _fetch_store_metadata(self, store_id: str, storage_backend: str, usecase_id: str) -> Dict[str, Any]:
        store_record = BaseRepository.select_one(  # type: ignore
            db_tbl=VectorStoreInfo, filters={"id": store_id, "vector_db": storage_backend}
        )

        if not store_record:
            raise VectorStoreError(
                f"Either Vector store with Id '{store_id}' does not exist " f"or not found with '{storage_backend}'."
            )

        if store_record["usecase_id"] != usecase_id:
            raise VectorStoreError(f"Access denied for Vector Store '{store_id}'")

        return store_record

    async def prepare_index_emdeddings(
        self,
        payload: CreateVectorStoreFileRequest,
        model_name: str,
        model_path: str,
        context_length: int,
        embedding_service: Any,
        timezone: str,
    ) -> Tuple[List[Dict[str, Any]], int, Dict[str, Any]]:
        content_list: List[str] = []
        for document in payload.file_contents:
            self._content_length_validation(context_length, model_name, model_path, document.content, "index")
            content_list.append(document.content)

        embeddings = await embedding_service.get_embeddings(model_name=model_name, batch=content_list)

        docs_with_embeddings: List[Dict[str, Any]] = []
        for i, doc in enumerate(payload.file_contents):
            docs_with_embeddings.append(
                {
                    "content": doc.content,
                    "embedding": embeddings.data[i].embedding,
                    "links": doc.links,
                    "meta_data": json.dumps(doc.metadata, ensure_ascii=False) if doc.metadata else None,
                    "topics": doc.topics,
                    "author": doc.author,
                }
            )

        data_size = get_deepsize(docs_with_embeddings)

        now_dt = datetime.now(ZoneInfo(timezone))

        common_file_info: Dict[str, Any] = {
            "file_id": payload.file_id,
            "file_name": payload.file_name,
            "file_version": 1,
            "created_at": now_dt.strftime("%Y-%m-%d %H:%M:%S"),
            "usage_bytes": data_size,
            "chunking_strategy": json.dumps(payload.chunking_strategy.dict())  # type: ignore
            if getattr(payload, "chunking_strategy", None)
            else None,
            "attributes": payload.attributes or {},
            "status": FileStatus.COMPLETED.value,
        }

        return docs_with_embeddings, data_size, common_file_info

    async def _update_store_stats(self, store_record: Dict[str, Any], file_size: int, increment: bool = True) -> bool:
        try:
            # Update file counts
            current_file_counts = store_record.get("file_counts", {}) or FileCountsModel().model_dump()
            updated_file_counts = current_file_counts.copy()

            # Update usage and timestamp
            now_dt = datetime.now(ZoneInfo(self.settings.timezone))

            if increment:
                updated_file_counts["completed"] = updated_file_counts.get("completed", 0) + 1
                updated_file_counts["total"] = updated_file_counts.get("total", 0) + 1
                new_usage = (store_record.get("usage_bytes", 0) or 0) + file_size
            else:
                updated_file_counts["completed"] = max(updated_file_counts.get("completed", 0) - 1, 0)
                updated_file_counts["total"] = max(updated_file_counts.get("total", 0) - 1, 0)
                new_usage = max((store_record.get("usage_bytes", 0) or 0) - file_size, 0)

            update_data: Dict[str, Any] = {
                "last_active_at": now_dt.strftime("%Y-%m-%d %H:%M:%S"),
                "usage_bytes": new_usage,
                "file_counts": updated_file_counts,
            }

            BaseRepository.update_many(  # type: ignore
                db_tbl=VectorStoreInfo,
                filters={"id": store_record["id"]},
                data=update_data,
            )

            logger.info(f"Updated store stats for '{store_record['id']}': +{file_size} bytes")
            return True

        except Exception as e:
            logger.error(f"Failed to update store stats for '{store_record['id']}': {e}", exc_info=True)
            return False

    # ================= LIST_STORES - HELPER METHODS =================
    async def _build_list_response(
        self, stores: List[Dict[str, Any]], limit: int, after: Optional[str], before: Optional[str]
    ) -> List[CreateVectorStoreResponse]:
        response_stores: List[CreateVectorStoreResponse] = []

        for item in stores:
            created_at = int(item["created_at"].timestamp()) if item.get("created_at") else 0
            last_active_at = int(item["last_active_at"].timestamp()) if item.get("last_active_at") else 0

            response_stores.append(
                CreateVectorStoreResponse(
                    id=str(item["id"]),
                    created_at=created_at,
                    name=item["name"],
                    usage_bytes=item.get("usage_bytes", 0),
                    file_counts=item.get("file_counts", FileCountsModel()).copy()
                    if item.get("file_counts")
                    else FileCountsModel(),
                    status=VectorStoreStatus.COMPLETED.value,
                    expires_after=item.get("expires_after"),
                    last_active_at=last_active_at,
                    metadata=item.get("metadata_vs"),
                )
            )

        # Handle pagination
        start_index = 0
        end_index = len(response_stores)

        if after:
            after_indices = [i for i, store in enumerate(response_stores) if store.id == after]
            if after_indices:
                start_index = after_indices[0] + 1

        if before:
            before_indices = [i for i, store in enumerate(response_stores) if store.id == before]
            if before_indices:
                end_index = before_indices[0]
                start_index = max(0, end_index - limit)
                return response_stores[start_index:end_index]

        return response_stores[start_index : start_index + limit]

    async def _fetch_metadata(self, vector_id: str, usecase_id: str) -> Dict[str, Any]:
        from src.db.platform_meta_tables import VectorStoreInfo
        from src.repository.base_repository import BaseRepository

        col = BaseRepository.select_one(db_tbl=VectorStoreInfo, filters={"id": vector_id})  # type: ignore
        if not col:
            raise VectorStoreError(f"Vector Store '{vector_id}' does not exist")
        if col["usecase_id"] != usecase_id:
            raise VectorStoreError(f"Access denied for Vector Store '{vector_id}'")
        return col

    async def _delete_metadata(self, vector_id: str) -> None:
        from src.db.platform_meta_tables import VectorStoreInfo
        from src.repository.base_repository import BaseRepository

        BaseRepository.delete(db_tbl=VectorStoreInfo, filters={"id": vector_id})  # type: ignore
        logger.info(f"Deleted metadata for vector store '{vector_id}'")

    def _restore_metadata_on_failure(self, backup_record: Dict[str, Any]) -> None:
        from src.db.platform_meta_tables import VectorStoreInfo
        from src.repository.base_repository import BaseRepository

        BaseRepository.insert_one(db_tbl=VectorStoreInfo, data=backup_record)  # type: ignore
        logger.info(f"Restored metadata for failed DDL rollback: {backup_record['name']}")

    async def _update_vectorstore_stats_after_delete(self, vectorstoreid: str, deleted_file_size: int) -> bool:
        try:
            current_vs = BaseRepository.select_one(db_tbl=VectorStoreInfo, filters={"id": vectorstoreid})  # type: ignore
            if not current_vs:
                logger.warning(f":: STATS :: No vector store found for id '{vectorstoreid}'")
                return False

            # Adjust file counts safely
            current_file_counts = current_vs.get("file_counts", {}) or FileCountsModel().model_dump()
            updated_file_counts = current_file_counts.copy()
            updated_file_counts["completed"] = max(updated_file_counts.get("completed", 0) - 1, 0)
            updated_file_counts["total"] = max(updated_file_counts.get("total", 0) - 1, 0)

            # Compute new usage and timestamps
            now_dt = datetime.now(ZoneInfo(self.settings.timezone))

            new_usage = max(current_vs.get("usage_bytes", 0) - deleted_file_size, 0)

            update_data: Dict[str, Any] = {
                "last_active_at": now_dt.strftime("%Y-%m-%d %H:%M:%S"),
                "usage_bytes": new_usage,
                "file_counts": updated_file_counts,
            }

            BaseRepository.update_many(  # type: ignore
                db_tbl=VectorStoreInfo,
                filters={"id": vectorstoreid},
                data=update_data,
            )

            logger.info(f":: STATS :: Updated metadata for vectorstore '{vectorstoreid}': {update_data}")
            return True

        except Exception as e:
            logger.error(f":: STATS :: Failed to update stats for '{vectorstoreid}': {e}", exc_info=True)
            return False

    def _content_length_validation(
        self, context_length: int, model_name: str, model_path: str, text: str, validation_for: str
    ) -> None:
        index_or_search = "content" if validation_for == "index" else "search text"
        token_count = self.tokenizer_service.get_token_count(model_name, text, model_path)

        if token_count > context_length:
            raise DocumentMaxTokenLimitExceededError(
                f"Document exceeds maximum {index_or_search} length: "
                f"max {context_length} tokens, received {token_count} tokens."
            )

    # ================================================================
    # ABSTRACT BACKEND METHODS
    # ================================================================
    @abstractmethod
    async def _create_backend_store(
        self,
        payload: CreateVectorStoreRequest,
        usecase_id: str,
        embedding_dimensions: int,
        store_id: str,
        now_dt: datetime,
        expires_at: Optional[datetime],
    ) -> Dict[str, Any]:
        """Create backend store and return response dict"""
        ...

    @abstractmethod
    async def _validate_backend_type(self, record: Dict[str, Any]) -> None:
        """Ensures backend type matches the store type (e.g., pgvector / elasticsearch)."""
        ...

    @abstractmethod
    async def _drop_backend_tables(self, store_name: str) -> None:
        """Drops backend tables/indices"""
        ...

    @abstractmethod
    async def _delete_metadata_and_chunks(
        self, store_name: str, vectorstoreid: str, vectorstorefileid: str
    ) -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:
        """
        Delete metadata and chunks from backend.
        Returns: (file_info_record, chunk_records)
        """
        ...

    @abstractmethod
    async def _index_backend(
        self,
        payload: CreateVectorStoreFileRequest,
        store_id: str,
        store_name: str,
        model_name: str,
        context_length: int,
        model_path: str,
        embedding_dimensions: int,
    ) -> int:
        """
        Index documents to backend.
        Returns: data_size in bytes
        """
        ...

    @abstractmethod
    async def _list_backend_stores(
        self,
        usecase_id: str,
        limit: int,
        after: Optional[str],
        before: Optional[str],
        order: str,
        vector_db: Optional[str],
    ) -> List[Dict[str, Any]]:
        """List stores from backend"""
        ...

    @abstractmethod
    async def _semantic_search(self, search_request: Any, index_name: str, model_name: str) -> List[SearchResult]:
        """Semantic similarity search using embeddings"""
        ...

    @abstractmethod
    async def _fulltext_search(self, search_request: Any, index_name: str, model_name: str) -> List[SearchResult]:
        """Full-text BM25 search"""
        ...

    @abstractmethod
    async def _hybrid_search(self, search_request: Any, index_name: str, model_name: str) -> List[SearchResult]:
        """Hybrid search combining semantic + fulltext"""
        ...

    @abstractmethod
    async def _get_file_info(self, store_name: str, vectorstoreid: str, vectorstorefileid: str) -> Optional[dict]:
        """
        Backend-specific implementation for fetching a file info record.
        PG: Fetch from table <store_name>_file_info
        ES: Fetch from index <store_name>_file_info
        """
        ...

    @abstractmethod
    async def _get_chunk_records(self, store_name: str, vectorstorefileid: str) -> List[dict]:
        """
        Backend-specific implementation for fetching file chunk records.
        PG: Fetch from table <store_name>_chunks
        ES: Fetch from index <store_name>_chunks
        """
        ...

    # ================================================================
    # RESPONSE BUILDERS
    # ================================================================
    def _build_store_response(
        self,
        store_id: str,
        name: str,
        created_at: int,
        status: str,
        last_active_at: int,
        vs_metadata: Optional[Dict[str, Any]],
        error_details: Optional[VectorStoreErrorDetails],
    ) -> CreateVectorStoreResponse:
        return CreateVectorStoreResponse(
            id=store_id,
            name=name,
            object="vector_store",
            created_at=created_at,
            last_active_at=last_active_at,
            status=status,
            metadata=vs_metadata,
            file_counts={"total": 0},
            expires_at=None,
            expires_after=None,
            last_error=error_details,
        )

    def _build_storefile_response(
        self,
        payload: CreateVectorStoreFileRequest,
        data_size: int,
        store_id: str,
        status: str,
        error_details: Optional[VectorStoreErrorDetails] = None,
    ) -> CreateVectorStoreFileResponse:
        return CreateVectorStoreFileResponse(
            id=getattr(payload, "file_id", None),
            object="vector_store.file",
            usage_bytes=data_size,
            created_at=0,
            vector_store_id=store_id,
            status=status,
            attributes=getattr(payload, "attributes", None),
            chunking_strategy=getattr(payload, "chunking_strategy", None),
            last_error=error_details,
        )

    def _build_delete_response(
        self, id_: str, is_file: bool = False, deleted: bool = True
    ) -> Union[DeleteVectorStoreResponse, DeleteVectorStoreFileResponse]:
        obj = "vector_store.file.deleted" if is_file else "vector_store.deleted"
        resp_cls = DeleteVectorStoreFileResponse if is_file else DeleteVectorStoreResponse
        return resp_cls(id=id_, object=obj, deleted=deleted)

    def _response_to_object_retrieve_file(
        self,
        vector_store_file: Dict[str, Any],
        exception: bool = False,
    ) -> RetrieveFileResponse:
        """Convert raw Elasticsearch records into a standardized RetrieveFileResponse."""
        if exception or not vector_store_file:
            return RetrieveFileResponse(
                file_id=str(vector_store_file.get("file_id", "")) if vector_store_file else "",
                filename="",
                attributes=[],
                content=[],
            )

        # Normalize attributes
        attributes: List[AttributesItem] = []
        raw_attrs = vector_store_file.get("attributes", {})
        if isinstance(raw_attrs, dict):
            for key, value in raw_attrs.items():
                attributes.append(AttributesItem(key=key, value=str(value)))
        elif isinstance(raw_attrs, list):
            for attr in raw_attrs:
                if isinstance(attr, dict):
                    attributes.append(AttributesItem(**attr))

        # Normalize text chunks
        content_items: List[ContentItem] = []
        raw_content = vector_store_file.get("content", [])
        for text_segment in raw_content:
            if isinstance(text_segment, str) and text_segment.strip():
                content_items.append(ContentItem(type="text", text=text_segment.strip()))

        # Deduplicate while preserving order (optional but helpful)
        seen = set()
        unique_content: List[ContentItem] = []
        for item in content_items:
            if item.text not in seen:
                unique_content.append(item)
                seen.add(item.text)

        return RetrieveFileResponse(
            file_id=str(vector_store_file.get("file_id", "")),
            filename=str(vector_store_file.get("filename", "")),
            attributes=attributes,
            content=unique_content,
        )

>> /genai_platform_services/src/services/strategies/chunking_strategies.py
import re
from typing import List, Optional

from src.services.factory.chunking_factory import BaseChunkingStrategy, ChunkingFactory


@ChunkingFactory.register(
    "recursive",
    description="Recursively splits text using a list of separators until chunk_size is respected.",
    tags=["rule-based", "recursive"],
)
class RecursiveChunkingStrategy(BaseChunkingStrategy):
    recursion_limit: int = 10

    async def _chunk(self, text: str, separators: Optional[List[str]]) -> List[str]:
        if not text.strip():
            return []

        if not separators:
            separators = ["\n\n", "\n", ". ", " "]

        return self._recursive_split(text, separators, 0)

    def _recursive_split(self, text: str, separators: List[str], depth: int) -> List[str]:
        if len(text) <= self.config.chunk_size or depth >= self.recursion_limit:
            return [text.strip()]

        for sep in separators:
            parts = text.split(sep)
            if len(parts) == 1:
                continue

            chunks = []
            current_parts = []
            current_length = 0

            for part in parts:
                part_len = len(part) + len(sep)
                if current_length + part_len <= self.config.chunk_size:
                    current_parts.append(part)
                    current_length += part_len
                else:
                    if current_parts:
                        chunks.append(sep.join(current_parts).strip())
                    current_parts = [part]
                    current_length = len(part)

            if current_parts:
                chunks.append(sep.join(current_parts).strip())

            refined_chunks = []
            for chunk in chunks:
                if len(chunk) > self.config.chunk_size and len(separators) > 1:
                    refined_chunks.extend(self._recursive_split(chunk, separators[1:], depth + 1))
                else:
                    refined_chunks.append(chunk)

            return refined_chunks

        # fallback: fixed-length slicing
        return [text[i : i + self.config.chunk_size] for i in range(0, len(text), self.config.chunk_size)]


@ChunkingFactory.register(
    "fixed",
    description="Fixed-size chunking preferring separators but slicing to chunk_size as fallback.",
    tags=["rule-based", "fixed"],
)
class FixedChunkingStrategy(BaseChunkingStrategy):
    async def _chunk(self, text: str, separators: Optional[List[str]]) -> List[str]:
        if not text.strip():
            return []

        if not separators:
            separators = [".", "\n\n", " "]

        sep_pattern = "|".join(re.escape(sep) for sep in separators)
        parts = re.split(f"({sep_pattern})", text)

        chunks = []
        current = ""
        for part in parts:
            if not part:
                continue
            if len(current) + len(part) <= self.config.chunk_size:
                current += part
            else:
                chunks.append(current.strip())
                current = part
        if current:
            chunks.append(current.strip())

        # remove empty strings
        return [c for c in chunks if c]

>> /genai_platform_services/src/services/strategies/vector_store_PG_strategy.py
from collections import defaultdict
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

from sqlalchemy import asc, desc

from src.config import get_settings
from src.db.connection import create_session
from src.db.platform_meta_tables import VectorStoreInfo
from src.exception.document_store_exception import DocumentStoreIndexingError
from src.exception.exceptions import VectorStoreError
from src.logging_config import Logger
from src.models.storage_payload import SearchRequest
from src.models.vector_store_payload import (
    CreateVectorStoreRequest,
    SearchResult,
    StorageBackend,
)
from src.repository.base_repository import BaseRepository
from src.repository.document_repository import DocumentRepository
from src.repository.vectorstore_ddl import VectorStoreDDL
from src.services.base_class.vector_store_base import BaseVectorStore, VectorStoreConfig
from src.services.embedding_service import EmbeddingService
from src.services.factory.vector_store_factory import VectorStoreFactory
from src.utility.vector_store_utils import (
    create_chunks_tbl_model,
    create_file_info_tbl_model,
)

logger = Logger.create_logger(__name__)


@VectorStoreFactory.register("pgvector", description="Postgres + pgvector backend")
class PGVectorStore(BaseVectorStore):
    """Concrete PGVector implementation of the Vector Store interface."""

    def __init__(
        self,
        config: VectorStoreConfig,
        embedding_service: Optional[EmbeddingService] = None,
        document_repository: Optional[DocumentRepository] = None,
    ) -> None:
        """
        Initialize PGVectorStore backend with injected dependencies.
        Falls back to a default DocumentRepository if one isn't provided.
        """
        settings = config.extra or get_settings()
        document_repository = document_repository
        super().__init__(
            document_repository=document_repository,
            embedding_service=embedding_service,
            settings=settings,
        )
        # super().__init__(document_repository=None, embedding_service=embedding_service, settings=config.extra)

        logger.debug(
            f"[PGVectorStore] Initialized with repo={type(self.document_repository)}, embedding_service={type(self.embedding_service)}"
        )

    # =================================================================
    # Implement all abstract methods required by BaseVectorStore
    # =================================================================

    async def _create_backend_store(
        self,
        payload: CreateVectorStoreRequest,
        usecase_id: str,
        embedding_dimensions: int,
        store_id: str,
        now_dt: datetime,
        expires_at: Optional[datetime],
    ) -> Dict[str, Any]:
        try:
            # Create PostgreSQL tables with pgvector extension
            VectorStoreDDL.create_tables_and_index(table_name=payload.name, dimensions=embedding_dimensions)
            logger.info(f"[PGVector] Created tables for '{payload.name}' with {embedding_dimensions} dims")

            # Build and insert metadata using base class helper
            insert_data = self._build_metadata_dict(
                store_id=store_id,
                payload=payload,
                usecase_id=usecase_id,
                now_dt=now_dt,
                expires_at=expires_at,
                backend_type=StorageBackend.PGVECTOR.value,
            )

            BaseRepository.insert_one(db_tbl=VectorStoreInfo, data=insert_data)  # type: ignore
            logger.info(f"[PGVector] Metadata inserted for store '{store_id}'")

            # Return standardized response using base class helper
            return self._build_create_response_dict(
                store_id=store_id,
                payload=payload,
                now_dt=now_dt,
                expires_at=expires_at,
            )

        except VectorStoreError:
            try:
                BaseRepository.delete(db_tbl=VectorStoreInfo, filters={"id": store_id})  # type: ignore
                VectorStoreDDL.drop_table_and_index(tbl_name=payload.name)
                logger.info(f"[PGVector] Rollback successful for '{payload.name}'")
            except Exception as cleanup_err:
                logger.warning(f"[PGVector] Cleanup failed: {cleanup_err}")
            raise
        except Exception as e:
            logger.error(f"[PGVector] Store creation failed: {e}", exc_info=True)

            # Rollback: delete metadata and tables
            try:
                BaseRepository.delete(db_tbl=VectorStoreInfo, filters={"id": store_id})  # type: ignore
                VectorStoreDDL.drop_table_and_index(tbl_name=payload.name)
                logger.info(f"[PGVector] Rollback successful for '{payload.name}'")
            except Exception as cleanup_err:
                logger.warning(f"[PGVector] Cleanup failed: {cleanup_err}")

            raise VectorStoreError(f"PGVector store creation failed: {str(e)}")

    async def _index_backend(
        self,
        payload: Any,
        store_id: str,
        store_name: str,
        model_name: str,
        context_length: int,
        model_path: str,
        embedding_dimensions: int,
    ) -> int:
        try:
            vs_file_info = create_file_info_tbl_model(f"{store_name}_file_info")  # type: ignore
            existing = BaseRepository.select_one(
                db_tbl=vs_file_info,
                filters={"file_name": payload.file_name, "vs_id": store_id},
                session_factory=create_session,
            )
            if existing:
                raise DocumentStoreIndexingError(
                    f"Duplicate entry found for file_name '{payload.file_name}' with vector_store_name '{store_name}'"
                )

            docs_with_embeddings, data_size, file_info = await self.prepare_index_emdeddings(
                payload=payload,
                model_name=model_name,
                model_path=model_path,
                context_length=context_length,
                embedding_service=self.embedding_service,
                timezone=self.settings.timezone,
            )

            document_repository = DocumentRepository(f"{store_name}_chunks", embedding_dimensions=embedding_dimensions)

            document_repository.insert_documents(
                table_name=f"{store_name}_chunks",
                documents=docs_with_embeddings,
                service_type="vectorstore",
                file_id=payload.file_id,
                file_name=payload.file_name,
            )

            file_metadata: Dict[str, Any] = {
                **file_info,
                "vs_id": store_id,
                "active": True,
            }

            BaseRepository.insert_one(db_tbl=vs_file_info, data=file_metadata, session_factory=create_session)

            logger.info(f"[PGVector] Indexed {len(docs_with_embeddings)} documents for file '{payload.file_id}'")
            return data_size
        except DocumentStoreIndexingError:
            raise
        except Exception as e:
            logger.exception(f"[PGVector] Indexing failed: {e}")

            # Cleanup on failure
            try:
                vs_file_info = create_file_info_tbl_model(f"{store_name}_file_info")
                vs_chunks = create_chunks_tbl_model(f"{store_name}_chunks", embedding_dimensions)  # type: ignore

                BaseRepository.delete(
                    db_tbl=vs_file_info,
                    filters={"file_id": payload.file_id, "vs_id": store_id},
                    session_factory=create_session,
                )
                BaseRepository.delete(
                    db_tbl=vs_chunks,
                    filters={"file_id": payload.file_id},
                    session_factory=create_session,
                )
            except Exception as cleanup_err:
                logger.warning(f"[PGVector] Cleanup failed: {cleanup_err}")

            raise DocumentStoreIndexingError(f"Indexing failed: {str(e)}")

    async def _drop_backend_tables(self, store_name: str) -> None:
        """Drop PostgreSQL tables for the vector store."""
        VectorStoreDDL.drop_table_and_index(tbl_name=store_name)

    async def _list_backend_stores(
        self,
        usecase_id: str,
        limit: int,
        after: Optional[str],
        before: Optional[str],
        order: str,
        vector_db: Optional[str],
    ) -> List[Dict[str, Any]]:
        try:
            if order.lower() not in {"asc", "desc"}:
                raise VectorStoreError(f"Invalid orderby value '{order}'")

            order_by_clause = (
                desc(VectorStoreInfo.created_at) if order.lower() == "desc" else asc(VectorStoreInfo.created_at)
            )

            # Build filters
            filters: Dict[str, Any] = {"usecase_id": usecase_id}
            if vector_db:  # If backend specified, filter by it
                filters["vector_db"] = vector_db

            response = BaseRepository.select_many(  # type: ignore
                db_tbl=VectorStoreInfo,
                filters=filters,
                order_by=order_by_clause,
            )

            logger.info(f"[PGVector] Listed {len(response)} stores for usecase '{usecase_id}' (filter: {vector_db})")
            return response  # type: ignore

        except Exception as e:
            logger.exception(f"[PGVector] List stores failed: {e}")
            raise VectorStoreError(f"Failed to list stores: {e}")

    async def _validate_backend_type(self, record: Dict[str, Any]) -> None:
        """Ensure the vector_db field indicates PGVector."""
        if record["vector_db"] != StorageBackend.PGVECTOR.value:
            raise VectorStoreError(f"Vector Store '{record['id']}' is not stored in PGVector")

    async def _delete_metadata_and_chunks(
        self, store_name: str, vectorstoreid: str, vectorstorefileid: str
    ) -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:
        """Delete file metadata and chunks from PostgreSQL."""
        vs_file_info_tbl = create_file_info_tbl_model(f"{store_name}_file_info")  # type: ignore
        vs_chunks_tbl = create_chunks_tbl_model(f"{store_name}_chunks", dimensions=0)  # type: ignore

        file_info_record = BaseRepository.select_one(
            db_tbl=vs_file_info_tbl,
            filters={"file_id": vectorstorefileid, "vs_id": vectorstoreid},
            session_factory=create_session,
        )
        if not file_info_record:
            raise VectorStoreError(f":: PG :: File '{vectorstorefileid}' not found in store '{vectorstoreid}'")

        chunk_records = BaseRepository.select_many(
            db_tbl=vs_chunks_tbl,
            filters={"file_id": vectorstorefileid},
            session_factory=create_session,
        )
        chunk_count = len(chunk_records) if chunk_records else 0
        logger.info(f":: PG :: Found {chunk_count} chunks for file '{vectorstorefileid}'")

        deleted_meta = BaseRepository.delete(
            db_tbl=vs_file_info_tbl,
            filters={"file_id": vectorstorefileid, "vs_id": vectorstoreid},
            session_factory=create_session,
        )
        if deleted_meta == 0:
            raise VectorStoreError(f":: PG :: Metadata delete failed for '{vectorstorefileid}'")

        try:
            deleted_chunks = BaseRepository.delete(
                db_tbl=vs_chunks_tbl,
                filters={"file_id": vectorstorefileid},
                session_factory=create_session,
            )
            if deleted_chunks == 0:
                raise VectorStoreError(f":: PG :: No chunks deleted for '{vectorstorefileid}'")
        except Exception as chunk_err:
            logger.error(f":: PG :: Chunk delete failed: {chunk_err}", exc_info=True)
            try:
                BaseRepository.insert_one(
                    db_tbl=vs_file_info_tbl,
                    data=file_info_record,
                    session_factory=create_session,
                )
                logger.info(f":: ROLLBACK :: Restored metadata for '{vectorstorefileid}' after chunk deletion failure.")
            except Exception as rollback_err:
                logger.error(f":: ROLLBACK :: Failed to restore metadata: {rollback_err}", exc_info=True)
            raise VectorStoreError(f"Chunk deletion failed â€” metadata restored for '{vectorstorefileid}'")

        logger.info(f":: PG :: Deleted metadata and {chunk_count} chunks for file '{vectorstorefileid}' successfully.")
        return file_info_record, chunk_records

    async def _fulltext_search(
        self, search_request: SearchRequest, index_name: str, model_name: str
    ) -> list[SearchResult]:
        _, results = self.document_repository.fulltext_search(
            query=search_request.search_text,
            search_terms=search_request.content_filter,
            include_links=search_request.link_filter,
            include_topics=search_request.topic_filter,
            top_k=search_request.limit,
            min_relevance_score=search_request.min_score,
        )
        return results  # type: ignore

    async def _semantic_search(
        self,
        search_request: Any,
        index_name: str,
        model_name: str,
    ) -> List[SearchResult]:
        embeddings = await self.embedding_service.get_embeddings(
            model_name=self.settings.default_model_embeddings,
            batch=[search_request.search_text],
        )
        query_vector = embeddings.data[0].embedding
        _, results = self.document_repository.sematic_search(
            query_vector=query_vector,
            search_terms=search_request.content_filter,
            include_links=search_request.link_filter,
            include_topics=search_request.topic_filter,
            top_k=search_request.limit,
            min_similarity_score=search_request.min_score,
        )
        return results  # type: ignore

    async def _hybrid_search(
        self,
        search_request: Any,
        index_name: str,
        model_name: str,
    ) -> List[SearchResult]:
        semantic_results: list[SearchResult] = await self._semantic_search(search_request, index_name, model_name)
        fulltext_results: list[SearchResult] = await self._fulltext_search(search_request, index_name, model_name)
        logger.info(
            f"Hybrid search -> Semantic search results: {len(semantic_results)}, "
            f"Full-text search results: {len(fulltext_results)}"
        )
        score_map = defaultdict(lambda: {"semantic": 0.0, "fulltext": 0.0})  # type: ignore
        result_map: Dict[str, SearchResult] = {}
        for res in semantic_results:
            score_map[res.file_id]["semantic"] = res.score
            result_map[res.file_id] = res
        for res in fulltext_results:
            score_map[res.file_id]["fulltext"] = res.score
            result_map.setdefault(res.file_id, res)
        reranked: List[SearchResult] = []
        for file_id_, scores in score_map.items():
            weighted_score = round(0.6 * scores["semantic"] + 0.4 * scores["fulltext"], 4)
            result = result_map[file_id_]
            result.score = weighted_score
            reranked.append(result)
        reranked.sort(key=lambda r: r.score, reverse=True)
        return reranked[: search_request.limit]

    async def _get_file_info(self, store_name: str, vectorstoreid: str, vectorstorefileid: str) -> dict:
        """Fetch file metadata from Postgres tables."""
        vs_file_info_tbl = create_file_info_tbl_model(f"{store_name}_file_info")  # type: ignore
        return BaseRepository.select_one(  # type: ignore
            db_tbl=vs_file_info_tbl,
            filters={"file_id": vectorstorefileid, "vs_id": vectorstoreid},
            session_factory=create_session,
        )

    async def _get_chunk_records(self, store_name: str, vectorstorefileid: str) -> List[dict]:
        """Fetch file chunks from Postgres tables."""
        vs_chunks_tbl = create_chunks_tbl_model(f"{store_name}_chunks", dimensions=0)  # type: ignore
        return BaseRepository.select_many(
            db_tbl=vs_chunks_tbl,
            filters={"file_id": vectorstorefileid},
            session_factory=create_session,
        )

>> /genai_platform_services/src/services/strategies/vector_store_ES_strategy.py
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple
from uuid import uuid4

from elasticsearch import Elasticsearch, helpers
from sqlalchemy import asc, desc

from src.config import get_settings
from src.db.elasticsearch_connection import get_elasticsearch_client
from src.db.platform_meta_tables import VectorStoreInfo
from src.exception.document_store_exception import (
    DocumentStoreIndexingError,
    DocumentStoreSearchError,
)
from src.exception.exceptions import VectorStoreError
from src.logging_config import Logger
from src.models.vector_store_payload import (
    ContentBlock,
    CreateVectorStoreFileRequest,
    CreateVectorStoreRequest,
    FileStatus,
    SearchResult,
    StorageBackend,
)
from src.repository.base_repository import BaseRepository
from src.repository.elasticsearch_ddl import ElasticsearchDDL
from src.repository.elasticsearch_dml import ElasticsearchDML
from src.services.base_class.vector_store_base import BaseVectorStore, VectorStoreConfig
from src.services.embedding_service import EmbeddingService
from src.services.factory.vector_store_factory import VectorStoreFactory

logger = Logger.create_logger(__name__)


@VectorStoreFactory.register("elasticsearch", description="Elasticsearch + GCP backend")
class ElasticsearchGCPVectorStore(BaseVectorStore):
    """Concrete Elasticsearch + GCP implementation of the Vector Store interface."""

    def __init__(
        self,
        config: VectorStoreConfig,
        embedding_service: Optional[EmbeddingService] = None,
        document_repository: Optional[Any] = None,
    ):
        """
        Initialize Elasticsearch Vector Store backend.
        Mirrors PGVectorStore signature for uniformity across backends.
        """
        settings = config.extra or get_settings()
        self.client: Elasticsearch = get_elasticsearch_client()

        super().__init__(
            document_repository=document_repository,
            embedding_service=embedding_service,
            settings=settings,
        )
        # super().__init__(document_repository=None, embedding_service=embedding_service, settings=config.extra)

    # ================================================================
    # REQUIRED ABSTRACT IMPLEMENTATIONS
    # ================================================================
    async def _create_backend_store(
        self,
        payload: CreateVectorStoreRequest,
        usecase_id: str,
        embedding_dimensions: int,
        store_id: str,
        now_dt: datetime,
        expires_at: Optional[datetime],
    ) -> Dict[str, Any]:
        try:
            # Check if ES indices already exist
            existing_indices = ElasticsearchDDL.check_indices_exist(payload.name)
            if existing_indices["both"]:
                logger.warning(f"[ES] Indices for '{payload.name}' already exist")
                raise VectorStoreError(f"Elasticsearch indices already exist for '{payload.name}'")

            # Create Elasticsearch indices (_file_info and _chunks)
            ElasticsearchDDL.create_vectorstore_indices(payload.name, embedding_dimensions)
            logger.info(f"[ES] Created indices for '{payload.name}' with {embedding_dimensions} dims")

            # Build and insert metadata using base class helper
            insert_data = self._build_metadata_dict(
                store_id=store_id,
                payload=payload,
                usecase_id=usecase_id,
                now_dt=now_dt,
                expires_at=expires_at,
                backend_type=StorageBackend.ELASTICSEARCH.value,
            )

            BaseRepository.insert_one(db_tbl=VectorStoreInfo, data=insert_data)  # type: ignore
            logger.info(f"[ES] Metadata inserted for store '{store_id}'")

            # Return standardized response using base class helper
            return self._build_create_response_dict(
                store_id=store_id,
                payload=payload,
                now_dt=now_dt,
                expires_at=expires_at,
            )

        except VectorStoreError:
            try:
                BaseRepository.delete(db_tbl=VectorStoreInfo, filters={"id": store_id})  # type: ignore
                ElasticsearchDDL.drop_indices(payload.name)
                logger.info(f"[ES] Rollback successful for '{payload.name}'")
            except Exception as cleanup_err:
                logger.warning(f"[ES] Cleanup failed: {cleanup_err}")
            raise
        except Exception as e:
            logger.error(f"[ES] Store creation failed: {e}", exc_info=True)

            # Rollback: delete metadata and indices
            try:
                BaseRepository.delete(db_tbl=VectorStoreInfo, filters={"id": store_id})  # type: ignore
                ElasticsearchDDL.drop_indices(payload.name)
                logger.info(f"[ES] Rollback successful for '{payload.name}'")
            except Exception as cleanup_err:
                logger.warning(f"[ES] Cleanup failed: {cleanup_err}")

            raise VectorStoreError(f"Elasticsearch store creation failed: {str(e)}")

    async def _validate_backend_type(self, record: Dict[str, Any]) -> None:
        """Ensure the vector_db field indicates Elasticsearch."""
        if record["vector_db"] != StorageBackend.ELASTICSEARCH.value:
            raise VectorStoreError(f"Vector Store '{record['id']}' is not stored in Elasticsearch")

    async def _index_backend(
        self,
        payload: CreateVectorStoreFileRequest,
        store_id: str,
        store_name: str,
        model_name: str,
        context_length: int,
        model_path: str,
        embedding_dimensions: int,
    ) -> int:
        chunks_index = f"{store_name}_chunks"
        file_info_index = f"{store_name}_file_info"

        try:
            existing = self.client.get(index=file_info_index, id=payload.file_id, ignore=[404])  # type: ignore

            if existing and existing.get("found"):
                raise DocumentStoreIndexingError(
                    f"Duplicate entry found for file_name '{payload.file_name}' with vector_store_name '{store_name}'"
                )
        except Exception as es_err:
            if "index_not_found_exception" not in str(es_err):
                raise

        try:
            docs_with_embeddings, data_size, file_info = await self.prepare_index_emdeddings(
                payload=payload,
                model_name=model_name,
                model_path=model_path,
                context_length=context_length,
                embedding_service=self.embedding_service,
                timezone=self.settings.timezone,
            )

            # Bulk index to Elasticsearch
            self._bulk_index_documents(
                chunks_index,
                docs_with_embeddings,
                file_id=payload.file_id,
                file_name=payload.file_name,
            )

            file_info_doc: Dict[str, Any] = {
                **file_info,
                "created_at": datetime.now().isoformat(),
                "vs_id": store_id,
                "active": True,
            }
            logger.info(file_info_doc)

            self.client.index(index=file_info_index, id=payload.file_id, body=file_info_doc)

            logger.info(f"[ES] Indexed {len(docs_with_embeddings)} documents for file '{payload.file_id}'")
            return data_size

        except DocumentStoreIndexingError:
            raise
        except Exception as e:
            logger.exception(f"[ES] Indexing failed: {e}")

            # Cleanup on failure
            try:
                self._delete_documents_by_file_id(chunks_index, payload.file_id)
                self.client.delete(index=file_info_index, id=payload.file_id, ignore=[404])  # type: ignore
            except Exception as cleanup_err:
                logger.error(f"[ES] Cleanup failed: {cleanup_err}")

            raise DocumentStoreIndexingError(f"Indexing failed: {str(e)}")

    def _bulk_index_documents(
        self, index_name: str, documents: List[Dict[str, Any]], file_id: str, file_name: str
    ) -> int:
        actions: List[Dict[str, Any]] = []
        for doc in documents:
            doc_id = str(uuid4())
            doc_body: Dict[str, Any] = {
                "id": doc_id,
                "content": doc.get("content"),
                "embedding": doc.get("embedding"),
                "links": doc.get("links"),
                "topics": doc.get("topics"),
                "author": doc.get("author"),
                "meta_data": doc.get("meta_data"),
                "created_at": datetime.now().isoformat(),
                "file_id": file_id,
                "file_name": file_name,
            }
            actions.append({"_index": index_name, "_id": doc_id, "_source": doc_body})

        try:
            success, failed = helpers.bulk(
                self.client, actions, chunk_size=self.settings.elasticsearch_bulk_chunk_size, raise_on_error=False
            )
            logger.info(f"[ES] Indexed {success} documents, {len(failed)} failed")  # type: ignore
            return success
        except Exception as e:
            logger.error(f"[ES] Bulk indexing failed: {e}")
            raise

    def _delete_documents_by_file_id(self, index_name: str, file_id: str) -> int:
        query: Dict[str, Any] = {"query": {"term": {"file_id": file_id}}}
        try:
            response = self.client.delete_by_query(index=index_name, body=query)
            return response.get("deleted", 0)  # type: ignore
        except Exception as e:
            logger.error(f"[ES] Delete documents failed: {e}")
            raise

    async def _drop_backend_tables(self, store_name: str) -> None:
        """Drops Elasticsearch indices (file_info + chunks)."""
        try:
            ElasticsearchDDL.drop_indices(store_name)
            logger.info(f"[ES+GCP] Dropped indices for store '{store_name}'")
        except Exception as err:
            logger.error(f"[ES+GCP] Failed to drop indices: {err}", exc_info=True)
            raise VectorStoreError(f"Elasticsearch index deletion failed for store '{store_name}'")

    async def _fetch_file_backend(self, vectorstoreid: str, vectorstorefileid: str, usecase_id: str) -> Dict[str, Any]:
        store_name = f"{vectorstoreid}_file_info"
        query: Dict[str, Any] = {
            "query": {
                "bool": {
                    "must": [
                        {"term": {"file_id": vectorstorefileid}},
                        {"term": {"vs_id": vectorstoreid}},
                    ]
                }
            }
        }
        record = ElasticsearchDML.select_one(store_name, query)
        if not record:
            raise VectorStoreError(f":: ES :: File '{vectorstorefileid}' not found in store '{vectorstoreid}'")
        return record

    async def _list_backend_stores(
        self,
        usecase_id: str,
        limit: int,
        after: Optional[str],
        before: Optional[str],
        order: str,
        vector_db: Optional[str],
    ) -> List[Dict[str, Any]]:
        try:
            if order.lower() not in {"asc", "desc"}:
                raise VectorStoreError(f"Invalid orderby value '{order}'")

            order_by_clause = (
                desc(VectorStoreInfo.created_at) if order.lower() == "desc" else asc(VectorStoreInfo.created_at)
            )

            # Build filters
            filters: Dict[str, Any] = {"usecase_id": usecase_id}
            if vector_db:
                filters["vector_db"] = vector_db

            response = BaseRepository.select_many(  # type: ignore
                db_tbl=VectorStoreInfo,
                filters=filters,
                order_by=order_by_clause,
            )

            logger.info(f"[ES] Listed {len(response)} stores for usecase '{usecase_id}' (filter: {vector_db})")
            return response  # type: ignore

        except Exception as e:
            logger.exception(f"[ES] List stores failed: {e}")
            raise VectorStoreError(f"Failed to list stores: {e}")

    # ================================================================
    # DELETE OPERATIONS
    # ================================================================
    async def _delete_metadata_and_chunks(
        self, store_name: str, vectorstoreid: str, vectorstorefileid: str
    ) -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:
        """
        Delete file metadata and its chunks from Elasticsearch.
        - Delete file info record first â†’ raise if fails.
        - Delete chunks â†’ if fails, restore file info only (no chunk restore).
        """

        vs_file_info = f"{store_name}_file_info"
        vs_chunks = f"{store_name}_chunks"

        file_query: Dict[str, Any] = {
            "query": {
                "bool": {
                    "must": [
                        {"term": {"file_id": vectorstorefileid}},
                        {"term": {"vs_id": vectorstoreid}},
                    ]
                }
            }
        }

        # Step 1 â€” Fetch file info record
        file_info_record = ElasticsearchDML.select_one(vs_file_info, file_query)
        if not file_info_record:
            raise VectorStoreError(
                f":: ES :: File '{vectorstorefileid}' does not exist in vector store '{vectorstoreid}'"
            )

        # Step 2 â€” Delete file info metadata
        try:
            deleted_meta = ElasticsearchDML.delete(index_name=vs_file_info, doc_id=vectorstorefileid)
            if not deleted_meta or deleted_meta.get("deleted", 0) == 0:
                raise VectorStoreError(f":: ES :: Failed to delete metadata for '{vectorstorefileid}'")
            logger.info(f":: ES :: Deleted file_info '{vectorstorefileid}' from '{vs_file_info}'")
        except Exception as err:
            logger.error(f":: ES :: Metadata delete failed: {err}", exc_info=True)
            raise

        # Step 3 â€” Fetch chunk references for logging only
        chunk_query: Dict[str, Any] = {
            "query": {
                "bool": {
                    "must": [
                        {"term": {"file_id": vectorstorefileid}},
                    ]
                }
            }
        }

        chunk_records = ElasticsearchDML.select_many(vs_chunks, chunk_query)
        logger.info(f":: ES :: Found {len(chunk_records)} chunks for deletion in '{vs_chunks}'")

        # Step 4 â€” Delete chunks, rollback file_info only if chunk delete fails
        try:
            deleted_chunks = ElasticsearchDML.delete(index_name=vs_chunks, query=chunk_query)
            deleted_count = deleted_chunks.get("deleted", 0)
            if deleted_count <= 0:
                raise VectorStoreError(f":: ES :: No chunks deleted for '{vectorstorefileid}'")

            logger.info(f":: ES :: Successfully deleted {deleted_count} chunks for '{vectorstorefileid}'")

        except Exception as chunk_err:
            logger.error(f":: ES :: Chunk delete failed: {chunk_err}", exc_info=True)

            # Step 5 â€” Rollback file_info only
            restored = self._restore_file_info_record(vs_file_info, file_info_record)
            if restored:
                logger.info(
                    f":: ROLLBACK :: Restored file_info record for '{vectorstorefileid}' after chunk delete failure"
                )
            else:
                logger.error(f":: ROLLBACK :: Failed to restore file_info record for '{vectorstorefileid}'")

            raise VectorStoreError(
                f":: ES :: Chunk deletion failed. File info rollback {'succeeded' if restored else 'failed'} for '{vectorstorefileid}'"
            )

        logger.info(f":: ES :: Deleted metadata and {len(chunk_records)} chunks for '{vectorstorefileid}' successfully")
        return file_info_record, chunk_records

    def _restore_file_info_record(self, index_name: str, file_info_record: Dict[str, Any]) -> bool:
        try:
            if not file_info_record:
                logger.warning(f"No file_info_record provided for restore into {index_name}")
                return False

            restore_data: Dict[str, Any] = {
                "active": file_info_record.get("active", "true"),
                "vs_id": file_info_record["vs_id"],
                "file_id": file_info_record["file_id"],
                "file_name": file_info_record["file_name"],
                "file_version": file_info_record.get("file_version", "1"),
                "created_at": file_info_record.get("created_at"),
                "usage_bytes": file_info_record.get("usage_bytes", 0),
                "chunking_strategy": file_info_record.get("chunking_strategy"),
                "attributes": file_info_record.get("attributes") or {},
                "status": file_info_record.get("status", FileStatus.COMPLETED.value),
            }

            self.client.index(
                index=index_name,
                id=file_info_record["file_id"],
                body=restore_data,
            )

            logger.info(f"Successfully restored file_info record '{file_info_record['file_id']}' into {index_name}")
            return True

        except Exception as e:
            logger.error(
                f"Failed to restore file_info record '{file_info_record.get('file_id', '')}': {e}",
                exc_info=True,
            )
            return False

    # ================================================================
    # SEARCH OPERATIONS
    # ================================================================
    async def _semantic_search(
        self,
        search_request: Any,
        index_name: str,
        model_name: str,
    ) -> List[SearchResult]:
        embeddings = await self.embedding_service.get_embeddings(
            model_name=model_name,
            batch=[search_request.search_text],
        )
        query_vector = embeddings.data[0].embedding

        filters = self._build_filters(search_request)

        knn_query: Dict[str, Any] = {
            "field": "embedding",
            "query_vector": query_vector,
            "k": search_request.limit,
            "num_candidates": search_request.limit * 10,
        }

        if filters:
            knn_query["filter"] = filters

        query_body: Dict[str, Any] = {
            "knn": knn_query,
            "min_score": search_request.min_score,
            "size": search_request.limit,
            "_source": {"excludes": ["embedding"]},
        }

        response = self.client.search(index=index_name, body=query_body)

        results: List[SearchResult] = []
        for hit in response["hits"]["hits"]:
            source = hit["_source"]
            results.append(
                SearchResult(
                    id=source.get("id", ""),
                    file_id=source.get("file_id", ""),
                    filename=source.get("file_name", ""),
                    score=hit["_score"],
                    content=[ContentBlock(type="text", text=source.get("content", ""))],
                    attributes=source.get("meta_data", {}),
                )
            )

        return results

    async def _fulltext_search(
        self,
        search_request: Any,
        index_name: str,
        model_name: str,
    ) -> List[SearchResult]:
        filters = self._build_filters(search_request)

        query_body: Dict[str, Any] = {
            "query": {
                "bool": {
                    "must": [
                        {
                            "multi_match": {
                                "query": search_request.search_text,
                                "fields": ["content^2", "topics", "author", "links"],
                                "type": "best_fields",
                                "fuzziness": "AUTO",
                            }
                        }
                    ]
                }
            },
            "min_score": search_request.min_score,
            "size": search_request.limit,
            "_source": {"excludes": ["embedding"]},
        }

        if filters:
            query_body["query"]["bool"]["filter"] = filters

        response = self.client.search(index=index_name, body=query_body)

        results: List[SearchResult] = []
        for hit in response["hits"]["hits"]:
            source = hit["_source"]
            results.append(
                SearchResult(
                    id=source.get("id", ""),
                    file_id=source.get("file_id", ""),
                    filename=source.get("file_name", ""),
                    score=hit["_score"],
                    content=[ContentBlock(type="text", text=source.get("content", ""))],
                    attributes=source.get("meta_data", {}),
                )
            )

        return results

    async def _hybrid_search(
        self,
        search_request: Any,
        index_name: str,
        model_name: str,
    ) -> List[SearchResult]:
        try:
            embeddings = await self.embedding_service.get_embeddings(
                model_name=model_name,
                batch=[search_request.search_text],
            )
            query_vector = embeddings.data[0].embedding

            filters = self._build_filters(search_request)

            filter_dict: Dict[str, Any] = {"filter": filters} if filters else {}

            query_body: Dict[str, Any] = {
                "rank": {
                    "rrf": {
                        "window_size": getattr(self.settings, "rrf_window_size", 100),
                        "rank_constant": getattr(self.settings, "rrf_rank_constant", 60),
                    }
                },
                "query": {
                    "bool": {
                        "must": [
                            {
                                "multi_match": {
                                    "query": search_request.search_text,
                                    "fields": ["content^2", "topics", "author", "links"],
                                    "type": "best_fields",
                                    "fuzziness": "AUTO",
                                }
                            }
                        ],
                        **filter_dict,
                    }
                },
                "knn": {
                    "field": "embedding",
                    "query_vector": query_vector,
                    "k": search_request.limit,
                    "num_candidates": search_request.limit * 10,
                    **filter_dict,
                },
                "_source": {"excludes": ["embedding"]},
                "size": search_request.limit,
            }

            response = self.client.search(index=index_name, body=query_body)

            results: List[SearchResult] = []
            for hit in response["hits"]["hits"]:
                source = hit["_source"]
                results.append(
                    SearchResult(
                        id=source.get("id", ""),
                        file_id=source.get("file_id", ""),
                        filename=source.get("file_name", ""),
                        score=hit.get("_score", 0.0) or 0.0,
                        content=[ContentBlock(type="text", text=source.get("content", ""))],
                        attributes=source.get("meta_data", {}),
                    )
                )

            logger.info(
                f"[HYBRID] Native RRF returned {len(results)} results from index '{index_name}' "
                f"(window={query_body['rank']['rrf']['window_size']}, k={query_body['rank']['rrf']['rank_constant']})"
            )
            return results

        except Exception as e:
            logger.exception(f"[HYBRID] Native RRF search failed: {str(e)}")
            raise DocumentStoreSearchError(f"Hybrid search failed: {str(e)}")

    def _build_filters(self, search_request: Any) -> Optional[Dict[str, Any]]:
        """Build Elasticsearch query filters from search request."""
        filters: List[Dict[str, Any]] = []

        if search_request.content_filter:
            filters.append({"terms": {"content": search_request.content_filter}})

        if search_request.link_filter:
            filters.append({"terms": {"links": search_request.link_filter}})

        if search_request.topic_filter:
            filters.append({"terms": {"topics": search_request.topic_filter}})

        if not filters:
            return None

        return {"bool": {"must": filters}} if len(filters) > 1 else filters[0]

    async def _get_file_info(self, store_name: str, vectorstoreid: str, vectorstorefileid: str) -> dict:
        """Fetch file metadata from Elasticsearch index."""
        file_info_index = f"{store_name}_file_info"
        file_info_query = {
            "query": {
                "bool": {
                    "must": [
                        {"term": {"file_id": vectorstorefileid}},
                        {"term": {"vs_id": vectorstoreid}},
                    ]
                }
            }
        }
        return ElasticsearchDML.select_one(index_name=file_info_index, query=file_info_query)  # type: ignore

    async def _get_chunk_records(self, store_name: str, vectorstorefileid: str) -> List[dict]:
        """Fetch chunk documents from Elasticsearch index."""
        chunks_index = f"{store_name}_chunks"
        chunks_query = {"query": {"bool": {"must": [{"term": {"file_id": vectorstorefileid}}]}}}
        return ElasticsearchDML.select_many(index_name=chunks_index, query=chunks_query, size=500)

>> /genai_platform_services/src/services/service_layer/chunking_service.py
from typing import List, Optional, Protocol

from src.logging_config import Logger
from src.services.factory.chunking_factory import (
    ChunkingConfig,
    ChunkingFactory,
    ChunkingStrategyNotFoundError,
)

logger_instance = Logger.create_logger(__name__)


class LoggerProtocol(Protocol):
    """Minimal logger interface used by this service."""

    def info(self, msg: str) -> None: ...
    def exception(self, msg: str) -> None: ...


class ChunkingService:
    """Async service that coordinates strategy selection and execution."""

    def __init__(
        self,
        logger: Optional[LoggerProtocol] = logger_instance,
        config: Optional[dict] = None,
    ) -> None:
        self.logger = logger
        self.config = config

    async def chunk_text(
        self,
        text: str,
        strategy_name: str,
        chunk_size: int = 500,
        overlap: int = 50,
        separators: Optional[List[str]] = None,
    ) -> List[str]:
        config = ChunkingConfig(chunk_size=chunk_size, overlap=overlap, separators=separators)

        try:
            strategy = ChunkingFactory.create(strategy_name, config=config)

            if self.logger:
                self.logger.info(f"[ChunkingService] using strategy={strategy_name} cfg={config.dict()}")

            chunks: List[str] = await strategy.chunk(text)
            return chunks

        except ChunkingStrategyNotFoundError:
            raise

        except Exception:
            if self.logger:
                self.logger.exception("Unexpected error in chunk_text")
            raise

>> /genai_platform_services/src/services/service_layer/vector_store_service.py
from typing import List, Optional

# from src.models.storage_payload import SearchRequest
from src.models.vector_store_payload import (
    CreateVectorStoreFileRequest,
    CreateVectorStoreFileResponse,
    CreateVectorStoreRequest,
    CreateVectorStoreResponse,
    DeleteVectorStoreFileResponse,
    DeleteVectorStoreResponse,
    RetrieveFileResponse,
    SearchVectorStoreRequest,
    SearchVectorStoreResponse,
)
from src.repository.document_repository import DocumentRepository
from src.services.base_class.vector_store_base import BaseVectorStore
from src.services.embedding_service import EmbeddingService
from src.services.factory.vector_store_factory import (  # type: ignore[attr-defined]
    VectorStoreConfig,
    VectorStoreFactory,
)


class VectorStoreService:
    """
    High-level service that delegates vector store operations
    to the appropriate backend via VectorStoreFactory.
    """

    def __init__(
        self,
        backend_name: str,
        embedding_service: EmbeddingService,
        document_repository: Optional[DocumentRepository] = None,
    ) -> None:
        self.embedding_service: EmbeddingService = embedding_service
        config = VectorStoreConfig(backend=backend_name)
        if document_repository:
            config.document_repository = document_repository  # type: ignore
        self.vectorstore: BaseVectorStore = VectorStoreFactory.create(
            name=backend_name, config=config, embedding_service=embedding_service
        )

    # ---------------------------------------------------------------------
    # CRUD + Search operations (delegated to strategy)
    # ---------------------------------------------------------------------

    async def create_store(
        self,
        payload: CreateVectorStoreRequest,
        usecase_id: str,
        embedding_dimensions: int,
    ) -> CreateVectorStoreResponse:
        """Creates a new vector store (database or index)."""
        result = await self.vectorstore.create_store(payload, usecase_id, embedding_dimensions=embedding_dimensions)
        return CreateVectorStoreResponse.model_validate(result)

    async def create_store_file(
        self,
        payload: CreateVectorStoreFileRequest,
        store_id: str,
        usecase_id: str,
        model_name: str,
        context_length: int,
        model_path: str,
        embedding_dimensions: int,
    ) -> CreateVectorStoreFileResponse:
        """Indexes a file/document into the vector store."""
        result = await self.vectorstore.create_store_file(
            payload,
            store_id,
            usecase_id,
            model_name,
            context_length,
            model_path,
            embedding_dimensions,
        )
        return CreateVectorStoreFileResponse.model_validate(result)

    async def search(
        self,
        search_request: SearchVectorStoreRequest,
        store_id: str,
        model_name: str,
        context_length: int,
        model_path: str,
    ) -> SearchVectorStoreResponse:
        """Performs semantic/hybrid/full-text search."""
        return await self.vectorstore.search_vector_store(
            search_request,
            store_id,
            model_name,
            context_length,
            model_path,
        )

    async def delete_store(self, store_id: str, usecase_id: str) -> DeleteVectorStoreResponse:
        """Deletes the entire vector store (DB or index)."""
        result = await self.vectorstore.delete(store_id, usecase_id)
        return DeleteVectorStoreResponse.model_validate(result)

    async def delete_file(self, store_id: str, file_id: str, usecase_id: str) -> DeleteVectorStoreFileResponse:
        """Deletes a specific file/document from a vector store."""
        result = await self.vectorstore.delete_by_id(store_id, file_id, usecase_id)
        return DeleteVectorStoreFileResponse.model_validate(result)

    async def retrieve_file(self, store_id: str, file_id: str, usecase_id: str) -> RetrieveFileResponse:
        """Retrieves a specific file/document and its metadata."""
        return await self.vectorstore.retrieve_by_id(store_id, file_id, usecase_id)  # type: ignore

    async def list_stores(
        self,
        usecase_id: str,
        limit: int = 50,
        after: Optional[str] = None,
        before: Optional[str] = None,
        order: str = "desc",
        vector_db: Optional[str] = None,
    ) -> List[CreateVectorStoreResponse]:
        """
        Lists available vector stores for a use case.
        If vector_db is None, lists all stores regardless of backend.
        If vector_db is specified, filters by that backend.
        """
        return await self.vectorstore.list_stores(usecase_id, limit, after, before, order, vector_db)  # type: ignore

>> /genai_platform_services/src/services/factory/chunking_factory.py
import asyncio
import json
import os
from abc import ABC, abstractmethod
from importlib import import_module
from typing import Any, Callable, Dict, List, Optional, Type

from pydantic import BaseModel

CHUNKING_MODULES = os.getenv("CHUNKING_MODULES", "src.services.strategies.chunking_strategies").split(",")


class ChunkingStrategyNotFoundError(Exception):
    pass


def ensure_async(func: Callable[..., Any]) -> Callable[..., Any]:
    import functools as _functools

    from fastapi.concurrency import run_in_threadpool

    @_functools.wraps(func)
    async def wrapper(*args: Any, **kwargs: Any) -> Any:
        if asyncio.iscoroutinefunction(func):
            return await func(*args, **kwargs)
        return await run_in_threadpool(func, *args, **kwargs)

    return wrapper


class ChunkingConfig(BaseModel):
    chunk_size: int = 500
    overlap: int = 50
    separators: Optional[List[str]] = None


class BaseChunkingStrategy(ABC):
    def __init__(self, config: ChunkingConfig) -> None:
        if config.overlap >= config.chunk_size:
            raise ValueError("`overlap` must be smaller than `chunk_size`.")
        self.config = config

    @ensure_async
    async def chunk(self, text: str) -> List[str]:
        text = self._normalize(text)
        if not text:
            return []

        separators = self.config.separators or []
        chunks = await self._chunk(text, separators)
        return self._apply_overlap(chunks)

    @abstractmethod
    async def _chunk(self, text: str, separators: List[str]) -> List[str]:
        raise NotImplementedError()

    def _normalize(self, text: str) -> str:
        import re

        text = re.sub(r"\s+", " ", text).strip()
        return text

    def _apply_overlap(self, chunks: List[str]) -> List[str]:
        if not chunks or self.config.overlap <= 0:
            return chunks

        overlapped: List[str] = [chunks[0]]
        for i in range(1, len(chunks)):
            prev_words = overlapped[i - 1].split()
            overlap_region = " ".join(prev_words[-self.config.overlap :]) if prev_words else ""
            curr = chunks[i]

            if overlap_region and not curr.lower().startswith(overlap_region.lower()):
                curr = f"{overlap_region} {curr}".strip()

            overlapped.append(curr)
        return overlapped


class ChunkingFactory:
    _instance: Optional["ChunkingFactory"] = None
    _registry: Dict[str, Dict[str, Any]] = {}
    _loaded: bool = False
    _cache: Dict[str, BaseChunkingStrategy] = {}

    def __new__(cls) -> "ChunkingFactory":
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    @classmethod
    def register(
        cls, name: str, description: str = "", tags: Optional[List[str]] = None
    ) -> Callable[[Type[BaseChunkingStrategy]], Type[BaseChunkingStrategy]]:
        def decorator(strategy_cls: Type[BaseChunkingStrategy]) -> Type[BaseChunkingStrategy]:
            cls._registry[name] = {
                "class": strategy_cls,
                "description": description or (strategy_cls.__doc__ or "").strip(),
                "tags": tags or [],
            }
            return strategy_cls

        return decorator

    @classmethod
    def _load_strategies(cls) -> None:
        if not cls._loaded:
            for module in CHUNKING_MODULES:
                module = module.strip()
                if module:
                    import_module(module)
            cls._loaded = True

    @classmethod
    def list_strategies(cls) -> List[Dict[str, Any]]:
        cls._load_strategies()
        return [
            {"name": name, "description": meta["description"], "tags": meta["tags"]}
            for name, meta in cls._registry.items()
        ]

    @classmethod
    def _cache_key(cls, name: str, config: ChunkingConfig) -> str:
        cfg_json = json.dumps(config.dict(), sort_keys=True, default=str)
        return f"{name}:{cfg_json}"

    @classmethod
    def create(cls, name: str, config: Optional[ChunkingConfig] = None) -> BaseChunkingStrategy:
        cls._load_strategies()
        meta = cls._registry.get(name)

        if not meta:
            raise ChunkingStrategyNotFoundError(
                f"Chunking strategy '{name}' not found. Registered: {list(cls._registry.keys())}"
            )

        config = config or ChunkingConfig()
        key = cls._cache_key(name, config)

        cached = cls._cache.get(key)
        if cached is not None:
            return cached

        strategy_cls: Type[BaseChunkingStrategy] = meta["class"]
        instance = strategy_cls(config)
        cls._cache[key] = instance
        return instance

>> /genai_platform_services/src/services/factory/vector_store_factory.py
import json
import os
from importlib import import_module
from typing import Any, Callable, Dict, List, Optional, Type

from src.logging_config import Logger
from src.services.base_class.vector_store_base import BaseVectorStore, VectorStoreConfig
from src.services.embedding_service import EmbeddingService

logger = Logger.create_logger(__name__)

VECTORSTORE_MODULES = os.getenv(
    "VECTORSTORE_MODULES",
    "src.services.strategies.vector_store_PG_strategy, src.services.strategies.vector_store_ES_strategy",
).split(",")


class VectorStoreNotFoundError(Exception):
    """Raised when a requested vector store backend is not registered."""

    pass


class VectorStoreFactory:
    """
    Factory for creating vector store backend instances.
    Supports registration, caching, and lazy loading of backends.
    """

    _instance: Optional["VectorStoreFactory"] = None
    _registry: Dict[str, Dict[str, Any]] = {}
    _loaded: bool = False
    _cache: Dict[str, BaseVectorStore] = {}

    def __new__(cls) -> "VectorStoreFactory":
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    @classmethod
    def register(
        cls, name: str, description: str = "", tags: Optional[List[str]] = None
    ) -> Callable[[Type[BaseVectorStore]], Type[BaseVectorStore]]:
        """
        Decorator to register a vector store backend.

        Usage:
            @VectorStoreFactory.register("pgvector", description="PostgreSQL backend")
            class PGVectorStore(BaseVectorStore):
                ...
        """

        def decorator(store_cls: Type[BaseVectorStore]) -> Type[BaseVectorStore]:
            cls._registry[name.lower()] = {
                "class": store_cls,
                "description": description or (store_cls.__doc__ or "").strip(),
                "tags": tags or [],
            }
            logger.debug(f"Registered vector store backend: {name}")
            return store_cls

        return decorator

    @classmethod
    def _load_backends(cls) -> None:
        """Lazy load all backend modules from environment configuration."""
        if not cls._loaded:
            for module in VECTORSTORE_MODULES:
                module = module.strip()
                if module:
                    try:
                        import_module(module)
                        logger.debug(f"Loaded vector store backend module: {module}")
                    except Exception as e:
                        logger.warning(f"Failed to load vector store backend {module}: {e}")
            cls._loaded = True

    @classmethod
    def list_backends(cls) -> List[Dict[str, Any]]:
        """
        List all registered vector store backends.

        Returns:
            List of dicts with backend metadata (name, description, tags)
        """
        cls._load_backends()
        return [{"name": k, "description": v["description"], "tags": v["tags"]} for k, v in cls._registry.items()]

    @classmethod
    def _cache_key(cls, name: str, config: VectorStoreConfig) -> str:
        """Generate cache key from backend name and config."""
        return f"{name}:{json.dumps(config.dict(), sort_keys=True, default=str)}"

    @classmethod
    def create(
        cls,
        name: str,
        config: Optional[VectorStoreConfig] = None,
        embedding_service: Optional[EmbeddingService] = None,
    ) -> BaseVectorStore:
        """
        Create or retrieve a cached vector store backend instance.

        Args:
            name: Backend name (e.g., "pgvector", "elasticsearch")
            config: Configuration for the backend
            embedding_service: Embedding service instance

        Returns:
            BaseVectorStore instance

        Raises:
            VectorStoreNotFoundError: If backend is not registered
        """
        cls._load_backends()

        meta = cls._registry.get(name.lower())
        if not meta:
            raise VectorStoreNotFoundError(
                f"Vector store backend '{name}' not found. " f"Registered backends: {list(cls._registry.keys())}"
            )

        config = config or VectorStoreConfig(backend=name)
        key = cls._cache_key(name, config)

        # Return cached instance if available
        if key in cls._cache:
            logger.debug(f"Using cached backend instance: {name}")
            return cls._cache[key]

        # Create new instance
        backend_cls: Type[BaseVectorStore] = meta["class"]
        instance = backend_cls(config, embedding_service)
        cls._cache[key] = instance

        logger.info(f"Created new vector store backend: {name}")
        return instance

    @classmethod
    def clear_cache(cls) -> None:
        """Clear all cached backend instances."""
        cls._cache.clear()
        logger.info("Cleared vector store backend cache")

    @classmethod
    def is_registered(cls, name: str) -> bool:
        """Check if a backend is registered."""
        cls._load_backends()
        return name.lower() in cls._registry

>> /genai_platform_services/src/utility/utils.py
from typing import Any

import httpx
from fastapi import Header, HTTPException, status

from src.config import Settings, get_settings
from src.logging_config import Logger
from src.models.completion_payload import ChatCompletionRequest
from src.utility.vector_store_helpers import _fetch_usecase_info

logger = Logger.create_logger(__name__)
settings = get_settings()


async def _fetch_prompt(
    endpoint: str, headers_tuple: tuple, prompt_name: str, params_tuple: tuple, verify: bool
) -> str:
    """
    Fetch a single prompt by ID using the endpoint: {endpoint}/{prompt_id}
    Handles HTTP errors: 400, 401, 403, 404, and 5xx.
    """

    headers = dict(headers_tuple)
    params = dict(params_tuple)

    logger.info(f"[PromptHub] Fetching prompt from: {endpoint}")
    logger.info(f"[PromptHub] Prompt Name: {prompt_name}")

    try:
        async with httpx.AsyncClient(verify=verify, timeout=30) as client:
            response = await client.get(endpoint, headers=headers, params=params)
            logger.info(f"[PromptHub] Status code: {response.status_code}")
            response.raise_for_status()

            data = response.json().get("data", {})
            value = data.get("value")
            name = data.get("name", "")

            if value:
                logger.info(f"[PromptHub] Successfully fetched prompt '{name}' (Name: {prompt_name})")
                return str(value)
            else:
                logger.warning(f"[PromptHub] Prompt value missing in response for Name: {prompt_name}")
                raise HTTPException(status_code=400, detail="Prompt value not found.")

    except httpx.HTTPStatusError as e:
        status = e.response.status_code
        logger.error(f"[PromptHub] HTTP error {status}: {e.response.text}")

        match status:
            case 400:
                raise HTTPException(status_code=400, detail="Bad Request: Invalid prompt ID or format.")
            case 401:
                raise HTTPException(status_code=401, detail="Unauthorized: Invalid or missing API key.")
            case 403:
                raise HTTPException(status_code=403, detail="Forbidden: Access denied.")
            case 404:
                raise HTTPException(status_code=404, detail=f"Prompt with Name '{prompt_name}' not found.")
            case _:
                raise HTTPException(status_code=502, detail="PromptHub returned an unexpected error.")

    except httpx.RequestError as e:
        logger.error(f"[PromptHub] Network error: {str(e)}")
        raise HTTPException(status_code=503, detail="PromptHub service is unreachable.")

    except Exception as e:
        logger.exception(f"[PromptHub] Unexpected error: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal error while fetching prompt.")


async def fetch_prompt_by_prompt_name(
    prompt_name: str,
    base_api_key: str,
    token: str | None = None,
    usecase_id: int | None = None,
    settings: Settings | None = None,
) -> str:
    """
    Public wrapper to fetch a prompt by Name using injected or default settings.
    """
    settings = settings or get_settings()

    if not base_api_key and not token:
        logger.error("API key/Token is missing.")
        raise HTTPException(status_code=401, detail="Missing API key/Token.")

    if token and not base_api_key:
        endpoint = f"{settings.prompt_hub_endpoint}{settings.prompt_hub_get_prompt_api_internal}"
        headers = {"Authorization": token}
        params = {"useCaseId": usecase_id, "promptName": prompt_name}
    else:
        endpoint = f"{settings.prompt_hub_endpoint}{settings.prompt_hub_get_prompt_api}"
        masked_key = base_api_key[:6] + "****" + base_api_key[-4:]
        logger.info(f"[PromptHub] Using API key (masked): {masked_key}")

        headers = {"lite-llm-api-key": base_api_key}
        params = {"promptName": prompt_name}

    headers_tuple = tuple(sorted(headers.items()))
    params_tuple = tuple(sorted(params.items()))

    return await _fetch_prompt(endpoint, headers_tuple, prompt_name, params_tuple, settings.verify)


async def validate_models_by_api_key(
    base_api_key: str,
    model_name: str,
) -> bool:
    data = await _fetch_usecase_info(base_api_key)
    model_lst = data["models"]
    if len(model_lst) == 0:  # TODO: Remove this condition
        logger.info("Received empty Model list: Requested Model is part of the usecase (Temporary).")
        return True
    if model_name in model_lst:
        logger.info("Requested Model is part of the usecase.")
        return True
    else:
        logger.info("Requested Model is not part of the usecase.")
        return False


async def _fetch_usecase_info_by_id(usecase_id: int) -> Any:
    validation_url = f"{settings.prompt_hub_endpoint}{settings.prompt_hub_get_usecase_by_id}{usecase_id}"
    verify = settings.deployment_env == "PROD"
    async with httpx.AsyncClient(verify=verify) as client:
        resp = await client.get(validation_url, headers={"INTERNAL-API-KEY": settings.internal_api_key})
    if resp.status_code != 200:
        raise HTTPException(status_code=401, detail="Invalid or unauthorized API key")

    try:
        data = resp.json().get("data", {})
        return data
    except (KeyError, ValueError, TypeError):
        raise HTTPException(status_code=502, detail="Malformed response from Prompt Hub")


async def get_apikey_and_validate_models(
    usecase_id: int,
    model_name: str,
) -> tuple[bool, str]:
    data = await _fetch_usecase_info_by_id(usecase_id)
    model_lst = data["models"]
    llm_api_key = data["llmApiKey"]
    if len(model_lst) == 0:  # TODO: Remove this condition
        logger.info("Received empty Model list: Requested Model is part of the usecase (Temporary).")
        return True, llm_api_key
    if model_name in model_lst:
        logger.info("Requested Model is part of the usecase.")
        return True, llm_api_key
    else:
        logger.info("Requested Model is not part of the usecase.")
        return False, llm_api_key


async def validate_model_for_usecase(
    request: ChatCompletionRequest,
    x_base_api_key: str = Header(..., alias="x-base-api-key"),
) -> bool:
    is_valid = await validate_models_by_api_key(x_base_api_key, request.model_name)
    if not is_valid:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Requested model '{request.model_name}' is not part of the usecase",
        )
    return is_valid

>> /genai_platform_services/src/utility/vector_store_helpers.py
from typing import Any, Dict

import httpx
from fastapi import Header, HTTPException, status

from src.config import get_settings
from src.db.platform_meta_tables import CollectionInfo, EmbeddingModels, VectorStoreInfo
from src.exception.exceptions import EmbeddingModelError
from src.logging_config import Logger
from src.models.vector_store_payload import CreateVectorStoreRequest
from src.repository.base_repository import BaseRepository
from src.utility.vector_store_utils import is_valid_name

settings = get_settings()
logger = Logger.create_logger(__name__)


async def check_embedding_model(model_name: str) -> tuple[str, int, int]:
    row = BaseRepository.select_one(db_tbl=EmbeddingModels, filters={"model_name": model_name})  # type: ignore
    if not row:
        raise EmbeddingModelError(f"Embedding model name '{model_name}' not found.")
    required_keys = {"model_path", "dimensions", "context_length"}
    missing = required_keys - row.keys()
    if missing:
        raise EmbeddingModelError(f"Embedding model '{model_name}' is missing fields: {', '.join(missing)}")
    return str(row["model_path"]), int(row["dimensions"]), int(row["context_length"])


async def _fetch_usecase_info(api_key: str) -> Any:
    validation_url = f"{settings.prompt_hub_endpoint}{settings.prompt_hub_get_usecase_by_apikey}"
    verify = settings.deployment_env == "PROD"
    async with httpx.AsyncClient(verify=verify) as client:
        resp = await client.get(validation_url, headers={"lite-llm-api-key": api_key})
    if resp.status_code != 200:
        raise HTTPException(status_code=401, detail="Invalid or unauthorized API key")

    try:
        data = resp.json().get("data", {})
        return data
    except (KeyError, ValueError, TypeError):
        raise HTTPException(status_code=502, detail="Malformed response from Prompt Hub")


async def validate_store_access(api_key: str, vector_store: str, service_type: str = "collection") -> Any:
    # use_case_id is team_id
    use_case_id = await _fetch_usecase_info(api_key)
    if service_type == "collection":
        col = BaseRepository.select_one(  # type: ignore
            db_tbl=CollectionInfo, filters={"collection_name": vector_store}
        )  # TODO - Remove
    else:
        col = BaseRepository.select_one(db_tbl=VectorStoreInfo, filters={"id": vector_store})  # type: ignore
    if not col:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Vector Store '{vector_store}' not found in DB.",
        )
    if str(col["usecase_id"]) != use_case_id["team_id"]:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN, detail=f"User is not authorized for this {service_type}."
        )
    return col["model_name"]


async def get_usecase_id_by_api_key(api_key: str) -> Any:
    data = await _fetch_usecase_info(api_key)
    return data["team_id"]


async def validate_vector_store_name(
    request: CreateVectorStoreRequest,
) -> str:
    is_valid, error_message = is_valid_name(request.name)
    if not is_valid:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={"message": error_message},
        )
    return request.name


async def get_valid_usecase_id(
    x_base_api_key: str = Header(..., alias="x-base-api-key"),
) -> Any:
    usecase_id = await get_usecase_id_by_api_key(x_base_api_key)
    if not usecase_id:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid or missing API key. Unable to resolve usecase_id.",
        )
    return usecase_id


async def get_valid_embedding_model(
    request: CreateVectorStoreRequest,
) -> Dict[str, object]:
    try:
        model_path, embedding_dimensions, context_length = await check_embedding_model(request.embedding_model)
        return {
            "model_path": model_path,
            "embedding_dimensions": embedding_dimensions,
            "context_length": context_length,
        }
    except Exception as exc:
        logger.exception(f"Invalid embedding model: {request.embedding_model}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(exc),
        )


async def get_store_model_info(
    store_id: str,
    x_base_api_key: str = Header(..., alias="x-base-api-key"),
) -> Dict[str, object]:
    try:
        # validate store access -> returns model_name (or raises)
        model_name = await validate_store_access(
            api_key=x_base_api_key,
            vector_store=store_id,
            service_type="vector store",
        )
        if not model_name:
            logger.error(f"No model_name returned for store {store_id}")
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"Access denied or no model mapped for vector store '{store_id}'",
            )

        # validate/check embedding model metadata
        model_path, embedding_dimensions, context_length = await check_embedding_model(model_name=model_name)

        return {
            "model_name": model_name,
            "model_path": model_path,
            "embedding_dimensions": embedding_dimensions,
            "context_length": context_length,
        }

    except HTTPException:
        raise
    except Exception as exc:
        logger.exception(f"Error validating store access / embedding model for store '{store_id}': {exc}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to validate store access or embedding model.",
        ) from exc

>> /genai_platform_services/src/utility/vector_store_utils.py
from __future__ import annotations

import re
import sys
from typing import Any, Dict, List, Mapping, Optional, Tuple, Type, TypeVar, Union

from pgvector.sqlalchemy import Vector  # type: ignore
from sqlalchemy import (
    BigInteger,
    Boolean,
    Column,
    DateTime,
    Integer,
    String,
    Text,
    func,
)
from sqlalchemy.dialects.postgresql import ARRAY, JSONB, TSVECTOR, UUID

from src.config import get_settings
from src.db.base import BaseDBA
from src.models.storage_payload import SearchRequest
from src.models.vector_store_payload import SearchType, StorageBackend

settings = get_settings()

T = TypeVar("T", bound=BaseDBA)

POSTGRES_RESERVED_KEYWORDS = {
    "select",
    "insert",
    "update",
    "delete",
    "from",
    "where",
    "table",
    "create",
    "drop",
    "alter",
    "join",
    "on",
    "as",
    "and",
    "or",
    "not",
    "null",
    "into",
    "values",
    "set",
    "group",
    "order",
    "by",
    "limit",
    "having",
    "distinct",
    "union",
    "all",
    "case",
    "when",
    "then",
    "else",
    "end",
}


def is_valid_name(name: str) -> Tuple[bool, Optional[str]]:
    if not isinstance(name, str):
        return False, "Table name must be a string."  # type: ignore
    if not name:
        return False, "Table name cannot be empty."
    if not re.fullmatch(r"[a-zA-Z][a-zA-Z0-9_]{0,62}", name):
        return False, (
            "Table name must start with a letter and can contain only letters, digits, or underscores. "
            "Maximum length is 63 characters."
        )
    if name.lower() in POSTGRES_RESERVED_KEYWORDS:
        return False, f"'{name}' is a reserved PostgreSQL keyword and cannot be used as a table name."
    return True, None


def get_deepsize(obj: list, seen=None) -> int:  # type: ignore
    """
    Recursively compute the size of *obj* (including nested containers).
    Used only when indexing files â€“ logic is unchanged.
    """
    if seen is None:
        seen = set()

    obj_id = id(obj)
    if obj_id in seen:
        return 0
    seen.add(obj_id)

    size = sys.getsizeof(obj)

    if isinstance(obj, dict):  # type: ignore
        size += sum(get_deepsize(k, seen) + get_deepsize(v, seen) for k, v in obj.items())  # type: ignore
    elif isinstance(obj, (list, tuple, set, frozenset)):
        size += sum(get_deepsize(item, seen) for item in obj)

    return size


def create_file_info_tbl_model(table_name: str) -> Type[T]:
    attrs = {
        "__tablename__": table_name,
        "__table_args__": {"extend_existing": True},  # Use extend_existing if table already exists
        "vs_id": Column(UUID(as_uuid=True), primary_key=True, nullable=False),
        "file_id": Column(UUID(as_uuid=True), primary_key=True, nullable=False),
        "file_name": Column(String(255), nullable=False),
        "file_version": Column(Integer, nullable=False),
        "created_at": Column(DateTime, nullable=False, server_default=func.now()),
        "last_error": Column(Text, nullable=True),
        "usage_bytes": Column(BigInteger, nullable=False, server_default="0"),
        "chunking_strategy": Column(String(255), nullable=False),
        "metadata_vs": Column(JSONB, nullable=True),
        "attributes": Column(JSONB, nullable=True),
        "active": Column(Boolean, nullable=False, server_default="true"),
        "status": Column(String(32), nullable=False),
    }
    return type(f"Dynamic_{table_name}", (BaseDBA,), attrs)


def create_chunks_tbl_model(table_name: str, dimensions: int) -> Type[T]:
    attrs = {
        "__tablename__": table_name,
        "__table_args__": {"extend_existing": True},
        "id": Column(UUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid()),
        "file_id": Column(UUID(as_uuid=True), nullable=False),
        "file_name": Column(String(255), nullable=False),
        "embedding": Column(Vector(dimensions), nullable=False),
        "content": Column(Text, nullable=False),
        "links": Column(ARRAY(String), nullable=True),
        "topics": Column(ARRAY(String), nullable=True),
        "author": Column(String, nullable=True),
        "meta_data": Column(JSONB, nullable=True),
        "search_vector": Column(TSVECTOR, nullable=True),
    }
    return type(f"Dynamic_{table_name}", (BaseDBA,), attrs)


def _extract_filters(
    filt: Union[Dict[str, Any], List[Any]],
    content: List[str],
    link: List[str],
    topic: List[str],
) -> None:
    if isinstance(filt, list):
        for item in filt:
            _extract_filters(item, content, link, topic)
        return

    if not isinstance(filt, Mapping):
        # Anything that is not a dict/list is ignored
        return  # type: ignore

    # ---------- ComparisonFilter ----------
    if "key" in filt and "value" in filt:
        key = filt["key"]
        val = str(filt["value"])  # always store as string
        if key == "content":
            content.append(val)
        elif key == "link":
            link.append(val)
        elif key == "topic":
            topic.append(val)
        # other keys are ignored (they are not part of SearchRequest)
        return

    # ---------- CompoundFilter ----------
    inner = filt.get("filters")
    if inner is not None:
        _extract_filters(inner, content, link, topic)
    # otherwise ignore


def _to_plain_dict(payload: Any) -> Dict[str, Any]:
    if hasattr(payload, "model_dump"):
        return payload.model_dump()  # type: ignore
    if isinstance(payload, dict):
        return payload
    return dict(payload)


def payload_to_internal_format(
    api_payload: Any,
    *,
    collection: str,
) -> SearchRequest:
    payload = _to_plain_dict(api_payload)
    query = payload.get("query")
    if isinstance(query, list):
        search_text = query[0] if query else ""
    else:
        search_text = query or ""

    limit: int = payload.get("max_num_results", settings.default_document_limit)

    ranking = payload.get("ranking_options")
    min_score: float = (
        ranking.get("score_threshold")
        if ranking and ranking.get("score_threshold") is not None
        else settings.min_similarity_score
    )
    use_ranking: bool | None = None
    if ranking and "ranker" in ranking:
        use_ranking = ranking["ranker"] == "auto"

    content_filter: List[str] | None = None
    link_filter: List[str] | None = None
    topic_filter: List[str] | None = None

    raw_filters = payload.get("filters")
    if raw_filters:
        content_filter = []
        link_filter = []
        topic_filter = []
        _extract_filters(raw_filters, content_filter, link_filter, topic_filter)

        if not content_filter:
            content_filter = None
        if not link_filter:
            link_filter = None
        if not topic_filter:
            topic_filter = None

    return SearchRequest(
        collection=collection,
        search_type=SearchType(payload["search_type"]),
        storage_backend=StorageBackend(payload["storage_backend"]),
        search_text=search_text,
        content_filter=content_filter,
        link_filter=link_filter,
        topic_filter=topic_filter,
        limit=limit,
        min_score=min_score,
        use_ranking=use_ranking,
    )
